[
    {
        "label": "contextlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "contextlib",
        "description": "contextlib",
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "ExitStack",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "ExitStack",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "cv2",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "cv2",
        "description": "cv2",
        "detail": "cv2",
        "documentation": {}
    },
    {
        "label": "inspect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inspect",
        "description": "inspect",
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "distributed",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "distributed",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "urllib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib",
        "description": "urllib",
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "pkg_resources",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pkg_resources",
        "description": "pkg_resources",
        "detail": "pkg_resources",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "check_output",
        "importPath": "subprocess",
        "description": "subprocess",
        "isExtraImport": true,
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "IO",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "IO",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "EasyDict",
        "importPath": "easydict",
        "description": "easydict",
        "isExtraImport": true,
        "detail": "easydict",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_version",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "file_size",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "get_default_args",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "get_format_idx",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "url2file",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "yaml_save",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "Profile",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_version",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "file_size",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "get_default_args",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "get_format_idx",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "load_model",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "print_args",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "url2file",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "yaml_save",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "Profile",
        "importPath": "deploy_utils",
        "description": "deploy_utils",
        "isExtraImport": true,
        "detail": "deploy_utils",
        "documentation": {}
    },
    {
        "label": "config_loader",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "config_loader",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "create_logger",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "load_params_from_file",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "config_loader",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "config_loader",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "create_logger",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "load_params_from_file",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_pos_fullres",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_pos_fullres",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_pos_fullres",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_pos_fullres",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "color_map_tensorboard",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "write_tensorboard",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "color_map_tensorboard",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "write_tensorboard",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "load_params_from_file",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_pos_fullres",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_pos_fullres",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_pos_fullres",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_pos_fullres",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "color_map_tensorboard",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "write_tensorboard",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "color_map_tensorboard",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "write_tensorboard",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "load_params_from_file",
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "isExtraImport": true,
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "LightStereo",
        "importPath": "stereo.modeling.models.lightstereo.lightstereo",
        "description": "stereo.modeling.models.lightstereo.lightstereo",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.lightstereo.lightstereo",
        "documentation": {}
    },
    {
        "label": "LightStereo",
        "importPath": "stereo.modeling.models.lightstereo.lightstereo",
        "description": "stereo.modeling.models.lightstereo.lightstereo",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.lightstereo.lightstereo",
        "documentation": {}
    },
    {
        "label": "LightStereo",
        "importPath": "stereo.modeling.models.lightstereo.lightstereo",
        "description": "stereo.modeling.models.lightstereo.lightstereo",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.lightstereo.lightstereo",
        "documentation": {}
    },
    {
        "label": "LightStereo",
        "importPath": "stereo.modeling.models.lightstereo.lightstereo",
        "description": "stereo.modeling.models.lightstereo.lightstereo",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.lightstereo.lightstereo",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "optimize_for_mobile",
        "importPath": "torch.utils.mobile_optimizer",
        "description": "torch.utils.mobile_optimizer",
        "isExtraImport": true,
        "detail": "torch.utils.mobile_optimizer",
        "documentation": {}
    },
    {
        "label": "optimize_for_mobile",
        "importPath": "torch.utils.mobile_optimizer",
        "description": "torch.utils.mobile_optimizer",
        "isExtraImport": true,
        "detail": "torch.utils.mobile_optimizer",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "build_trainer",
        "importPath": "stereo.modeling",
        "description": "stereo.modeling",
        "isExtraImport": true,
        "detail": "stereo.modeling",
        "documentation": {}
    },
    {
        "label": "STTR",
        "importPath": "stereo.modeling.models.sttr.sttr",
        "description": "stereo.modeling.models.sttr.sttr",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.sttr.sttr",
        "documentation": {}
    },
    {
        "label": "STTR",
        "importPath": "stereo.modeling.models.sttr.sttr",
        "description": "stereo.modeling.models.sttr.sttr",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.sttr.sttr",
        "documentation": {}
    },
    {
        "label": "PSMNet",
        "importPath": "stereo.modeling.models.psmnet.psmnet",
        "description": "stereo.modeling.models.psmnet.psmnet",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.psmnet.psmnet",
        "documentation": {}
    },
    {
        "label": "PSMNet",
        "importPath": "stereo.modeling.models.psmnet.psmnet",
        "description": "stereo.modeling.models.psmnet.psmnet",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.psmnet.psmnet",
        "documentation": {}
    },
    {
        "label": "MSNet2D",
        "importPath": "stereo.modeling.models.msnet.MSNet2D",
        "description": "stereo.modeling.models.msnet.MSNet2D",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.msnet.MSNet2D",
        "documentation": {}
    },
    {
        "label": "MSNet2D",
        "importPath": "stereo.modeling.models.msnet.MSNet2D",
        "description": "stereo.modeling.models.msnet.MSNet2D",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.msnet.MSNet2D",
        "documentation": {}
    },
    {
        "label": "MSNet3D",
        "importPath": "stereo.modeling.models.msnet.MSNet3D",
        "description": "stereo.modeling.models.msnet.MSNet3D",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.msnet.MSNet3D",
        "documentation": {}
    },
    {
        "label": "MSNet3D",
        "importPath": "stereo.modeling.models.msnet.MSNet3D",
        "description": "stereo.modeling.models.msnet.MSNet3D",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.msnet.MSNet3D",
        "documentation": {}
    },
    {
        "label": "IGEVStereo",
        "importPath": "stereo.modeling.models.igev.igev_stereo",
        "description": "stereo.modeling.models.igev.igev_stereo",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.igev.igev_stereo",
        "documentation": {}
    },
    {
        "label": "IGEVStereo",
        "importPath": "stereo.modeling.models.igev.igev_stereo",
        "description": "stereo.modeling.models.igev.igev_stereo",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.igev.igev_stereo",
        "documentation": {}
    },
    {
        "label": "GwcNet",
        "importPath": "stereo.modeling.models.gwcnet.gwcnet",
        "description": "stereo.modeling.models.gwcnet.gwcnet",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.gwcnet.gwcnet",
        "documentation": {}
    },
    {
        "label": "GwcNet",
        "importPath": "stereo.modeling.models.gwcnet.gwcnet",
        "description": "stereo.modeling.models.gwcnet.gwcnet",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.gwcnet.gwcnet",
        "documentation": {}
    },
    {
        "label": "FADNet",
        "importPath": "stereo.modeling.models.fadnet.fadnet",
        "description": "stereo.modeling.models.fadnet.fadnet",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.fadnet.fadnet",
        "documentation": {}
    },
    {
        "label": "FADNet",
        "importPath": "stereo.modeling.models.fadnet.fadnet",
        "description": "stereo.modeling.models.fadnet.fadnet",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.fadnet.fadnet",
        "documentation": {}
    },
    {
        "label": "CoEx",
        "importPath": "stereo.modeling.models.coex.coex",
        "description": "stereo.modeling.models.coex.coex",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.coex.coex",
        "documentation": {}
    },
    {
        "label": "CoEx",
        "importPath": "stereo.modeling.models.coex.coex",
        "description": "stereo.modeling.models.coex.coex",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.coex.coex",
        "documentation": {}
    },
    {
        "label": "CFNet",
        "importPath": "stereo.modeling.models.cfnet.cfnet",
        "description": "stereo.modeling.models.cfnet.cfnet",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "CFNet",
        "importPath": "stereo.modeling.models.cfnet.cfnet",
        "description": "stereo.modeling.models.cfnet.cfnet",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "GwcNet",
        "importPath": "stereo.modeling.models.casnet.cas_gwc",
        "description": "stereo.modeling.models.casnet.cas_gwc",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "GwcNet",
        "importPath": "stereo.modeling.models.casnet.cas_gwc",
        "description": "stereo.modeling.models.casnet.cas_gwc",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "PSMNet",
        "importPath": "stereo.modeling.models.casnet.cas_psm",
        "description": "stereo.modeling.models.casnet.cas_psm",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "PSMNet",
        "importPath": "stereo.modeling.models.casnet.cas_psm",
        "description": "stereo.modeling.models.casnet.cas_psm",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "init",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "torchvision.transforms.functional",
        "description": "torchvision.transforms.functional",
        "isExtraImport": true,
        "detail": "torchvision.transforms.functional",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "TensorDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "RandomSampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "TensorDataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "torchvision",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision",
        "description": "torchvision",
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "models",
        "importPath": "torchvision",
        "description": "torchvision",
        "isExtraImport": true,
        "detail": "torchvision",
        "documentation": {}
    },
    {
        "label": "PIL",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL",
        "description": "PIL",
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageFilter",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageOps",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "ImageFilter",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "Image",
        "importPath": "PIL",
        "description": "PIL",
        "isExtraImport": true,
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "os,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.",
        "description": "os.",
        "detail": "os.",
        "documentation": {}
    },
    {
        "label": "onnx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnx",
        "description": "onnx",
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "helper",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "numpy_helper",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "TensorProto",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "ModelProto",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "NodeProto",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "GraphProto",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "helper",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "TensorProto",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "numpy_helper",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "numpy_helper",
        "importPath": "onnx",
        "description": "onnx",
        "isExtraImport": true,
        "detail": "onnx",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "onnxruntime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnxruntime",
        "description": "onnxruntime",
        "detail": "onnxruntime",
        "documentation": {}
    },
    {
        "label": "onnxoptimizer",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "onnxoptimizer",
        "description": "onnxoptimizer",
        "detail": "onnxoptimizer",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "ColorJitter",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "ColorJitter",
        "importPath": "torchvision.transforms",
        "description": "torchvision.transforms",
        "isExtraImport": true,
        "detail": "torchvision.transforms",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "imageio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "imageio",
        "description": "imageio",
        "detail": "imageio",
        "documentation": {}
    },
    {
        "label": "h5py",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "h5py",
        "description": "h5py",
        "detail": "h5py",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "gradcheck",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Variable",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "gradcheck",
        "importPath": "torch.autograd",
        "description": "torch.autograd",
        "isExtraImport": true,
        "detail": "torch.autograd",
        "documentation": {}
    },
    {
        "label": "once_differentiable",
        "importPath": "torch.autograd.function",
        "description": "torch.autograd.function",
        "isExtraImport": true,
        "detail": "torch.autograd.function",
        "documentation": {}
    },
    {
        "label": "once_differentiable",
        "importPath": "torch.autograd.function",
        "description": "torch.autograd.function",
        "isExtraImport": true,
        "detail": "torch.autograd.function",
        "documentation": {}
    },
    {
        "label": "once_differentiable",
        "importPath": "torch.autograd.function",
        "description": "torch.autograd.function",
        "isExtraImport": true,
        "detail": "torch.autograd.function",
        "documentation": {}
    },
    {
        "label": "once_differentiable",
        "importPath": "torch.autograd.function",
        "description": "torch.autograd.function",
        "isExtraImport": true,
        "detail": "torch.autograd.function",
        "documentation": {}
    },
    {
        "label": "_pair",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_single",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_pair",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_pair",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_triple",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_pair",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_single",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_pair",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_pair",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "_triple",
        "importPath": "torch.nn.modules.utils",
        "description": "torch.nn.modules.utils",
        "isExtraImport": true,
        "detail": "torch.nn.modules.utils",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "BuildExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDAExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDA_HOME",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CppExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDAExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "BuildExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDAExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDA_HOME",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CppExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "CUDAExtension",
        "importPath": "torch.utils.cpp_extension",
        "description": "torch.utils.cpp_extension",
        "isExtraImport": true,
        "detail": "torch.utils.cpp_extension",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "lru_cache",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "kaiming_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "kaiming_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "kaiming_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "kaiming_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "constant_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "xavier_uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "constant_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "xavier_uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "kaiming_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "kaiming_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "kaiming_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "kaiming_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "constant_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "xavier_uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "constant_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "xavier_uniform_",
        "importPath": "torch.nn.init",
        "description": "torch.nn.init",
        "isExtraImport": true,
        "detail": "torch.nn.init",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "one_hot",
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "isExtraImport": true,
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "softmax",
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "isExtraImport": true,
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "one_hot",
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "isExtraImport": true,
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "softmax",
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "isExtraImport": true,
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv3d",
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv3d",
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv3d",
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv3d",
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "isExtraImport": true,
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "FeatureAtt",
        "importPath": "stereo.modeling.models.igev.igev_blocks",
        "description": "stereo.modeling.models.igev.igev_blocks",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.igev.igev_blocks",
        "documentation": {}
    },
    {
        "label": "FeatureAtt",
        "importPath": "stereo.modeling.models.igev.igev_blocks",
        "description": "stereo.modeling.models.igev.igev_blocks",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.igev.igev_blocks",
        "documentation": {}
    },
    {
        "label": "DeformConv",
        "importPath": "stereo.libs.AANet.deform_conv",
        "description": "stereo.libs.AANet.deform_conv",
        "isExtraImport": true,
        "detail": "stereo.libs.AANet.deform_conv",
        "documentation": {}
    },
    {
        "label": "ModulatedDeformConv",
        "importPath": "stereo.libs.AANet.deform_conv",
        "description": "stereo.libs.AANet.deform_conv",
        "isExtraImport": true,
        "detail": "stereo.libs.AANet.deform_conv",
        "documentation": {}
    },
    {
        "label": "DeformConv",
        "importPath": "stereo.libs.AANet.deform_conv",
        "description": "stereo.libs.AANet.deform_conv",
        "isExtraImport": true,
        "detail": "stereo.libs.AANet.deform_conv",
        "documentation": {}
    },
    {
        "label": "ModulatedDeformConv",
        "importPath": "stereo.libs.AANet.deform_conv",
        "description": "stereo.libs.AANet.deform_conv",
        "isExtraImport": true,
        "detail": "stereo.libs.AANet.deform_conv",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "isExtraImport": true,
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "timm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "timm",
        "description": "timm",
        "detail": "timm",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "RoIAlign",
        "importPath": "torchvision.ops",
        "description": "torchvision.ops",
        "isExtraImport": true,
        "detail": "torchvision.ops",
        "documentation": {}
    },
    {
        "label": "FeaturePyramidNetwork",
        "importPath": "torchvision.ops",
        "description": "torchvision.ops",
        "isExtraImport": true,
        "detail": "torchvision.ops",
        "documentation": {}
    },
    {
        "label": "RoIAlign",
        "importPath": "torchvision.ops",
        "description": "torchvision.ops",
        "isExtraImport": true,
        "detail": "torchvision.ops",
        "documentation": {}
    },
    {
        "label": "FeaturePyramidNetwork",
        "importPath": "torchvision.ops",
        "description": "torchvision.ops",
        "isExtraImport": true,
        "detail": "torchvision.ops",
        "documentation": {}
    },
    {
        "label": "torch,pdb,logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.pdb.logging",
        "description": "torch.pdb.logging",
        "detail": "torch.pdb.logging",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "interpolate",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "torch,logging,os,sys,urllib,warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.logging.os.sys.urllib.warnings",
        "description": "torch.logging.os.sys.urllib.warnings",
        "detail": "torch.logging.os.sys.urllib.warnings",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "core.submodule",
        "description": "core.submodule",
        "isExtraImport": true,
        "detail": "core.submodule",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "core.submodule",
        "description": "core.submodule",
        "isExtraImport": true,
        "detail": "core.submodule",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "core.submodule",
        "description": "core.submodule",
        "isExtraImport": true,
        "detail": "core.submodule",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "core.submodule",
        "description": "core.submodule",
        "isExtraImport": true,
        "detail": "core.submodule",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "Utils",
        "description": "Utils",
        "isExtraImport": true,
        "detail": "Utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "Utils",
        "description": "Utils",
        "isExtraImport": true,
        "detail": "Utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "Utils",
        "description": "Utils",
        "isExtraImport": true,
        "detail": "Utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "Utils",
        "description": "Utils",
        "isExtraImport": true,
        "detail": "Utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "Utils",
        "description": "Utils",
        "isExtraImport": true,
        "detail": "Utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "Utils",
        "description": "Utils",
        "isExtraImport": true,
        "detail": "Utils",
        "documentation": {}
    },
    {
        "label": "torch,pdb,logging,timm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.pdb.logging.timm",
        "description": "torch.pdb.logging.timm",
        "detail": "torch.pdb.logging.timm",
        "documentation": {}
    },
    {
        "label": "sys,os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys.os",
        "description": "sys.os",
        "detail": "sys.os",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereo.modeling.models.foundationstereo.core.extractor",
        "description": "stereo.modeling.models.foundationstereo.core.extractor",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereo.modeling.models.foundationstereo.core.extractor",
        "description": "stereo.modeling.models.foundationstereo.core.extractor",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "Combined_Geo_Encoding_Volume",
        "importPath": "stereo.modeling.models.foundationstereo.core.geometry",
        "description": "stereo.modeling.models.foundationstereo.core.geometry",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.core.geometry",
        "documentation": {}
    },
    {
        "label": "Combined_Geo_Encoding_Volume",
        "importPath": "stereo.modeling.models.foundationstereo.core.geometry",
        "description": "stereo.modeling.models.foundationstereo.core.geometry",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.core.geometry",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "isExtraImport": true,
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "time,huggingface_hub",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time.huggingface_hub",
        "description": "time.huggingface_hub",
        "detail": "time.huggingface_hub",
        "documentation": {}
    },
    {
        "label": "torch,pdb,os,sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.pdb.os.sys",
        "description": "torch.pdb.os.sys",
        "detail": "torch.pdb.os.sys",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "importPath": "core.utils.utils",
        "description": "core.utils.utils",
        "isExtraImport": true,
        "detail": "core.utils.utils",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "importPath": "core.utils.utils",
        "description": "core.utils.utils",
        "isExtraImport": true,
        "detail": "core.utils.utils",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "flash_attn_qkvpacked_func",
        "importPath": "flash_attn",
        "description": "flash_attn",
        "isExtraImport": true,
        "detail": "flash_attn",
        "documentation": {}
    },
    {
        "label": "flash_attn_func",
        "importPath": "flash_attn",
        "description": "flash_attn",
        "isExtraImport": true,
        "detail": "flash_attn",
        "documentation": {}
    },
    {
        "label": "flash_attn_qkvpacked_func",
        "importPath": "flash_attn",
        "description": "flash_attn",
        "isExtraImport": true,
        "detail": "flash_attn",
        "documentation": {}
    },
    {
        "label": "flash_attn_func",
        "importPath": "flash_attn",
        "description": "flash_attn",
        "isExtraImport": true,
        "detail": "flash_attn",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "core.extractor",
        "description": "core.extractor",
        "isExtraImport": true,
        "detail": "core.extractor",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "core.extractor",
        "description": "core.extractor",
        "isExtraImport": true,
        "detail": "core.extractor",
        "documentation": {}
    },
    {
        "label": "torch,os,sys,pdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.os.sys.pdb",
        "description": "torch.os.sys.pdb",
        "detail": "torch.os.sys.pdb",
        "documentation": {}
    },
    {
        "label": "vit_small",
        "importPath": "dinov2.models.vision_transformer",
        "description": "dinov2.models.vision_transformer",
        "isExtraImport": true,
        "detail": "dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_base",
        "importPath": "dinov2.models.vision_transformer",
        "description": "dinov2.models.vision_transformer",
        "isExtraImport": true,
        "detail": "dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_large",
        "importPath": "dinov2.models.vision_transformer",
        "description": "dinov2.models.vision_transformer",
        "isExtraImport": true,
        "detail": "dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "BlockChunk",
        "importPath": "dinov2.models.vision_transformer",
        "description": "dinov2.models.vision_transformer",
        "isExtraImport": true,
        "detail": "dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_small",
        "importPath": "dinov2.models.vision_transformer",
        "description": "dinov2.models.vision_transformer",
        "isExtraImport": true,
        "detail": "dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_base",
        "importPath": "dinov2.models.vision_transformer",
        "description": "dinov2.models.vision_transformer",
        "isExtraImport": true,
        "detail": "dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_large",
        "importPath": "dinov2.models.vision_transformer",
        "description": "dinov2.models.vision_transformer",
        "isExtraImport": true,
        "detail": "dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "BlockChunk",
        "importPath": "dinov2.models.vision_transformer",
        "description": "dinov2.models.vision_transformer",
        "isExtraImport": true,
        "detail": "dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "importPath": "depth_anything.blocks",
        "description": "depth_anything.blocks",
        "isExtraImport": true,
        "detail": "depth_anything.blocks",
        "documentation": {}
    },
    {
        "label": "_make_scratch",
        "importPath": "depth_anything.blocks",
        "description": "depth_anything.blocks",
        "isExtraImport": true,
        "detail": "depth_anything.blocks",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "importPath": "depth_anything.blocks",
        "description": "depth_anything.blocks",
        "isExtraImport": true,
        "detail": "depth_anything.blocks",
        "documentation": {}
    },
    {
        "label": "_make_scratch",
        "importPath": "depth_anything.blocks",
        "description": "depth_anything.blocks",
        "isExtraImport": true,
        "detail": "depth_anything.blocks",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "auto_fp16",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "force_fp32",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "auto_fp16",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "OptimizerHook",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "HOOKS",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "ModuleList",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "force_fp32",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "ModuleList",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "auto_fp16",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "force_fp32",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "auto_fp16",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "OptimizerHook",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "HOOKS",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "ModuleList",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "force_fp32",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "ModuleList",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner",
        "description": "mmcv.runner",
        "isExtraImport": true,
        "detail": "mmcv.runner",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABCMeta",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "mmcv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mmcv",
        "description": "mmcv",
        "detail": "mmcv",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_activation_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "MODELS",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_plugin_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "caffe2_xavier_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "PLUGIN_LAYERS",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "caffe2_xavier_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "normal_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "xavier_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_activation_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_norm_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "xavier_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_activation_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "MODELS",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_plugin_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "caffe2_xavier_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "PLUGIN_LAYERS",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "caffe2_xavier_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "normal_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "xavier_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_activation_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "build_norm_layer",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "xavier_init",
        "importPath": "mmcv.cnn",
        "description": "mmcv.cnn",
        "isExtraImport": true,
        "detail": "mmcv.cnn",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "abc",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "abc",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "deque",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "Counter",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "ATTENTION",
        "importPath": "mmcv.cnn.bricks.registry",
        "description": "mmcv.cnn.bricks.registry",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.registry",
        "documentation": {}
    },
    {
        "label": "FEEDFORWARD_NETWORK",
        "importPath": "mmcv.cnn.bricks.registry",
        "description": "mmcv.cnn.bricks.registry",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.registry",
        "documentation": {}
    },
    {
        "label": "TRANSFORMER_LAYER",
        "importPath": "mmcv.cnn.bricks.registry",
        "description": "mmcv.cnn.bricks.registry",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.registry",
        "documentation": {}
    },
    {
        "label": "TRANSFORMER_LAYER_SEQUENCE",
        "importPath": "mmcv.cnn.bricks.registry",
        "description": "mmcv.cnn.bricks.registry",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.registry",
        "documentation": {}
    },
    {
        "label": "ATTENTION",
        "importPath": "mmcv.cnn.bricks.registry",
        "description": "mmcv.cnn.bricks.registry",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.registry",
        "documentation": {}
    },
    {
        "label": "FEEDFORWARD_NETWORK",
        "importPath": "mmcv.cnn.bricks.registry",
        "description": "mmcv.cnn.bricks.registry",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.registry",
        "documentation": {}
    },
    {
        "label": "TRANSFORMER_LAYER",
        "importPath": "mmcv.cnn.bricks.registry",
        "description": "mmcv.cnn.bricks.registry",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.registry",
        "documentation": {}
    },
    {
        "label": "TRANSFORMER_LAYER_SEQUENCE",
        "importPath": "mmcv.cnn.bricks.registry",
        "description": "mmcv.cnn.bricks.registry",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.registry",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "build_from_cfg",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "build_from_cfg",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "deprecated_api_warning",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "build_from_cfg",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "build_from_cfg",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "deprecated_api_warning",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "Registry",
        "importPath": "mmcv.utils",
        "description": "mmcv.utils",
        "isExtraImport": true,
        "detail": "mmcv.utils",
        "documentation": {}
    },
    {
        "label": "BACKBONES",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "HEADS",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "BACKBONES",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "HEADS",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "build_loss",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "LOSSES",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "LOSSES",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "SEGMENTORS",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "BACKBONES",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "HEADS",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "BACKBONES",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "HEADS",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "build_loss",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "LOSSES",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "LOSSES",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "SEGMENTORS",
        "importPath": "mmseg.models.builder",
        "description": "mmseg.models.builder",
        "isExtraImport": true,
        "detail": "mmseg.models.builder",
        "documentation": {}
    },
    {
        "label": "BaseDecodeHead",
        "importPath": "mmseg.models.decode_heads.decode_head",
        "description": "mmseg.models.decode_heads.decode_head",
        "isExtraImport": true,
        "detail": "mmseg.models.decode_heads.decode_head",
        "documentation": {}
    },
    {
        "label": "BaseDecodeHead",
        "importPath": "mmseg.models.decode_heads.decode_head",
        "description": "mmseg.models.decode_heads.decode_head",
        "isExtraImport": true,
        "detail": "mmseg.models.decode_heads.decode_head",
        "documentation": {}
    },
    {
        "label": "BaseDecodeHead",
        "importPath": "mmseg.models.decode_heads.decode_head",
        "description": "mmseg.models.decode_heads.decode_head",
        "isExtraImport": true,
        "detail": "mmseg.models.decode_heads.decode_head",
        "documentation": {}
    },
    {
        "label": "BaseDecodeHead",
        "importPath": "mmseg.models.decode_heads.decode_head",
        "description": "mmseg.models.decode_heads.decode_head",
        "isExtraImport": true,
        "detail": "mmseg.models.decode_heads.decode_head",
        "documentation": {}
    },
    {
        "label": "resize",
        "importPath": "mmseg.ops",
        "description": "mmseg.ops",
        "isExtraImport": true,
        "detail": "mmseg.ops",
        "documentation": {}
    },
    {
        "label": "resize",
        "importPath": "mmseg.ops",
        "description": "mmseg.ops",
        "isExtraImport": true,
        "detail": "mmseg.ops",
        "documentation": {}
    },
    {
        "label": "resize",
        "importPath": "mmseg.ops",
        "description": "mmseg.ops",
        "isExtraImport": true,
        "detail": "mmseg.ops",
        "documentation": {}
    },
    {
        "label": "resize",
        "importPath": "mmseg.ops",
        "description": "mmseg.ops",
        "isExtraImport": true,
        "detail": "mmseg.ops",
        "documentation": {}
    },
    {
        "label": "resize",
        "importPath": "mmseg.ops",
        "description": "mmseg.ops",
        "isExtraImport": true,
        "detail": "mmseg.ops",
        "documentation": {}
    },
    {
        "label": "resize",
        "importPath": "mmseg.ops",
        "description": "mmseg.ops",
        "isExtraImport": true,
        "detail": "mmseg.ops",
        "documentation": {}
    },
    {
        "label": "torch.utils.checkpoint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "isExtraImport": true,
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "isExtraImport": true,
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "get_root_logger",
        "importPath": "mmseg.utils",
        "description": "mmseg.utils",
        "isExtraImport": true,
        "detail": "mmseg.utils",
        "documentation": {}
    },
    {
        "label": "get_root_logger",
        "importPath": "mmseg.utils",
        "description": "mmseg.utils",
        "isExtraImport": true,
        "detail": "mmseg.utils",
        "documentation": {}
    },
    {
        "label": "build_positional_encoding",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer_layer_sequence",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_positional_encoding",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer_layer_sequence",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "POSITIONAL_ENCODING",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "BaseTransformerLayer",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerLayerSequence",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer_layer_sequence",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_positional_encoding",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer_layer_sequence",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_positional_encoding",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer_layer_sequence",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "POSITIONAL_ENCODING",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "BaseTransformerLayer",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerLayerSequence",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer_layer_sequence",
        "importPath": "mmcv.cnn.bricks.transformer",
        "description": "mmcv.cnn.bricks.transformer",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.transformer",
        "documentation": {}
    },
    {
        "label": "point_sample",
        "importPath": "mmcv.ops",
        "description": "mmcv.ops",
        "isExtraImport": true,
        "detail": "mmcv.ops",
        "documentation": {}
    },
    {
        "label": "point_sample",
        "importPath": "mmcv.ops",
        "description": "mmcv.ops",
        "isExtraImport": true,
        "detail": "mmcv.ops",
        "documentation": {}
    },
    {
        "label": "point_sample",
        "importPath": "mmcv.ops",
        "description": "mmcv.ops",
        "isExtraImport": true,
        "detail": "mmcv.ops",
        "documentation": {}
    },
    {
        "label": "point_sample",
        "importPath": "mmcv.ops",
        "description": "mmcv.ops",
        "isExtraImport": true,
        "detail": "mmcv.ops",
        "documentation": {}
    },
    {
        "label": "get_class_weight",
        "importPath": "mmseg.models.losses.utils",
        "description": "mmseg.models.losses.utils",
        "isExtraImport": true,
        "detail": "mmseg.models.losses.utils",
        "documentation": {}
    },
    {
        "label": "weight_reduce_loss",
        "importPath": "mmseg.models.losses.utils",
        "description": "mmseg.models.losses.utils",
        "isExtraImport": true,
        "detail": "mmseg.models.losses.utils",
        "documentation": {}
    },
    {
        "label": "weight_reduce_loss",
        "importPath": "mmseg.models.losses.utils",
        "description": "mmseg.models.losses.utils",
        "isExtraImport": true,
        "detail": "mmseg.models.losses.utils",
        "documentation": {}
    },
    {
        "label": "get_class_weight",
        "importPath": "mmseg.models.losses.utils",
        "description": "mmseg.models.losses.utils",
        "isExtraImport": true,
        "detail": "mmseg.models.losses.utils",
        "documentation": {}
    },
    {
        "label": "weight_reduce_loss",
        "importPath": "mmseg.models.losses.utils",
        "description": "mmseg.models.losses.utils",
        "isExtraImport": true,
        "detail": "mmseg.models.losses.utils",
        "documentation": {}
    },
    {
        "label": "weight_reduce_loss",
        "importPath": "mmseg.models.losses.utils",
        "description": "mmseg.models.losses.utils",
        "isExtraImport": true,
        "detail": "mmseg.models.losses.utils",
        "documentation": {}
    },
    {
        "label": "add_prefix",
        "importPath": "mmseg.core",
        "description": "mmseg.core",
        "isExtraImport": true,
        "detail": "mmseg.core",
        "documentation": {}
    },
    {
        "label": "add_prefix",
        "importPath": "mmseg.core",
        "description": "mmseg.core",
        "isExtraImport": true,
        "detail": "mmseg.core",
        "documentation": {}
    },
    {
        "label": "builder",
        "importPath": "mmseg.models",
        "description": "mmseg.models",
        "isExtraImport": true,
        "detail": "mmseg.models",
        "documentation": {}
    },
    {
        "label": "builder",
        "importPath": "mmseg.models",
        "description": "mmseg.models",
        "isExtraImport": true,
        "detail": "mmseg.models",
        "documentation": {}
    },
    {
        "label": "BaseSegmentor",
        "importPath": "mmseg.models.segmentors.base",
        "description": "mmseg.models.segmentors.base",
        "isExtraImport": true,
        "detail": "mmseg.models.segmentors.base",
        "documentation": {}
    },
    {
        "label": "BaseSegmentor",
        "importPath": "mmseg.models.segmentors.base",
        "description": "mmseg.models.segmentors.base",
        "isExtraImport": true,
        "detail": "mmseg.models.segmentors.base",
        "documentation": {}
    },
    {
        "label": "build_dropout",
        "importPath": "mmcv.cnn.bricks.drop",
        "description": "mmcv.cnn.bricks.drop",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.drop",
        "documentation": {}
    },
    {
        "label": "build_dropout",
        "importPath": "mmcv.cnn.bricks.drop",
        "description": "mmcv.cnn.bricks.drop",
        "isExtraImport": true,
        "detail": "mmcv.cnn.bricks.drop",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner.base_module",
        "description": "mmcv.runner.base_module",
        "isExtraImport": true,
        "detail": "mmcv.runner.base_module",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "mmcv.runner.base_module",
        "description": "mmcv.runner.base_module",
        "isExtraImport": true,
        "detail": "mmcv.runner.base_module",
        "documentation": {}
    },
    {
        "label": "BaseModule",
        "importPath": "mmcv.runner.base_module",
        "description": "mmcv.runner.base_module",
        "isExtraImport": true,
        "detail": "mmcv.runner.base_module",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "mmcv.runner.base_module",
        "description": "mmcv.runner.base_module",
        "isExtraImport": true,
        "detail": "mmcv.runner.base_module",
        "documentation": {}
    },
    {
        "label": "custom_fwd",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "custom_bwd",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "custom_fwd",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "custom_fwd",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "custom_bwd",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "custom_fwd",
        "importPath": "torch.cuda.amp",
        "description": "torch.cuda.amp",
        "isExtraImport": true,
        "detail": "torch.cuda.amp",
        "documentation": {}
    },
    {
        "label": "dinov2.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dinov2.distributed",
        "description": "dinov2.distributed",
        "detail": "dinov2.distributed",
        "documentation": {}
    },
    {
        "label": "get_global_rank",
        "importPath": "dinov2.distributed",
        "description": "dinov2.distributed",
        "isExtraImport": true,
        "detail": "dinov2.distributed",
        "documentation": {}
    },
    {
        "label": "get_global_size",
        "importPath": "dinov2.distributed",
        "description": "dinov2.distributed",
        "isExtraImport": true,
        "detail": "dinov2.distributed",
        "documentation": {}
    },
    {
        "label": "get_global_rank",
        "importPath": "dinov2.distributed",
        "description": "dinov2.distributed",
        "isExtraImport": true,
        "detail": "dinov2.distributed",
        "documentation": {}
    },
    {
        "label": "get_global_size",
        "importPath": "dinov2.distributed",
        "description": "dinov2.distributed",
        "isExtraImport": true,
        "detail": "dinov2.distributed",
        "documentation": {}
    },
    {
        "label": "SamplerType",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_data_loader",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_dataset",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "SamplerType",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_data_loader",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_dataset",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_dataset",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "DatasetWithEnumeratedTargets",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "SamplerType",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_data_loader",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "SamplerType",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_data_loader",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_dataset",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "collate_data_and_cast",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "DataAugmentationDINO",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "MaskingGenerator",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "SamplerType",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_data_loader",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_dataset",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "SamplerType",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_data_loader",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_dataset",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_dataset",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "DatasetWithEnumeratedTargets",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "SamplerType",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_data_loader",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "SamplerType",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_data_loader",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_dataset",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "collate_data_and_cast",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "DataAugmentationDINO",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "MaskingGenerator",
        "importPath": "dinov2.data",
        "description": "dinov2.data",
        "isExtraImport": true,
        "detail": "dinov2.data",
        "documentation": {}
    },
    {
        "label": "make_classification_eval_transform",
        "importPath": "dinov2.data.transforms",
        "description": "dinov2.data.transforms",
        "isExtraImport": true,
        "detail": "dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "make_classification_eval_transform",
        "importPath": "dinov2.data.transforms",
        "description": "dinov2.data.transforms",
        "isExtraImport": true,
        "detail": "dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "make_classification_train_transform",
        "importPath": "dinov2.data.transforms",
        "description": "dinov2.data.transforms",
        "isExtraImport": true,
        "detail": "dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "make_classification_eval_transform",
        "importPath": "dinov2.data.transforms",
        "description": "dinov2.data.transforms",
        "isExtraImport": true,
        "detail": "dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "make_classification_eval_transform",
        "importPath": "dinov2.data.transforms",
        "description": "dinov2.data.transforms",
        "isExtraImport": true,
        "detail": "dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "make_classification_eval_transform",
        "importPath": "dinov2.data.transforms",
        "description": "dinov2.data.transforms",
        "isExtraImport": true,
        "detail": "dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "make_classification_train_transform",
        "importPath": "dinov2.data.transforms",
        "description": "dinov2.data.transforms",
        "isExtraImport": true,
        "detail": "dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "make_classification_eval_transform",
        "importPath": "dinov2.data.transforms",
        "description": "dinov2.data.transforms",
        "isExtraImport": true,
        "detail": "dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "AccuracyAveraging",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_topk_accuracy_metric",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "MetricType",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_metric",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "MetricType",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_metric",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "AccuracyAveraging",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_topk_accuracy_metric",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "MetricType",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_metric",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "MetricType",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_metric",
        "importPath": "dinov2.eval.metrics",
        "description": "dinov2.eval.metrics",
        "isExtraImport": true,
        "detail": "dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "setup_and_build_model",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "setup_and_build_model",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "setup_and_build_model",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "setup_and_build_model",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "setup_and_build_model",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "setup_and_build_model",
        "importPath": "dinov2.eval.setup",
        "description": "dinov2.eval.setup",
        "isExtraImport": true,
        "detail": "dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "ModelWithNormalize",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "ModelWithIntermediateLayers",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "ModelWithNormalize",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "ModelWithIntermediateLayers",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "importPath": "dinov2.eval.utils",
        "description": "dinov2.eval.utils",
        "isExtraImport": true,
        "detail": "dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "DistributedDataParallel",
        "importPath": "torch.nn.parallel",
        "description": "torch.nn.parallel",
        "isExtraImport": true,
        "detail": "torch.nn.parallel",
        "documentation": {}
    },
    {
        "label": "DistributedDataParallel",
        "importPath": "torch.nn.parallel",
        "description": "torch.nn.parallel",
        "isExtraImport": true,
        "detail": "torch.nn.parallel",
        "documentation": {}
    },
    {
        "label": "Checkpointer",
        "importPath": "fvcore.common.checkpoint",
        "description": "fvcore.common.checkpoint",
        "isExtraImport": true,
        "detail": "fvcore.common.checkpoint",
        "documentation": {}
    },
    {
        "label": "PeriodicCheckpointer",
        "importPath": "fvcore.common.checkpoint",
        "description": "fvcore.common.checkpoint",
        "isExtraImport": true,
        "detail": "fvcore.common.checkpoint",
        "documentation": {}
    },
    {
        "label": "PeriodicCheckpointer",
        "importPath": "fvcore.common.checkpoint",
        "description": "fvcore.common.checkpoint",
        "isExtraImport": true,
        "detail": "fvcore.common.checkpoint",
        "documentation": {}
    },
    {
        "label": "Checkpointer",
        "importPath": "fvcore.common.checkpoint",
        "description": "fvcore.common.checkpoint",
        "isExtraImport": true,
        "detail": "fvcore.common.checkpoint",
        "documentation": {}
    },
    {
        "label": "PeriodicCheckpointer",
        "importPath": "fvcore.common.checkpoint",
        "description": "fvcore.common.checkpoint",
        "isExtraImport": true,
        "detail": "fvcore.common.checkpoint",
        "documentation": {}
    },
    {
        "label": "PeriodicCheckpointer",
        "importPath": "fvcore.common.checkpoint",
        "description": "fvcore.common.checkpoint",
        "isExtraImport": true,
        "detail": "fvcore.common.checkpoint",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "setup_logging",
        "importPath": "dinov2.logging",
        "description": "dinov2.logging",
        "isExtraImport": true,
        "detail": "dinov2.logging",
        "documentation": {}
    },
    {
        "label": "gc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gc",
        "description": "gc",
        "detail": "gc",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "cuml.linear_model",
        "description": "cuml.linear_model",
        "isExtraImport": true,
        "detail": "cuml.linear_model",
        "documentation": {}
    },
    {
        "label": "LogisticRegression",
        "importPath": "cuml.linear_model",
        "description": "cuml.linear_model",
        "isExtraImport": true,
        "detail": "cuml.linear_model",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.backends.cudnn",
        "description": "torch.backends.cudnn",
        "detail": "torch.backends.cudnn",
        "documentation": {}
    },
    {
        "label": "MetricTracker",
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "isExtraImport": true,
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "Metric",
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "isExtraImport": true,
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "MetricCollection",
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "isExtraImport": true,
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "MetricCollection",
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "isExtraImport": true,
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "MetricTracker",
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "isExtraImport": true,
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "Metric",
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "isExtraImport": true,
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "MetricCollection",
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "isExtraImport": true,
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "MetricCollection",
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "isExtraImport": true,
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "as_torch_dtype",
        "importPath": "dinov2.utils.dtype",
        "description": "dinov2.utils.dtype",
        "isExtraImport": true,
        "detail": "dinov2.utils.dtype",
        "documentation": {}
    },
    {
        "label": "as_torch_dtype",
        "importPath": "dinov2.utils.dtype",
        "description": "dinov2.utils.dtype",
        "isExtraImport": true,
        "detail": "dinov2.utils.dtype",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "MulticlassAccuracy",
        "importPath": "torchmetrics.classification",
        "description": "torchmetrics.classification",
        "isExtraImport": true,
        "detail": "torchmetrics.classification",
        "documentation": {}
    },
    {
        "label": "MulticlassAccuracy",
        "importPath": "torchmetrics.classification",
        "description": "torchmetrics.classification",
        "isExtraImport": true,
        "detail": "torchmetrics.classification",
        "documentation": {}
    },
    {
        "label": "dim_zero_cat",
        "importPath": "torchmetrics.utilities.data",
        "description": "torchmetrics.utilities.data",
        "isExtraImport": true,
        "detail": "torchmetrics.utilities.data",
        "documentation": {}
    },
    {
        "label": "select_topk",
        "importPath": "torchmetrics.utilities.data",
        "description": "torchmetrics.utilities.data",
        "isExtraImport": true,
        "detail": "torchmetrics.utilities.data",
        "documentation": {}
    },
    {
        "label": "dim_zero_cat",
        "importPath": "torchmetrics.utilities.data",
        "description": "torchmetrics.utilities.data",
        "isExtraImport": true,
        "detail": "torchmetrics.utilities.data",
        "documentation": {}
    },
    {
        "label": "select_topk",
        "importPath": "torchmetrics.utilities.data",
        "description": "torchmetrics.utilities.data",
        "isExtraImport": true,
        "detail": "torchmetrics.utilities.data",
        "documentation": {}
    },
    {
        "label": "build_model_from_cfg",
        "importPath": "dinov2.models",
        "description": "dinov2.models",
        "isExtraImport": true,
        "detail": "dinov2.models",
        "documentation": {}
    },
    {
        "label": "build_model_from_cfg",
        "importPath": "dinov2.models",
        "description": "dinov2.models",
        "isExtraImport": true,
        "detail": "dinov2.models",
        "documentation": {}
    },
    {
        "label": "build_model_from_cfg",
        "importPath": "dinov2.models",
        "description": "dinov2.models",
        "isExtraImport": true,
        "detail": "dinov2.models",
        "documentation": {}
    },
    {
        "label": "build_model_from_cfg",
        "importPath": "dinov2.models",
        "description": "dinov2.models",
        "isExtraImport": true,
        "detail": "dinov2.models",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "dinov2.utils.config",
        "description": "dinov2.utils.config",
        "isExtraImport": true,
        "detail": "dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "dinov2.utils.config",
        "description": "dinov2.utils.config",
        "isExtraImport": true,
        "detail": "dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "dinov2.utils.config",
        "description": "dinov2.utils.config",
        "isExtraImport": true,
        "detail": "dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "dinov2.utils.config",
        "description": "dinov2.utils.config",
        "isExtraImport": true,
        "detail": "dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "dinov2.utils.utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dinov2.utils.utils",
        "description": "dinov2.utils.utils",
        "detail": "dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "has_batchnorms",
        "importPath": "dinov2.utils.utils",
        "description": "dinov2.utils.utils",
        "isExtraImport": true,
        "detail": "dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "CosineScheduler",
        "importPath": "dinov2.utils.utils",
        "description": "dinov2.utils.utils",
        "isExtraImport": true,
        "detail": "dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "has_batchnorms",
        "importPath": "dinov2.utils.utils",
        "description": "dinov2.utils.utils",
        "isExtraImport": true,
        "detail": "dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "CosineScheduler",
        "importPath": "dinov2.utils.utils",
        "description": "dinov2.utils.utils",
        "isExtraImport": true,
        "detail": "dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "copy,pdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy.pdb",
        "description": "copy.pdb",
        "detail": "copy.pdb",
        "documentation": {}
    },
    {
        "label": "torch,pdb",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.pdb",
        "description": "torch.pdb",
        "detail": "torch.pdb",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFNFused",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "MemEffAttention",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "NestedTensorBlock",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "DINOHead",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFNFused",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "MemEffAttention",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "NestedTensorBlock",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "DINOHead",
        "importPath": "dinov2.layers",
        "description": "dinov2.layers",
        "isExtraImport": true,
        "detail": "dinov2.layers",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.knn",
        "description": "dinov2.eval.knn",
        "isExtraImport": true,
        "detail": "dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.knn",
        "description": "dinov2.eval.knn",
        "isExtraImport": true,
        "detail": "dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "submit_jobs",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "submit_jobs",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "submit_jobs",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "submit_jobs",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "submit_jobs",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "submit_jobs",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "submit_jobs",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "submit_jobs",
        "importPath": "dinov2.run.submit",
        "description": "dinov2.run.submit",
        "isExtraImport": true,
        "detail": "dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.linear",
        "description": "dinov2.eval.linear",
        "isExtraImport": true,
        "detail": "dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.linear",
        "description": "dinov2.eval.linear",
        "isExtraImport": true,
        "detail": "dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.log_regression",
        "description": "dinov2.eval.log_regression",
        "isExtraImport": true,
        "detail": "dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.eval.log_regression",
        "description": "dinov2.eval.log_regression",
        "isExtraImport": true,
        "detail": "dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.train",
        "description": "dinov2.train",
        "isExtraImport": true,
        "detail": "dinov2.train",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "importPath": "dinov2.train",
        "description": "dinov2.train",
        "isExtraImport": true,
        "detail": "dinov2.train",
        "documentation": {}
    },
    {
        "label": "submitit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "submitit",
        "description": "submitit",
        "detail": "submitit",
        "documentation": {}
    },
    {
        "label": "get_slurm_executor_parameters",
        "importPath": "dinov2.utils.cluster",
        "description": "dinov2.utils.cluster",
        "isExtraImport": true,
        "detail": "dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_slurm_partition",
        "importPath": "dinov2.utils.cluster",
        "description": "dinov2.utils.cluster",
        "isExtraImport": true,
        "detail": "dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_user_checkpoint_path",
        "importPath": "dinov2.utils.cluster",
        "description": "dinov2.utils.cluster",
        "isExtraImport": true,
        "detail": "dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_slurm_executor_parameters",
        "importPath": "dinov2.utils.cluster",
        "description": "dinov2.utils.cluster",
        "isExtraImport": true,
        "detail": "dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_slurm_partition",
        "importPath": "dinov2.utils.cluster",
        "description": "dinov2.utils.cluster",
        "isExtraImport": true,
        "detail": "dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_user_checkpoint_path",
        "importPath": "dinov2.utils.cluster",
        "description": "dinov2.utils.cluster",
        "isExtraImport": true,
        "detail": "dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "DINOLoss",
        "importPath": "dinov2.loss",
        "description": "dinov2.loss",
        "isExtraImport": true,
        "detail": "dinov2.loss",
        "documentation": {}
    },
    {
        "label": "iBOTPatchLoss",
        "importPath": "dinov2.loss",
        "description": "dinov2.loss",
        "isExtraImport": true,
        "detail": "dinov2.loss",
        "documentation": {}
    },
    {
        "label": "KoLeoLoss",
        "importPath": "dinov2.loss",
        "description": "dinov2.loss",
        "isExtraImport": true,
        "detail": "dinov2.loss",
        "documentation": {}
    },
    {
        "label": "DINOLoss",
        "importPath": "dinov2.loss",
        "description": "dinov2.loss",
        "isExtraImport": true,
        "detail": "dinov2.loss",
        "documentation": {}
    },
    {
        "label": "iBOTPatchLoss",
        "importPath": "dinov2.loss",
        "description": "dinov2.loss",
        "isExtraImport": true,
        "detail": "dinov2.loss",
        "documentation": {}
    },
    {
        "label": "KoLeoLoss",
        "importPath": "dinov2.loss",
        "description": "dinov2.loss",
        "isExtraImport": true,
        "detail": "dinov2.loss",
        "documentation": {}
    },
    {
        "label": "get_params_groups_with_decay",
        "importPath": "dinov2.utils.param_groups",
        "description": "dinov2.utils.param_groups",
        "isExtraImport": true,
        "detail": "dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "fuse_params_groups",
        "importPath": "dinov2.utils.param_groups",
        "description": "dinov2.utils.param_groups",
        "isExtraImport": true,
        "detail": "dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "get_params_groups_with_decay",
        "importPath": "dinov2.utils.param_groups",
        "description": "dinov2.utils.param_groups",
        "isExtraImport": true,
        "detail": "dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "fuse_params_groups",
        "importPath": "dinov2.utils.param_groups",
        "description": "dinov2.utils.param_groups",
        "isExtraImport": true,
        "detail": "dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "get_fsdp_wrapper",
        "importPath": "dinov2.fsdp",
        "description": "dinov2.fsdp",
        "isExtraImport": true,
        "detail": "dinov2.fsdp",
        "documentation": {}
    },
    {
        "label": "ShardedGradScaler",
        "importPath": "dinov2.fsdp",
        "description": "dinov2.fsdp",
        "isExtraImport": true,
        "detail": "dinov2.fsdp",
        "documentation": {}
    },
    {
        "label": "get_fsdp_modules",
        "importPath": "dinov2.fsdp",
        "description": "dinov2.fsdp",
        "isExtraImport": true,
        "detail": "dinov2.fsdp",
        "documentation": {}
    },
    {
        "label": "reshard_fsdp_model",
        "importPath": "dinov2.fsdp",
        "description": "dinov2.fsdp",
        "isExtraImport": true,
        "detail": "dinov2.fsdp",
        "documentation": {}
    },
    {
        "label": "FSDPCheckpointer",
        "importPath": "dinov2.fsdp",
        "description": "dinov2.fsdp",
        "isExtraImport": true,
        "detail": "dinov2.fsdp",
        "documentation": {}
    },
    {
        "label": "get_fsdp_wrapper",
        "importPath": "dinov2.fsdp",
        "description": "dinov2.fsdp",
        "isExtraImport": true,
        "detail": "dinov2.fsdp",
        "documentation": {}
    },
    {
        "label": "ShardedGradScaler",
        "importPath": "dinov2.fsdp",
        "description": "dinov2.fsdp",
        "isExtraImport": true,
        "detail": "dinov2.fsdp",
        "documentation": {}
    },
    {
        "label": "get_fsdp_modules",
        "importPath": "dinov2.fsdp",
        "description": "dinov2.fsdp",
        "isExtraImport": true,
        "detail": "dinov2.fsdp",
        "documentation": {}
    },
    {
        "label": "reshard_fsdp_model",
        "importPath": "dinov2.fsdp",
        "description": "dinov2.fsdp",
        "isExtraImport": true,
        "detail": "dinov2.fsdp",
        "documentation": {}
    },
    {
        "label": "FSDPCheckpointer",
        "importPath": "dinov2.fsdp",
        "description": "dinov2.fsdp",
        "isExtraImport": true,
        "detail": "dinov2.fsdp",
        "documentation": {}
    },
    {
        "label": "SSLMetaArch",
        "importPath": "dinov2.train.ssl_meta_arch",
        "description": "dinov2.train.ssl_meta_arch",
        "isExtraImport": true,
        "detail": "dinov2.train.ssl_meta_arch",
        "documentation": {}
    },
    {
        "label": "SSLMetaArch",
        "importPath": "dinov2.train.ssl_meta_arch",
        "description": "dinov2.train.ssl_meta_arch",
        "isExtraImport": true,
        "detail": "dinov2.train.ssl_meta_arch",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "OmegaConf",
        "importPath": "omegaconf",
        "description": "omegaconf",
        "isExtraImport": true,
        "detail": "omegaconf",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "dinov2.utils",
        "description": "dinov2.utils",
        "isExtraImport": true,
        "detail": "dinov2.utils",
        "documentation": {}
    },
    {
        "label": "utils",
        "importPath": "dinov2.utils",
        "description": "dinov2.utils",
        "isExtraImport": true,
        "detail": "dinov2.utils",
        "documentation": {}
    },
    {
        "label": "dinov2_default_config",
        "importPath": "dinov2.configs",
        "description": "dinov2.configs",
        "isExtraImport": true,
        "detail": "dinov2.configs",
        "documentation": {}
    },
    {
        "label": "dinov2_default_config",
        "importPath": "dinov2.configs",
        "description": "dinov2.configs",
        "isExtraImport": true,
        "detail": "dinov2.configs",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "open3d",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "open3d",
        "description": "open3d",
        "detail": "open3d",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "transformations",
        "description": "transformations",
        "isExtraImport": true,
        "detail": "transformations",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "transformations",
        "description": "transformations",
        "isExtraImport": true,
        "detail": "transformations",
        "documentation": {}
    },
    {
        "label": "os.path",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os.path",
        "description": "os.path",
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "stereo.utils",
        "description": "stereo.utils",
        "isExtraImport": true,
        "detail": "stereo.utils",
        "documentation": {}
    },
    {
        "label": "kornia",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "kornia",
        "description": "kornia",
        "detail": "kornia",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "antialiased_cnns",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "antialiased_cnns",
        "description": "antialiased_cnns",
        "detail": "antialiased_cnns",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "importPath": "timm.models.layers",
        "description": "timm.models.layers",
        "isExtraImport": true,
        "detail": "timm.models.layers",
        "documentation": {}
    },
    {
        "label": "_cfg",
        "importPath": "timm.models.mobilenetv3",
        "description": "timm.models.mobilenetv3",
        "isExtraImport": true,
        "detail": "timm.models.mobilenetv3",
        "documentation": {}
    },
    {
        "label": "_cfg",
        "importPath": "timm.models.mobilenetv3",
        "description": "timm.models.mobilenetv3",
        "isExtraImport": true,
        "detail": "timm.models.mobilenetv3",
        "documentation": {}
    },
    {
        "label": "epe_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "d1_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "threshold_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "epe_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "d1_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "threshold_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "epe_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "d1_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "threshold_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "epe_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "d1_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "threshold_metric",
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "isExtraImport": true,
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "torchvision.utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchvision.utils",
        "description": "torchvision.utils",
        "detail": "torchvision.utils",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "matplotlib.cm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.cm",
        "description": "matplotlib.cm",
        "detail": "matplotlib.cm",
        "documentation": {}
    },
    {
        "label": "torch,",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.",
        "description": "torch.",
        "detail": "torch.",
        "documentation": {}
    },
    {
        "label": "correlation_volume",
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "isExtraImport": true,
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_sub_volume",
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "isExtraImport": true,
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "InterlacedVolume",
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "isExtraImport": true,
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "isExtraImport": true,
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "isExtraImport": true,
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "correlation_volume",
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "isExtraImport": true,
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_sub_volume",
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "isExtraImport": true,
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "InterlacedVolume",
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "isExtraImport": true,
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "isExtraImport": true,
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "isExtraImport": true,
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "importPath": "stereo.modeling.disp_pred.disp_regression",
        "description": "stereo.modeling.disp_pred.disp_regression",
        "isExtraImport": true,
        "detail": "stereo.modeling.disp_pred.disp_regression",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "importPath": "stereo.modeling.disp_pred.disp_regression",
        "description": "stereo.modeling.disp_pred.disp_regression",
        "isExtraImport": true,
        "detail": "stereo.modeling.disp_pred.disp_regression",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "importPath": "stereo.modeling.disp_pred.disp_regression",
        "description": "stereo.modeling.disp_pred.disp_regression",
        "isExtraImport": true,
        "detail": "stereo.modeling.disp_pred.disp_regression",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "importPath": "stereo.modeling.disp_pred.disp_regression",
        "description": "stereo.modeling.disp_pred.disp_regression",
        "isExtraImport": true,
        "detail": "stereo.modeling.disp_pred.disp_regression",
        "documentation": {}
    },
    {
        "label": "context_upsample",
        "importPath": "stereo.modeling.disp_refinement.disp_refinement",
        "description": "stereo.modeling.disp_refinement.disp_refinement",
        "isExtraImport": true,
        "detail": "stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "context_upsample",
        "importPath": "stereo.modeling.disp_refinement.disp_refinement",
        "description": "stereo.modeling.disp_refinement.disp_refinement",
        "isExtraImport": true,
        "detail": "stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "CfgNode",
        "importPath": "yacs.config",
        "description": "yacs.config",
        "isExtraImport": true,
        "detail": "yacs.config",
        "documentation": {}
    },
    {
        "label": "CfgNode",
        "importPath": "yacs.config",
        "description": "yacs.config",
        "isExtraImport": true,
        "detail": "yacs.config",
        "documentation": {}
    },
    {
        "label": "MultiScaleDeformableAttention",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "MultiScaleDeformableAttention",
        "description": "MultiScaleDeformableAttention",
        "detail": "MultiScaleDeformableAttention",
        "documentation": {}
    },
    {
        "label": "MSDeformAttnFunction",
        "importPath": "functions.ms_deform_attn_func",
        "description": "functions.ms_deform_attn_func",
        "isExtraImport": true,
        "detail": "functions.ms_deform_attn_func",
        "documentation": {}
    },
    {
        "label": "ms_deform_attn_core_pytorch",
        "importPath": "functions.ms_deform_attn_func",
        "description": "functions.ms_deform_attn_func",
        "isExtraImport": true,
        "detail": "functions.ms_deform_attn_func",
        "documentation": {}
    },
    {
        "label": "MSDeformAttnFunction",
        "importPath": "functions.ms_deform_attn_func",
        "description": "functions.ms_deform_attn_func",
        "isExtraImport": true,
        "detail": "functions.ms_deform_attn_func",
        "documentation": {}
    },
    {
        "label": "ms_deform_attn_core_pytorch",
        "importPath": "functions.ms_deform_attn_func",
        "description": "functions.ms_deform_attn_func",
        "isExtraImport": true,
        "detail": "functions.ms_deform_attn_func",
        "documentation": {}
    },
    {
        "label": "torch.multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.multiprocessing",
        "description": "torch.multiprocessing",
        "detail": "torch.multiprocessing",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "log_every_n_seconds",
        "importPath": "nmrf.utils.logger",
        "description": "nmrf.utils.logger",
        "isExtraImport": true,
        "detail": "nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "log_every_n_seconds",
        "importPath": "nmrf.utils.logger",
        "description": "nmrf.utils.logger",
        "isExtraImport": true,
        "detail": "nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "dist_utils",
        "importPath": "nmrf.utils",
        "description": "nmrf.utils",
        "isExtraImport": true,
        "detail": "nmrf.utils",
        "documentation": {}
    },
    {
        "label": "frame_utils",
        "importPath": "nmrf.utils",
        "description": "nmrf.utils",
        "isExtraImport": true,
        "detail": "nmrf.utils",
        "documentation": {}
    },
    {
        "label": "dist_utils",
        "importPath": "nmrf.utils",
        "description": "nmrf.utils",
        "isExtraImport": true,
        "detail": "nmrf.utils",
        "documentation": {}
    },
    {
        "label": "frame_utils",
        "importPath": "nmrf.utils",
        "description": "nmrf.utils",
        "isExtraImport": true,
        "detail": "nmrf.utils",
        "documentation": {}
    },
    {
        "label": "colored",
        "importPath": "termcolor",
        "description": "termcolor",
        "isExtraImport": true,
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "colored",
        "importPath": "termcolor",
        "description": "termcolor",
        "isExtraImport": true,
        "detail": "termcolor",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "tabulate",
        "importPath": "tabulate",
        "description": "tabulate",
        "isExtraImport": true,
        "detail": "tabulate",
        "documentation": {}
    },
    {
        "label": "tabulate",
        "importPath": "tabulate",
        "description": "tabulate",
        "isExtraImport": true,
        "detail": "tabulate",
        "documentation": {}
    },
    {
        "label": "nmrf.utils.dist_utils",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "nmrf.utils.dist_utils",
        "description": "nmrf.utils.dist_utils",
        "detail": "nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "matplotlib.figure",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.figure",
        "description": "matplotlib.figure",
        "detail": "matplotlib.figure",
        "documentation": {}
    },
    {
        "label": "FigureCanvasAgg",
        "importPath": "matplotlib.backends.backend_agg",
        "description": "matplotlib.backends.backend_agg",
        "isExtraImport": true,
        "detail": "matplotlib.backends.backend_agg",
        "documentation": {}
    },
    {
        "label": "FigureCanvasAgg",
        "importPath": "matplotlib.backends.backend_agg",
        "description": "matplotlib.backends.backend_agg",
        "isExtraImport": true,
        "detail": "matplotlib.backends.backend_agg",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.layers",
        "description": "timm.layers",
        "isExtraImport": true,
        "detail": "timm.layers",
        "documentation": {}
    },
    {
        "label": "trunc_normal_",
        "importPath": "timm.layers",
        "description": "timm.layers",
        "isExtraImport": true,
        "detail": "timm.layers",
        "documentation": {}
    },
    {
        "label": "xformers.ops",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xformers.ops",
        "description": "xformers.ops",
        "detail": "xformers.ops",
        "documentation": {}
    },
    {
        "label": "load_state_dict_from_url",
        "importPath": "torch.hub",
        "description": "torch.hub",
        "isExtraImport": true,
        "detail": "torch.hub",
        "documentation": {}
    },
    {
        "label": "load_state_dict_from_url",
        "importPath": "torch.hub",
        "description": "torch.hub",
        "isExtraImport": true,
        "detail": "torch.hub",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "torch.jit.annotations",
        "description": "torch.jit.annotations",
        "isExtraImport": true,
        "detail": "torch.jit.annotations",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "torch.jit.annotations",
        "description": "torch.jit.annotations",
        "isExtraImport": true,
        "detail": "torch.jit.annotations",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "importPath": "torchvision.models.resnet",
        "description": "torchvision.models.resnet",
        "isExtraImport": true,
        "detail": "torchvision.models.resnet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "importPath": "torchvision.models.resnet",
        "description": "torchvision.models.resnet",
        "isExtraImport": true,
        "detail": "torchvision.models.resnet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "importPath": "torchvision.models.resnet",
        "description": "torchvision.models.resnet",
        "isExtraImport": true,
        "detail": "torchvision.models.resnet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "importPath": "torchvision.models.resnet",
        "description": "torchvision.models.resnet",
        "isExtraImport": true,
        "detail": "torchvision.models.resnet",
        "documentation": {}
    },
    {
        "label": "build_dataloader",
        "importPath": "stereo.datasets",
        "description": "stereo.datasets",
        "isExtraImport": true,
        "detail": "stereo.datasets",
        "documentation": {}
    },
    {
        "label": "build_dataloader",
        "importPath": "stereo.datasets",
        "description": "stereo.datasets",
        "isExtraImport": true,
        "detail": "stereo.datasets",
        "documentation": {}
    },
    {
        "label": "LinearWarmup",
        "importPath": "stereo.utils.warmup",
        "description": "stereo.utils.warmup",
        "isExtraImport": true,
        "detail": "stereo.utils.warmup",
        "documentation": {}
    },
    {
        "label": "LinearWarmup",
        "importPath": "stereo.utils.warmup",
        "description": "stereo.utils.warmup",
        "isExtraImport": true,
        "detail": "stereo.utils.warmup",
        "documentation": {}
    },
    {
        "label": "ClipGrad",
        "importPath": "stereo.utils.clip_grad",
        "description": "stereo.utils.clip_grad",
        "isExtraImport": true,
        "detail": "stereo.utils.clip_grad",
        "documentation": {}
    },
    {
        "label": "ClipGrad",
        "importPath": "stereo.utils.clip_grad",
        "description": "stereo.utils.clip_grad",
        "isExtraImport": true,
        "detail": "stereo.utils.clip_grad",
        "documentation": {}
    },
    {
        "label": "Lamb",
        "importPath": "stereo.utils.lamb",
        "description": "stereo.utils.lamb",
        "isExtraImport": true,
        "detail": "stereo.utils.lamb",
        "documentation": {}
    },
    {
        "label": "Lamb",
        "importPath": "stereo.utils.lamb",
        "description": "stereo.utils.lamb",
        "isExtraImport": true,
        "detail": "stereo.utils.lamb",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "Optimizer",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "Optimizer",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "Optimizer",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "Optimizer",
        "importPath": "torch.optim",
        "description": "torch.optim",
        "isExtraImport": true,
        "detail": "torch.optim",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "as_completed",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "ThreadPoolExecutor",
        "importPath": "concurrent.futures",
        "description": "concurrent.futures",
        "isExtraImport": true,
        "detail": "concurrent.futures",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "disp_to_color",
        "importPath": "stereo.utils.disp_color",
        "description": "stereo.utils.disp_color",
        "isExtraImport": true,
        "detail": "stereo.utils.disp_color",
        "documentation": {}
    },
    {
        "label": "disp_to_color",
        "importPath": "stereo.utils.disp_color",
        "description": "stereo.utils.disp_color",
        "isExtraImport": true,
        "detail": "stereo.utils.disp_color",
        "documentation": {}
    },
    {
        "label": "disp_to_color",
        "importPath": "stereo.utils.disp_color",
        "description": "stereo.utils.disp_color",
        "isExtraImport": true,
        "detail": "stereo.utils.disp_color",
        "documentation": {}
    },
    {
        "label": "disp_to_color",
        "importPath": "stereo.utils.disp_color",
        "description": "stereo.utils.disp_color",
        "isExtraImport": true,
        "detail": "stereo.utils.disp_color",
        "documentation": {}
    },
    {
        "label": "disp_to_color",
        "importPath": "stereo.utils.disp_color",
        "description": "stereo.utils.disp_color",
        "isExtraImport": true,
        "detail": "stereo.utils.disp_color",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "DATA_PATH_DICT",
        "importPath": "cfgs.data_basic",
        "description": "cfgs.data_basic",
        "isExtraImport": true,
        "detail": "cfgs.data_basic",
        "documentation": {}
    },
    {
        "label": "DATA_PATH_DICT",
        "importPath": "cfgs.data_basic",
        "description": "cfgs.data_basic",
        "isExtraImport": true,
        "detail": "cfgs.data_basic",
        "documentation": {}
    },
    {
        "label": "DATA_PATH_DICT",
        "importPath": "cfgs.data_basic",
        "description": "cfgs.data_basic",
        "isExtraImport": true,
        "detail": "cfgs.data_basic",
        "documentation": {}
    },
    {
        "label": "DATA_PATH_DICT",
        "importPath": "cfgs.data_basic",
        "description": "cfgs.data_basic",
        "isExtraImport": true,
        "detail": "cfgs.data_basic",
        "documentation": {}
    },
    {
        "label": "build_transform_by_cfg",
        "importPath": "stereo.datasets.dataset_template",
        "description": "stereo.datasets.dataset_template",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "DatasetTemplate",
        "importPath": "stereo.datasets.dataset_template",
        "description": "stereo.datasets.dataset_template",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "build_transform_by_cfg",
        "importPath": "stereo.datasets.dataset_template",
        "description": "stereo.datasets.dataset_template",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "build_transform_by_cfg",
        "importPath": "stereo.datasets.dataset_template",
        "description": "stereo.datasets.dataset_template",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "DatasetTemplate",
        "importPath": "stereo.datasets.dataset_template",
        "description": "stereo.datasets.dataset_template",
        "isExtraImport": true,
        "detail": "stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "thop",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "thop",
        "description": "thop",
        "detail": "thop",
        "documentation": {}
    },
    {
        "label": "quantize_static",
        "importPath": "onnxruntime.quantization",
        "description": "onnxruntime.quantization",
        "isExtraImport": true,
        "detail": "onnxruntime.quantization",
        "documentation": {}
    },
    {
        "label": "CalibrationDataReader",
        "importPath": "onnxruntime.quantization",
        "description": "onnxruntime.quantization",
        "isExtraImport": true,
        "detail": "onnxruntime.quantization",
        "documentation": {}
    },
    {
        "label": "QuantType",
        "importPath": "onnxruntime.quantization",
        "description": "onnxruntime.quantization",
        "isExtraImport": true,
        "detail": "onnxruntime.quantization",
        "documentation": {}
    },
    {
        "label": "quantize_static",
        "importPath": "onnxruntime.quantization",
        "description": "onnxruntime.quantization",
        "isExtraImport": true,
        "detail": "onnxruntime.quantization",
        "documentation": {}
    },
    {
        "label": "CalibrationDataReader",
        "importPath": "onnxruntime.quantization",
        "description": "onnxruntime.quantization",
        "isExtraImport": true,
        "detail": "onnxruntime.quantization",
        "documentation": {}
    },
    {
        "label": "QuantType",
        "importPath": "onnxruntime.quantization",
        "description": "onnxruntime.quantization",
        "isExtraImport": true,
        "detail": "onnxruntime.quantization",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "VisionDataset",
        "importPath": "torchvision.datasets",
        "description": "torchvision.datasets",
        "isExtraImport": true,
        "detail": "torchvision.datasets",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "GzipFile",
        "importPath": "gzip",
        "description": "gzip",
        "isExtraImport": true,
        "detail": "gzip",
        "documentation": {}
    },
    {
        "label": "ACCESS_READ",
        "importPath": "mmap",
        "description": "mmap",
        "isExtraImport": true,
        "detail": "mmap",
        "documentation": {}
    },
    {
        "label": "mmap",
        "importPath": "mmap",
        "description": "mmap",
        "isExtraImport": true,
        "detail": "mmap",
        "documentation": {}
    },
    {
        "label": "res",
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "isExtraImport": true,
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "compute_avgerr",
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "isExtraImport": true,
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "compute_rmse",
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "isExtraImport": true,
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "get_gpu_memory_global",
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "isExtraImport": true,
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "get_memory_usage",
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "isExtraImport": true,
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "get_model_size",
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "isExtraImport": true,
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "TrainingMode",
        "importPath": "torch.onnx",
        "description": "torch.onnx",
        "isExtraImport": true,
        "detail": "torch.onnx",
        "documentation": {}
    },
    {
        "label": "tensorrt",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorrt",
        "description": "tensorrt",
        "detail": "tensorrt",
        "documentation": {}
    },
    {
        "label": "pycuda.driver",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycuda.driver",
        "description": "pycuda.driver",
        "detail": "pycuda.driver",
        "documentation": {}
    },
    {
        "label": "pycuda.autoinit",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pycuda.autoinit",
        "description": "pycuda.autoinit",
        "detail": "pycuda.autoinit",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "pynvml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pynvml",
        "description": "pynvml",
        "detail": "pynvml",
        "documentation": {}
    },
    {
        "label": "DATA_PATH_DICT",
        "kind": 5,
        "importPath": "cfgs.data_basic",
        "description": "cfgs.data_basic",
        "peekOfCode": "DATA_PATH_DICT = {\n    'SceneFlowDataset': '/media/wanglin/Elements/datasets/SceneFlow',\n    # 'SceneFlowDataset': '/file_system/vepfs/public_data/SceneFlow',\n    'FlyingThings3DSubsetDataset': '/mnt/nas/algorithm/chenming.zhang/dataset/SceneFlow',\n    'KittiDataset12': '/home/lc/gaoshan/Workspace/dataset/kitti/dataset2012',\n    'KittiDataset15': '/home/lc/gaoshan/Workspace/KITTI/eval/kitti_dataset',\n    'DrivingDataset': '/mnt/nas/algorithm/chenming.zhang/dataset/DrivingStereo',\n    'MiddleburyDataset': '/home/lc/gaoshan/Workspace/dataset',\n    'ETH3DDataset': '/home/lc/gaoshan/Workspace/dataset/ETH3D',\n    'ArgoverseDataset': '/file_system/vepfs/public_data/stereo/argovers1_1',",
        "detail": "cfgs.data_basic",
        "documentation": {}
    },
    {
        "label": "Profile",
        "kind": 6,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "class Profile(contextlib.ContextDecorator):\n    # Profile class. Usage: @Profile() decorator or 'with Profile():' context manager\n    def __init__(self, t=0.0):\n        self.t = t\n        self.cuda = torch.cuda.is_available()\n    def __enter__(self):\n        self.start = self.time()\n        return self\n    def __exit__(self, type, value, traceback):\n        self.dt = self.time() - self.start  # delta-time",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)\n    else:  # list i.e. img_size=[640, 480]\n        imgsz = list(imgsz)  # convert to list if tuple\n        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]\n    if new_size != imgsz:",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_python",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def check_python(minimum='3.7.0'):\n    # Check current python version vs. required python version\n    check_version(platform.python_version(), minimum, name='Python ', hard=True)\ndef check_version(current='0.0.0', minimum='0.0.0', name='version ', pinned=False, hard=False, verbose=False, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Check version vs. required version\n    current, minimum = (pkg.parse_version(x) for x in (current, minimum))\n    result = (current == minimum) if pinned else (current >= minimum)  # bool\n    s = f'WARNING  {name}{minimum} is required by OpenStereo, but {name}{current} is currently installed'  # string",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_version",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def check_version(current='0.0.0', minimum='0.0.0', name='version ', pinned=False, hard=False, verbose=False, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Check version vs. required version\n    current, minimum = (pkg.parse_version(x) for x in (current, minimum))\n    result = (current == minimum) if pinned else (current >= minimum)  # bool\n    s = f'WARNING  {name}{minimum} is required by OpenStereo, but {name}{current} is currently installed'  # string\n    if hard:\n        assert result, emojis(s)  # assert min requirements met\n    if verbose and not result:",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def check_requirements(requirements=ROOT / 'requirements.txt', exclude=(), install=True, cmds='', logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Check installed dependencies meet requirements (pass *.txt file or list of packages or single package str)\n    prefix = colorstr('red', 'bold', 'requirements:')\n    check_python()  # check python version\n    if isinstance(requirements, Path):  # requirements.txt file\n        file = requirements.resolve()\n        assert file.exists(), f\"{prefix} {file} not found, check failed.\"\n        with file.open() as f:",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def colorstr(*input):\n    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n    colors = {\n        'black': '\\033[30m',  # basic colors\n        'red': '\\033[31m',\n        'green': '\\033[32m',\n        'yellow': '\\033[33m',\n        'blue': '\\033[34m',\n        'magenta': '\\033[35m',",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "file_size",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def file_size(path):\n    # Return file/dir size (MB)\n    mb = 1 << 20  # bytes to MiB (1024 ** 2)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / mb\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / mb\n    else:\n        return 0.0",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "get_default_args",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def get_default_args(func):\n    # Get func() default arguments\n    signature = inspect.signature(func)\n    return {k: v.default for k, v in signature.parameters.items() if v.default is not inspect.Parameter.empty}\ndef get_format_idx(df, format_name):\n    # pandas export\n    check_requirements('pandas')\n    import pandas as pd\n    idx = df.loc[df['Format'] == format_name].index\n    return idx[0] if not idx.empty else -1 ",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "get_format_idx",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def get_format_idx(df, format_name):\n    # pandas export\n    check_requirements('pandas')\n    import pandas as pd\n    idx = df.loc[df['Format'] == format_name].index\n    return idx[0] if not idx.empty else -1 \ndef load_model(model, weights, device=None, inplace=True):\n    from torch import nn\n    ckpt = torch.load(weights, map_location=device)\n    state_dict = ckpt['model_state'] if 'model_state' in ckpt else ckpt",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def load_model(model, weights, device=None, inplace=True):\n    from torch import nn\n    ckpt = torch.load(weights, map_location=device)\n    state_dict = ckpt['model_state'] if 'model_state' in ckpt else ckpt\n    model.load_state_dict(state_dict)\n    # Module compatibility updates\n    for m in model.modules():\n        t = type(m)\n        if t in (nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU):\n            m.inplace = inplace  # torch 1.7.0 compatibility",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "print_args",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def print_args(args: Optional[dict] = None, show_file=True, show_func=False, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Print function arguments (optional args dict)\n    x = inspect.currentframe().f_back  # previous frame\n    file, _, func, _, _ = inspect.getframeinfo(x)\n    if args is None:  # get args automatically\n        args, _, _, frm = inspect.getargvalues(x)\n        args = {k: v for k, v in frm.items() if k in args}\n    try:",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "url2file",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def url2file(url):\n    # Convert URL to filename, i.e. https://url.com/file.txt?auth -> file.txt\n    url = str(Path(url)).replace(':/', '://')  # Pathlib turns :// -> :/\n    return Path(urllib.parse.unquote(url)).name.split('?')[0]  # '%2F' to '/', split https://url.com/file.txt?auth\ndef yaml_save(file='data.yaml', data={}):\n    # Single-line safe yaml saving\n    with open(file, 'w') as f:\n        yaml.safe_dump({k: str(v) if isinstance(v, Path) else v for k, v in data.items()}, f, sort_keys=False)\nclass Profile(contextlib.ContextDecorator):\n    # Profile class. Usage: @Profile() decorator or 'with Profile():' context manager",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "yaml_save",
        "kind": 2,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "def yaml_save(file='data.yaml', data={}):\n    # Single-line safe yaml saving\n    with open(file, 'w') as f:\n        yaml.safe_dump({k: str(v) if isinstance(v, Path) else v for k, v in data.items()}, f, sort_keys=False)\nclass Profile(contextlib.ContextDecorator):\n    # Profile class. Usage: @Profile() decorator or 'with Profile():' context manager\n    def __init__(self, t=0.0):\n        self.t = t\n        self.cuda = torch.cuda.is_available()\n    def __enter__(self):",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # root directory\n# Settings\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of multiprocessing threads\nAUTOINSTALL = str(os.getenv('OPENSTEREO_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('OPENSTEREO_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "ROOT = FILE.parents[1]  # root directory\n# Settings\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of multiprocessing threads\nAUTOINSTALL = str(os.getenv('OPENSTEREO_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('OPENSTEREO_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "NUM_THREADS",
        "kind": 5,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "NUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of multiprocessing threads\nAUTOINSTALL = str(os.getenv('OPENSTEREO_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('OPENSTEREO_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "AUTOINSTALL",
        "kind": 5,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "AUTOINSTALL = str(os.getenv('OPENSTEREO_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('OPENSTEREO_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "VERBOSE",
        "kind": 5,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "VERBOSE = str(os.getenv('OPENSTEREO_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "kind": 5,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "TQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "FONT",
        "kind": 5,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "FONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "pd.options.display.max_columns",
        "kind": 5,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "pd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "os.environ['NUMEXPR_MAX_THREADS']",
        "kind": 5,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "os.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)\n    else:  # list i.e. img_size=[640, 480]\n        imgsz = list(imgsz)  # convert to list if tuple",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "os.environ['OMP_NUM_THREADS']",
        "kind": 5,
        "importPath": "deploy.deploy_utils",
        "description": "deploy.deploy_utils",
        "peekOfCode": "os.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)\n    else:  # list i.e. img_size=[640, 480]\n        imgsz = list(imgsz)  # convert to list if tuple\n        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]",
        "detail": "deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "export_onnx",
        "kind": 2,
        "importPath": "deploy.exp_onnx",
        "description": "deploy.exp_onnx",
        "peekOfCode": "def export_onnx(model, inputs, weights, opset=12, dynamic=False, simplify=True):\n    \"\"\"ONNX \"\"\"\n    import onnx\n    f = Path(weights).with_suffix('.onnx')\n    input_names = ['left_img', 'right_img']\n    output_names = ['disp_pred']\n    torch.onnx.export(\n        model,\n        {'data': inputs},\n        f,",
        "detail": "deploy.exp_onnx",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "deploy.exp_onnx",
        "description": "deploy.exp_onnx",
        "peekOfCode": "def run(config, weights, imgsz=(256, 512), batch_size=1, device='cpu',\n        half=False, dynamic=False, simplify=True, opset=12):\n    # \n    yaml_config = config_loader(config)\n    cfgs = EasyDict(yaml_config)\n    if not os.path.isfile(weights):\n        raise FileNotFoundError(f\": {weights}\")\n    model = load_model(LightStereo(cfgs.MODEL), weights)\n    # \n    h, w = imgsz",
        "detail": "deploy.exp_onnx",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "deploy.exp_onnx",
        "description": "deploy.exp_onnx",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, required=True, help=' config.yaml')\n    parser.add_argument('--weights', type=str, required=True, help='')\n    parser.add_argument('--imgsz', nargs=2, type=int, default=[256, 512], help=' (h w)')\n    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n    parser.add_argument('--device', default='cuda:0', help='cuda:0  cpu')\n    parser.add_argument('--simplify', action='store_true', help=' onnx-simplifier ')\n    parser.add_argument('--opset', type=int, default=12, help='ONNX opset version')\n    return parser.parse_args()",
        "detail": "deploy.exp_onnx",
        "documentation": {}
    },
    {
        "label": "export_formats",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def export_formats():\n    # export formats\n    x = [\n        ['PyTorch', '-', '.pt', True, True],\n        ['TorchScript', 'torchscript', '.torchscript', True, True],\n        ['ONNX', 'onnx', '.onnx', True, True],\n        # ['ONNX END2END', 'onnx_end2end', '_end2end.onnx', True, True],\n        ['OpenVINO', 'openvino', '_openvino_model', True, False],\n        ['TensorRT', 'engine', '.engine', False, True],\n        ['CoreML', 'coreml', '.mlmodel', True, False],",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "try_export",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def try_export(inner_func):\n    # export decorator, i..e @try_export\n    inner_args = get_default_args(inner_func)\n    def outer_func(*args, **kwargs):\n        prefix = inner_args['prefix']\n        try:\n            with Profile() as dt:\n                f, model = inner_func(*args, **kwargs)\n            logger.info(f'{prefix} export success  {dt.t:.1f}s, saved as {f} ({file_size(f):.1f} MB)')\n            return f, model",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "export_torchscript",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def export_torchscript(model, inputs, file, optimize, prefix=colorstr('TorchScript:')):\n    logger.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\n    f = Path(file).with_suffix('.torchscript')\n    ts = torch.jit.trace(model, inputs, strict=False)\n    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n        inputs['ref_img'].cpu()\n        inputs['tgt_img'].cpu()\n        model.cpu()\n        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f))\n    else:",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "export_onnx",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def export_onnx(model, inputs, weights, opset, dynamic, simplify, prefix=colorstr('ONNX:')):\n    # ONNX export\n    check_requirements('onnx', logger)\n    import onnx\n    logger.info(f'{prefix} starting export with onnx {onnx.__version__}...')\n    f = Path(weights).with_suffix('.onnx')\n    input_names = ['left_img', 'right_img']\n    output_names =  ['disp_pred']\n    if dynamic:\n        dynamic = {'left_img': {0: 'batch', 2: 'height', 3: 'width'},",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "export_openvino",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def export_openvino(file, half, prefix=colorstr('OpenVINO:')):\n    # OpenVINO export\n    check_requirements('openvino-dev', logger)  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino\n    logger.info(f'{prefix} starting export with openvino {openvino.__version__}...')\n    f = str(file).replace('.pt', f'_openvino_model{os.sep}')\n    half_arg = '--compress_to_fp16'\n    half_arg += '=True' if half else '=False'\n    cmd = f\"mo --input_model {Path(file).with_suffix('.onnx')} --output_dir {f} {half_arg}\"\n    subprocess.run(cmd.split(), check=True, env=os.environ)  # export",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "export_engine",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def export_engine(model, inputs, file, half, dynamic, simplify, optimize, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n    # TensorRT export https://developer.nvidia.com/tensorrt\n    assert inputs['left'].device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `python export.py --device 0`'\n    try:\n        import tensorrt as trt\n        import modelopt\n    except Exception:\n        if platform.system() == 'Linux':\n            check_requirements('tensorrt', logger, cmds='-U --index-url https://pypi.ngc.nvidia.com')\n            check_requirements('nvidia-modelopt[all]~=0.11.0', logger, cmds='--extra-index-url https://pypi.nvidia.com')",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "export_paddle",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def export_paddle(model, inputs, file, prefix=colorstr('PaddlePaddle:')):\n    # Paddle export\n    check_requirements(('paddlepaddle', 'x2paddle'), logger, cmds='-i https://pypi.tuna.tsinghua.edu.cn/simple')\n    import x2paddle\n    from x2paddle.convert import pytorch2paddle\n    logger.info(f'{prefix} starting export with X2Paddle {x2paddle.__version__}...')\n    f = str(file).replace('.pt', f'_paddle_model{os.sep}')\n    pytorch2paddle(module=model, save_dir=f, jit_type='trace', input_examples=[inputs])  # export\n    return f, None\n@try_export",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "export_coreml",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def export_coreml(model, im, file, int8, half, prefix=colorstr('CoreML:')):\n    # CoreML export\n    check_requirements('coremltools', logger, cmds='-i https://pypi.tuna.tsinghua.edu.cn/simple')\n    import coremltools as ct\n    logger.info(f'{prefix} starting export with coremltools {ct.__version__}...')\n    f = Path(file).with_suffix('.mlmodel')\n    ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n    ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im['left'].shape, scale=1 / 255, bias=[0, 0, 0])])\n    bits, mode = (8, 'kmeans_lut') if int8 else (16, 'linear') if half else (32, None)\n    if bits < 32:",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def run(\n        config=ROOT / '../../cfgs/psmnet/psmnet_kitti15.yaml',                                                                 # 'config.yaml path'\n        weights=ROOT / '../../output/KITTI2015/PSMNet/PSMNet_SceneFlow/checkpoints/PSMNet_SceneFlow_epoch_latest.pt',           # weights path\n        imgsz=(256, 512),                   # image (height, width)\n        batch_size=1,                       # batch size\n        device='cpu',                       # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        include=('torchscript', 'onnx'),    # include formats\n        half=False,                         # FP16 half-precision export\n        optimize=False,                     # TorchScript: optimize for mobile\n        int8=False,                         # CoreML INT8 quantization",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, default=ROOT / '../../cfgs/psmnet/psmnet_kitti15.yaml', help='<config>.yaml path')\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / '../../output/KITTI2015/PSMNet/PSMNet_SceneFlow/checkpoints/PSMNet_SceneFlow_epoch_latest.pt', help='model.pt path(s)')\n    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[256, 512], help='image size (h, w)')\n    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')\n    parser.add_argument('--optimize', action='store_true', help='TorchScript: optimize for mobile / TensorRT: optimize inference')\n    parser.add_argument('--int8', action='store_true', help='CoreML INT8 quantization')",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "def main(opt):\n    for opt.weights in (opt.weights if isinstance(opt.weights, list) else [opt.weights]):\n        run(**vars(opt))\nif __name__ == \"__main__\":\n    opt = parse_opt()\n    main(opt)",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root directory\nMODEL_NAME = ''\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != 'Windows':\n    RELATIVAE_ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom deploy_utils import (check_img_size, check_requirements, check_version, colorstr, file_size, get_default_args,\n                          get_format_idx, load_model, print_args, url2file, yaml_save, Profile)\nMACOS = platform.system() == 'Darwin'  # macOS environment",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "ROOT = FILE.parents[0]  # root directory\nMODEL_NAME = ''\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != 'Windows':\n    RELATIVAE_ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom deploy_utils import (check_img_size, check_requirements, check_version, colorstr, file_size, get_default_args,\n                          get_format_idx, load_model, print_args, url2file, yaml_save, Profile)\nMACOS = platform.system() == 'Darwin'  # macOS environment\nimport os",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "MODEL_NAME = ''\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != 'Windows':\n    RELATIVAE_ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom deploy_utils import (check_img_size, check_requirements, check_version, colorstr, file_size, get_default_args,\n                          get_format_idx, load_model, print_args, url2file, yaml_save, Profile)\nMACOS = platform.system() == 'Darwin'  # macOS environment\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "MACOS",
        "kind": 5,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "MACOS = platform.system() == 'Darwin'  # macOS environment\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n# Network\nsys.path.append(str(ROOT.parent))  # add ROOT to PATH\nfrom stereo.modeling import models, build_trainer\nfrom stereo.utils.common_utils import config_loader, create_logger, load_params_from_file\nfrom stereo.modeling.models.sttr.sttr import STTR\nfrom stereo.modeling.models.psmnet.psmnet import PSMNet\nfrom stereo.modeling.models.msnet.MSNet2D import MSNet2D",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "os.environ['PYTORCH_CUDA_ALLOC_CONF']",
        "kind": 5,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n# Network\nsys.path.append(str(ROOT.parent))  # add ROOT to PATH\nfrom stereo.modeling import models, build_trainer\nfrom stereo.utils.common_utils import config_loader, create_logger, load_params_from_file\nfrom stereo.modeling.models.sttr.sttr import STTR\nfrom stereo.modeling.models.psmnet.psmnet import PSMNet\nfrom stereo.modeling.models.msnet.MSNet2D import MSNet2D\nfrom stereo.modeling.models.msnet.MSNet3D import MSNet3D\nfrom stereo.modeling.models.igev.igev_stereo import IGEVStereo as IGEV",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "__net__",
        "kind": 5,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "__net__ = {\n    'STTR': STTR,\n    'PSMNet': PSMNet,\n    'MSNet2D': MSNet2D,\n    'MSNet3D': MSNet3D,\n    'IGEV': IGEV,\n    'GwcNet': GwcNet,\n    'FADNet': FADNet,\n    'CoExNet': CoEx,\n    # 'AANet': AANet,",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "deploy.export",
        "description": "deploy.export",
        "peekOfCode": "logger = logging.getLogger('export')\nlogging.basicConfig(level=logging.DEBUG, \n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\ndef export_formats():\n    # export formats\n    x = [\n        ['PyTorch', '-', '.pt', True, True],\n        ['TorchScript', 'torchscript', '.torchscript', True, True],\n        ['ONNX', 'onnx', '.onnx', True, True],\n        # ['ONNX END2END', 'onnx_end2end', '_end2end.onnx', True, True],",
        "detail": "deploy.export",
        "documentation": {}
    },
    {
        "label": "ExportWrapper",
        "kind": 6,
        "importPath": "deploy.export_onnx",
        "description": "deploy.export_onnx",
        "peekOfCode": "class ExportWrapper(nn.Module):\n    def __init__(self, model, device=\"cuda\"):\n        super().__init__()\n        self.model = model\n        # self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406], device=device).view(3, 1, 1))\n        # self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225], device=device).view(3, 1, 1))\n    def forward(self, left_img, right_img):\n       # HWC -> CHW, float 0-1\n        left = left_img.permute(2, 0, 1).float() / 255.0\n        right = right_img.permute(2, 0, 1).float() / 255.0",
        "detail": "deploy.export_onnx",
        "documentation": {}
    },
    {
        "label": "export_onnx",
        "kind": 2,
        "importPath": "deploy.export_onnx",
        "description": "deploy.export_onnx",
        "peekOfCode": "def export_onnx(model, dummy_left, dummy_right, weights, opset=12, dynamic=False, simplify=True):\n    \"\"\"ONNX \"\"\"\n    import onnx\n    f = Path(weights).with_suffix('.onnx')\n    input_names = ['left_img', 'right_img']\n    output_names = ['disp_pred']\n    torch.onnx.export(\n        model,\n        (dummy_left, dummy_right),   #  tuple dict\n        f,",
        "detail": "deploy.export_onnx",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "deploy.export_onnx",
        "description": "deploy.export_onnx",
        "peekOfCode": "def run(config, weights, imgsz=(256, 512), device='cpu',\n        dynamic=False, simplify=True, opset=12):\n    # \n    yaml_config = config_loader(config)\n    cfgs = EasyDict(yaml_config)\n    if not os.path.isfile(weights):\n        raise FileNotFoundError(f\": {weights}\")\n    base_model = load_model(LightStereo(cfgs.MODEL), weights)\n    # \n    model = ExportWrapper(base_model).eval().to(device)",
        "detail": "deploy.export_onnx",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "deploy.export_onnx",
        "description": "deploy.export_onnx",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, required=True, help=' config.yaml')\n    parser.add_argument('--weights', type=str, required=True, help='')\n    parser.add_argument('--imgsz', nargs=2, type=int, default=[256, 512], help=' (h w)')\n    parser.add_argument('--device', default='cuda:0', help='cuda:0  cpu')\n    parser.add_argument('--dynamic', action='store_true', help='ONNX/TensorRT: dynamic axes')\n    parser.add_argument('--simplify', action='store_true', help=' onnx-simplifier ')\n    parser.add_argument('--opset', type=int, default=12, help='ONNX opset version')\n    return parser.parse_args()",
        "detail": "deploy.export_onnx",
        "documentation": {}
    },
    {
        "label": "KITTIStereoDataset",
        "kind": 6,
        "importPath": "mgz.custom_loader",
        "description": "mgz.custom_loader",
        "peekOfCode": "class KITTIStereoDataset(Dataset):\n    def __init__(self, root, transform=None, resize=(512, 256)):\n        self.left_images = sorted(glob.glob(os.path.join(root, \"Image0\", \"*.jpg\")))\n        self.right_images = sorted(glob.glob(os.path.join(root, \"Image1\", \"*.jpg\")))\n        assert len(self.left_images) == len(self.right_images), \"\"\n        self.resize = resize\n        self.transform = transform\n    def __len__(self):\n        return len(self.left_images)\n    def __getitem__(self, idx):",
        "detail": "mgz.custom_loader",
        "documentation": {}
    },
    {
        "label": "custom_dataloader",
        "kind": 2,
        "importPath": "mgz.custom_loader",
        "description": "mgz.custom_loader",
        "peekOfCode": "def custom_dataloader(root,num_samples):\n    normalize_mean=[0.485, 0.456, 0.406]\n    normalize_std=[0.229, 0.224, 0.225]\n    transform = transforms.Compose([\n        transforms.Resize((256, 512)), \n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])\n    dataset = KITTIStereoDataset(root=root, transform=transform)\n    sampler = RandomSampler(dataset, replacement=True, num_samples=num_samples)",
        "detail": "mgz.custom_loader",
        "documentation": {}
    },
    {
        "label": "KITTIStereoDataset",
        "kind": 6,
        "importPath": "mgz.kitti_loader",
        "description": "mgz.kitti_loader",
        "peekOfCode": "class KITTIStereoDataset(Dataset):\n    \"\"\"\n    KITTI  (H, W, C)\n    \"\"\"\n    def __init__(self, root, resize=(512, 256)):\n        self.left_images = sorted(glob.glob(os.path.join(root, \"image_2\", \"*.png\")))\n        self.right_images = sorted(glob.glob(os.path.join(root, \"image_3\", \"*.png\")))\n        assert len(self.left_images) == len(self.right_images), \"\"\n        self.resize = resize\n    def __len__(self):",
        "detail": "mgz.kitti_loader",
        "documentation": {}
    },
    {
        "label": "SingleSampleDataLoader",
        "kind": 6,
        "importPath": "mgz.kitti_loader",
        "description": "mgz.kitti_loader",
        "peekOfCode": "class SingleSampleDataLoader:\n    \"\"\"\n     Dataset (H, W, C)\n    \"\"\"\n    def __init__(self, dataset, num_samples=None):\n        self.dataset = dataset\n        self.num_samples = num_samples if num_samples is not None else len(dataset)\n        self.indices = torch.randperm(len(dataset))[:self.num_samples]\n    def __iter__(self):\n        for idx in self.indices:",
        "detail": "mgz.kitti_loader",
        "documentation": {}
    },
    {
        "label": "custom_dataloader",
        "kind": 2,
        "importPath": "mgz.kitti_loader",
        "description": "mgz.kitti_loader",
        "peekOfCode": "def custom_dataloader(root, num_samples=10):\n    \"\"\"\n     KITTI  (H, W, C)\n    \"\"\"\n    dataset = KITTIStereoDataset(root=root)\n    loader = SingleSampleDataLoader(dataset, num_samples=num_samples)\n    return loader\nif __name__ == \"__main__\":\n    print('main')\n    # apt-get install -y libglib2.0-0 libglib2.0-dev",
        "detail": "mgz.kitti_loader",
        "documentation": {}
    },
    {
        "label": "initializer_to_numpy_map",
        "kind": 2,
        "importPath": "onnx.onnx_fuse",
        "description": "onnx.onnx_fuse",
        "peekOfCode": "def initializer_to_numpy_map(graph: GraphProto) -> Dict[str, np.ndarray]:\n    \"\"\" graph.initializer ->numpy  dict\"\"\"\n    m = {}\n    for init in graph.initializer:\n        arr = numpy_helper.to_array(init)\n        m[init.name] = arr\n    return m\ndef find_node_by_output_name(graph: GraphProto, out_name: str) -> Optional[NodeProto]:\n    for node in graph.node:\n        if out_name in node.output:",
        "detail": "onnx.onnx_fuse",
        "documentation": {}
    },
    {
        "label": "find_node_by_output_name",
        "kind": 2,
        "importPath": "onnx.onnx_fuse",
        "description": "onnx.onnx_fuse",
        "peekOfCode": "def find_node_by_output_name(graph: GraphProto, out_name: str) -> Optional[NodeProto]:\n    for node in graph.node:\n        if out_name in node.output:\n            return node\n    return None\ndef consumers_of_tensor(graph: GraphProto, tensor_name: str) -> List[NodeProto]:\n    \"\"\" tensor_name \"\"\"\n    out = []\n    for n in graph.node:\n        if tensor_name in n.input:",
        "detail": "onnx.onnx_fuse",
        "documentation": {}
    },
    {
        "label": "consumers_of_tensor",
        "kind": 2,
        "importPath": "onnx.onnx_fuse",
        "description": "onnx.onnx_fuse",
        "peekOfCode": "def consumers_of_tensor(graph: GraphProto, tensor_name: str) -> List[NodeProto]:\n    \"\"\" tensor_name \"\"\"\n    out = []\n    for n in graph.node:\n        if tensor_name in n.input:\n            out.append(n)\n    return out\ndef remove_node(graph: GraphProto, node: NodeProto):\n    \"\"\" graph.node  NodeProto identity \"\"\"\n    nodes = [n for n in graph.node if n is not node]",
        "detail": "onnx.onnx_fuse",
        "documentation": {}
    },
    {
        "label": "remove_node",
        "kind": 2,
        "importPath": "onnx.onnx_fuse",
        "description": "onnx.onnx_fuse",
        "peekOfCode": "def remove_node(graph: GraphProto, node: NodeProto):\n    \"\"\" graph.node  NodeProto identity \"\"\"\n    nodes = [n for n in graph.node if n is not node]\n    del graph.node[:]\n    graph.node.extend(nodes)\ndef replace_initializer(graph: GraphProto, name: str, np_arr: np.ndarray):\n    \"\"\" initializer name\"\"\"\n    # check exists\n    for i, init in enumerate(graph.initializer):\n        if init.name == name:",
        "detail": "onnx.onnx_fuse",
        "documentation": {}
    },
    {
        "label": "replace_initializer",
        "kind": 2,
        "importPath": "onnx.onnx_fuse",
        "description": "onnx.onnx_fuse",
        "peekOfCode": "def replace_initializer(graph: GraphProto, name: str, np_arr: np.ndarray):\n    \"\"\" initializer name\"\"\"\n    # check exists\n    for i, init in enumerate(graph.initializer):\n        if init.name == name:\n            graph.initializer[i].raw_data = numpy_helper.from_array(np_arr, name).raw_data\n            graph.initializer[i].data_type = numpy_helper.from_array(np_arr, name).data_type\n            graph.initializer[i].dims[:] = list(np_arr.shape)\n            return\n    # not found -> append",
        "detail": "onnx.onnx_fuse",
        "documentation": {}
    },
    {
        "label": "fold_batchnorm_into_conv",
        "kind": 2,
        "importPath": "onnx.onnx_fuse",
        "description": "onnx.onnx_fuse",
        "peekOfCode": "def fold_batchnorm_into_conv(model: ModelProto) -> Tuple[ModelProto, int]:\n    \"\"\"\n     graph Conv -> BatchNormalization  BN  Conv\n      y = Conv(x, W, b)  # b may be absent\n      y_bn = scale * (y - mean) / sqrt(var + eps) + B\n     W'  b' BatchNormalization  init BN \n     (new_model, fused_count)\n    \"\"\"\n    graph = model.graph\n    name_to_np = initializer_to_numpy_map(graph)",
        "detail": "onnx.onnx_fuse",
        "documentation": {}
    },
    {
        "label": "detect_conv_relu_and_mark",
        "kind": 2,
        "importPath": "onnx.onnx_fuse",
        "description": "onnx.onnx_fuse",
        "peekOfCode": "def detect_conv_relu_and_mark(graph: GraphProto) -> int:\n    \"\"\"\n     Conv->Relu  Relu  Conv node  attribute  annotation 'fused_activation'='Relu'\n     ONNX Conv  Relu graph-level \n    fused kernel/\n    \"\"\"\n    marked = 0\n    for node in list(graph.node):\n        if node.op_type != \"Conv\":\n            continue",
        "detail": "onnx.onnx_fuse",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "onnx.onnx_fuse",
        "description": "onnx.onnx_fuse",
        "peekOfCode": "def main(in_path: str, out_path: str):\n    model = onnx.load(in_path)\n    print(\"Loaded model:\", in_path)\n    print(\"Graph nodes before:\", len(model.graph.node))\n    model, fused_bn_count = fold_batchnorm_into_conv(model)\n    print(\"BatchNorm folded count:\", fused_bn_count)\n    marked_relu = detect_conv_relu_and_mark(model.graph)\n    print(\"Conv->Relu patterns annotated (marked):\", marked_relu)\n    # \n    onnx.checker.check_model(model)",
        "detail": "onnx.onnx_fuse",
        "documentation": {}
    },
    {
        "label": "left_img_path",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "left_img_path = \"/home/lc/share/mgz/datas/left.png\"\nright_img_path = \"/home/lc/share/mgz/datas/right.png\"\nonnx_model_path = \"/home/lc/gaoshan/Workspace/OpenStereo/output/model_480x640.onnx\"\nleft_img = Image.open(left_img_path).convert(\"RGB\")\nright_img = Image.open(right_img_path).convert(\"RGB\")\nnormalize_mean=[0.485, 0.456, 0.406]\nnormalize_std=[0.229, 0.224, 0.225]\ntransform = transforms.Compose([\n        transforms.Resize((256, 512)), \n        transforms.ToTensor(),",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "right_img_path",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "right_img_path = \"/home/lc/share/mgz/datas/right.png\"\nonnx_model_path = \"/home/lc/gaoshan/Workspace/OpenStereo/output/model_480x640.onnx\"\nleft_img = Image.open(left_img_path).convert(\"RGB\")\nright_img = Image.open(right_img_path).convert(\"RGB\")\nnormalize_mean=[0.485, 0.456, 0.406]\nnormalize_std=[0.229, 0.224, 0.225]\ntransform = transforms.Compose([\n        transforms.Resize((256, 512)), \n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "onnx_model_path",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "onnx_model_path = \"/home/lc/gaoshan/Workspace/OpenStereo/output/model_480x640.onnx\"\nleft_img = Image.open(left_img_path).convert(\"RGB\")\nright_img = Image.open(right_img_path).convert(\"RGB\")\nnormalize_mean=[0.485, 0.456, 0.406]\nnormalize_std=[0.229, 0.224, 0.225]\ntransform = transforms.Compose([\n        transforms.Resize((256, 512)), \n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n])        ",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "left_img",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "left_img = Image.open(left_img_path).convert(\"RGB\")\nright_img = Image.open(right_img_path).convert(\"RGB\")\nnormalize_mean=[0.485, 0.456, 0.406]\nnormalize_std=[0.229, 0.224, 0.225]\ntransform = transforms.Compose([\n        transforms.Resize((256, 512)), \n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n])        \nleft = transform(left_img).unsqueeze(0)",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "right_img",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "right_img = Image.open(right_img_path).convert(\"RGB\")\nnormalize_mean=[0.485, 0.456, 0.406]\nnormalize_std=[0.229, 0.224, 0.225]\ntransform = transforms.Compose([\n        transforms.Resize((256, 512)), \n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n])        \nleft = transform(left_img).unsqueeze(0)\nright = transform(right_img).unsqueeze(0)",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "transform = transforms.Compose([\n        transforms.Resize((256, 512)), \n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n])        \nleft = transform(left_img).unsqueeze(0)\nright = transform(right_img).unsqueeze(0)\nleft = left.cpu().numpy()\nright = right.cpu().numpy()\n# left.tofile(\"/home/lc/gaoshan/Workspace/OpenStereo/output/left1.npy\")",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "left",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "left = transform(left_img).unsqueeze(0)\nright = transform(right_img).unsqueeze(0)\nleft = left.cpu().numpy()\nright = right.cpu().numpy()\n# left.tofile(\"/home/lc/gaoshan/Workspace/OpenStereo/output/left1.npy\")\n# right.tofile(\"/home/lc/gaoshan/Workspace/OpenStereo/output/right1.npy\")\n# np.save(\"/home/lc/gaoshan/Workspace/OpenStereo/output/left.npy\", left)\n# np.save(\"/home/lc/gaoshan/Workspace/OpenStereo/output/right.npy\", right)\nsession = ort.InferenceSession(onnx_model_path)\noutputs = torch.tensor(session.run(['disp_pred'], ({'left_img': left,'right_img':right})))",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "right",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "right = transform(right_img).unsqueeze(0)\nleft = left.cpu().numpy()\nright = right.cpu().numpy()\n# left.tofile(\"/home/lc/gaoshan/Workspace/OpenStereo/output/left1.npy\")\n# right.tofile(\"/home/lc/gaoshan/Workspace/OpenStereo/output/right1.npy\")\n# np.save(\"/home/lc/gaoshan/Workspace/OpenStereo/output/left.npy\", left)\n# np.save(\"/home/lc/gaoshan/Workspace/OpenStereo/output/right.npy\", right)\nsession = ort.InferenceSession(onnx_model_path)\noutputs = torch.tensor(session.run(['disp_pred'], ({'left_img': left,'right_img':right})))\ndisp_pred_onnx = outputs.squeeze().cpu().numpy()",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "left",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "left = left.cpu().numpy()\nright = right.cpu().numpy()\n# left.tofile(\"/home/lc/gaoshan/Workspace/OpenStereo/output/left1.npy\")\n# right.tofile(\"/home/lc/gaoshan/Workspace/OpenStereo/output/right1.npy\")\n# np.save(\"/home/lc/gaoshan/Workspace/OpenStereo/output/left.npy\", left)\n# np.save(\"/home/lc/gaoshan/Workspace/OpenStereo/output/right.npy\", right)\nsession = ort.InferenceSession(onnx_model_path)\noutputs = torch.tensor(session.run(['disp_pred'], ({'left_img': left,'right_img':right})))\ndisp_pred_onnx = outputs.squeeze().cpu().numpy()\nnp.save(\"./output/run_disparity.npy\", disp_pred_onnx)",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "right",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "right = right.cpu().numpy()\n# left.tofile(\"/home/lc/gaoshan/Workspace/OpenStereo/output/left1.npy\")\n# right.tofile(\"/home/lc/gaoshan/Workspace/OpenStereo/output/right1.npy\")\n# np.save(\"/home/lc/gaoshan/Workspace/OpenStereo/output/left.npy\", left)\n# np.save(\"/home/lc/gaoshan/Workspace/OpenStereo/output/right.npy\", right)\nsession = ort.InferenceSession(onnx_model_path)\noutputs = torch.tensor(session.run(['disp_pred'], ({'left_img': left,'right_img':right})))\ndisp_pred_onnx = outputs.squeeze().cpu().numpy()\nnp.save(\"./output/run_disparity.npy\", disp_pred_onnx)\nprint(disp_pred_onnx)",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "session",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "session = ort.InferenceSession(onnx_model_path)\noutputs = torch.tensor(session.run(['disp_pred'], ({'left_img': left,'right_img':right})))\ndisp_pred_onnx = outputs.squeeze().cpu().numpy()\nnp.save(\"./output/run_disparity.npy\", disp_pred_onnx)\nprint(disp_pred_onnx)\ncv2.imwrite('./output/run_disparity.png', disp_pred_onnx)",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "outputs",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "outputs = torch.tensor(session.run(['disp_pred'], ({'left_img': left,'right_img':right})))\ndisp_pred_onnx = outputs.squeeze().cpu().numpy()\nnp.save(\"./output/run_disparity.npy\", disp_pred_onnx)\nprint(disp_pred_onnx)\ncv2.imwrite('./output/run_disparity.png', disp_pred_onnx)",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "disp_pred_onnx",
        "kind": 5,
        "importPath": "onnx.onnx_run",
        "description": "onnx.onnx_run",
        "peekOfCode": "disp_pred_onnx = outputs.squeeze().cpu().numpy()\nnp.save(\"./output/run_disparity.npy\", disp_pred_onnx)\nprint(disp_pred_onnx)\ncv2.imwrite('./output/run_disparity.png', disp_pred_onnx)",
        "detail": "onnx.onnx_run",
        "documentation": {}
    },
    {
        "label": "simplify_onnx",
        "kind": 2,
        "importPath": "onnx.optimize_onnx",
        "description": "onnx.optimize_onnx",
        "peekOfCode": "def simplify_onnx(input_model, simplified_model):\n    print(\">>> Simplifying ONNX model...\")\n    subprocess.run([\"python3\", \"-m\", \"onnxsim\", input_model, simplified_model], check=True)\n# ========== Step 1.5:  initializer  ==========\ndef fix_initializers(model: onnx.ModelProto) -> onnx.ModelProto:\n    \"\"\"\n     initializer  graph.input\n     onnxoptimizer  Unresolved value references\n    \"\"\"\n    input_names = [i.name for i in model.graph.input]",
        "detail": "onnx.optimize_onnx",
        "documentation": {}
    },
    {
        "label": "fix_initializers",
        "kind": 2,
        "importPath": "onnx.optimize_onnx",
        "description": "onnx.optimize_onnx",
        "peekOfCode": "def fix_initializers(model: onnx.ModelProto) -> onnx.ModelProto:\n    \"\"\"\n     initializer  graph.input\n     onnxoptimizer  Unresolved value references\n    \"\"\"\n    input_names = [i.name for i in model.graph.input]\n    for init in model.graph.initializer:\n        if init.name not in input_names:\n            #  initializer  shape  type  TensorValueInfo\n            vi = onnx.helper.make_tensor_value_info(",
        "detail": "onnx.optimize_onnx",
        "documentation": {}
    },
    {
        "label": "optimize_onnx",
        "kind": 2,
        "importPath": "onnx.optimize_onnx",
        "description": "onnx.optimize_onnx",
        "peekOfCode": "def optimize_onnx(input_model, optimized_model):\n    print(\">>> Optimizing ONNX model with onnxoptimizer...\")\n    passes = onnxoptimizer.get_available_passes()\n    print(onnxoptimizer.get_available_passes())\n    model = onnx.load(input_model)\n    model = fix_initializers(model)  #  initializer \n    # \n    original_nodes = len(model.graph.node)\n    original_ops = [node.op_type for node in model.graph.node]\n    print(f\": {original_nodes} \")",
        "detail": "onnx.optimize_onnx",
        "documentation": {}
    },
    {
        "label": "run_inference",
        "kind": 2,
        "importPath": "onnx.optimize_onnx",
        "description": "onnx.optimize_onnx",
        "peekOfCode": "def run_inference(model_path):\n    print(\">>> Running inference with ONNX Runtime...\")\n    sess_options = ort.SessionOptions()\n    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n    session = ort.InferenceSession(model_path, sess_options)\n    # \n    input_name = session.get_inputs()[0].name\n    input_shape = session.get_inputs()[0].shape\n    input_type = session.get_inputs()[0].type\n    print(f\"Input name: {input_name}, shape: {input_shape}, type: {input_type}\")",
        "detail": "onnx.optimize_onnx",
        "documentation": {}
    },
    {
        "label": "pad",
        "kind": 5,
        "importPath": "onnx.optimize_onnx",
        "description": "onnx.optimize_onnx",
        "peekOfCode": "pad = helper.make_node(\n    \"Pad\", \n    inputs=[\"input\"], outputs=[\"pad_out\"],\n    mode=\"constant\",\n    pads=[0, 0, 1, 1, 0, 0, 1, 1],  # only pad H/W\n    value=0.0\n)\npool = helper.make_node(\n    \"MaxPool\",\n    inputs=[\"pad_out\"], outputs=[\"pool_out\"],",
        "detail": "onnx.optimize_onnx",
        "documentation": {}
    },
    {
        "label": "pool",
        "kind": 5,
        "importPath": "onnx.optimize_onnx",
        "description": "onnx.optimize_onnx",
        "peekOfCode": "pool = helper.make_node(\n    \"MaxPool\",\n    inputs=[\"pad_out\"], outputs=[\"pool_out\"],\n    kernel_shape=[3, 3],\n    strides=[1, 1],\n    pads=[0, 0, 0, 0]\n)\n 1. split_init  \n\n",
        "detail": "onnx.optimize_onnx",
        "documentation": {}
    },
    {
        "label": "Profile",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "class Profile(contextlib.ContextDecorator):\n    # Profile class. Usage: @Profile() decorator or 'with Profile():' context manager\n    def __init__(self, t=0.0):\n        self.t = t\n        self.cuda = torch.cuda.is_available()\n    def __enter__(self):\n        self.start = self.time()\n        return self\n    def __exit__(self, type, value, traceback):\n        self.dt = self.time() - self.start  # delta-time",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_img_size",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)\n    else:  # list i.e. img_size=[640, 480]\n        imgsz = list(imgsz)  # convert to list if tuple\n        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]\n    if new_size != imgsz:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_python",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def check_python(minimum='3.7.0'):\n    # Check current python version vs. required python version\n    check_version(platform.python_version(), minimum, name='Python ', hard=True)\ndef check_version(current='0.0.0', minimum='0.0.0', name='version ', pinned=False, hard=False, verbose=False, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Check version vs. required version\n    current, minimum = (pkg.parse_version(x) for x in (current, minimum))\n    result = (current == minimum) if pinned else (current >= minimum)  # bool\n    s = f'WARNING  {name}{minimum} is required by OpenStereo, but {name}{current} is currently installed'  # string",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_version",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def check_version(current='0.0.0', minimum='0.0.0', name='version ', pinned=False, hard=False, verbose=False, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Check version vs. required version\n    current, minimum = (pkg.parse_version(x) for x in (current, minimum))\n    result = (current == minimum) if pinned else (current >= minimum)  # bool\n    s = f'WARNING  {name}{minimum} is required by OpenStereo, but {name}{current} is currently installed'  # string\n    if hard:\n        assert result, emojis(s)  # assert min requirements met\n    if verbose and not result:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "check_requirements",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def check_requirements(requirements=ROOT / 'requirements.txt', exclude=(), install=True, cmds='', logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Check installed dependencies meet requirements (pass *.txt file or list of packages or single package str)\n    prefix = colorstr('red', 'bold', 'requirements:')\n    check_python()  # check python version\n    if isinstance(requirements, Path):  # requirements.txt file\n        file = requirements.resolve()\n        assert file.exists(), f\"{prefix} {file} not found, check failed.\"\n        with file.open() as f:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "colorstr",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def colorstr(*input):\n    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n    colors = {\n        'black': '\\033[30m',  # basic colors\n        'red': '\\033[31m',\n        'green': '\\033[32m',\n        'yellow': '\\033[33m',\n        'blue': '\\033[34m',\n        'magenta': '\\033[35m',",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "file_size",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def file_size(path):\n    # Return file/dir size (MB)\n    mb = 1 << 20  # bytes to MiB (1024 ** 2)\n    path = Path(path)\n    if path.is_file():\n        return path.stat().st_size / mb\n    elif path.is_dir():\n        return sum(f.stat().st_size for f in path.glob('**/*') if f.is_file()) / mb\n    else:\n        return 0.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "get_default_args",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def get_default_args(func):\n    # Get func() default arguments\n    signature = inspect.signature(func)\n    return {k: v.default for k, v in signature.parameters.items() if v.default is not inspect.Parameter.empty}\ndef get_format_idx(df, format_name):\n    # pandas export\n    check_requirements('pandas')\n    import pandas as pd\n    idx = df.loc[df['Format'] == format_name].index\n    return idx[0] if not idx.empty else -1 ",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "get_format_idx",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def get_format_idx(df, format_name):\n    # pandas export\n    check_requirements('pandas')\n    import pandas as pd\n    idx = df.loc[df['Format'] == format_name].index\n    return idx[0] if not idx.empty else -1 \ndef load_model(model, weights, device=None, inplace=True):\n    from torch import nn\n    ckpt = torch.load(weights, map_location=device)\n    state_dict = ckpt['model_state'] if 'model_state' in ckpt else ckpt",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def load_model(model, weights, device=None, inplace=True):\n    from torch import nn\n    ckpt = torch.load(weights, map_location=device)\n    state_dict = ckpt['model_state'] if 'model_state' in ckpt else ckpt\n    model.load_state_dict(state_dict)\n    # Module compatibility updates\n    for m in model.modules():\n        t = type(m)\n        if t in (nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU):\n            m.inplace = inplace  # torch 1.7.0 compatibility",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "print_args",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def print_args(args: Optional[dict] = None, show_file=True, show_func=False, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Print function arguments (optional args dict)\n    x = inspect.currentframe().f_back  # previous frame\n    file, _, func, _, _ = inspect.getframeinfo(x)\n    if args is None:  # get args automatically\n        args, _, _, frm = inspect.getargvalues(x)\n        args = {k: v for k, v in frm.items() if k in args}\n    try:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "url2file",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def url2file(url):\n    # Convert URL to filename, i.e. https://url.com/file.txt?auth -> file.txt\n    url = str(Path(url)).replace(':/', '://')  # Pathlib turns :// -> :/\n    return Path(urllib.parse.unquote(url)).name.split('?')[0]  # '%2F' to '/', split https://url.com/file.txt?auth\ndef yaml_save(file='data.yaml', data={}):\n    # Single-line safe yaml saving\n    with open(file, 'w') as f:\n        yaml.safe_dump({k: str(v) if isinstance(v, Path) else v for k, v in data.items()}, f, sort_keys=False)\nclass Profile(contextlib.ContextDecorator):\n    # Profile class. Usage: @Profile() decorator or 'with Profile():' context manager",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "yaml_save",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "def yaml_save(file='data.yaml', data={}):\n    # Single-line safe yaml saving\n    with open(file, 'w') as f:\n        yaml.safe_dump({k: str(v) if isinstance(v, Path) else v for k, v in data.items()}, f, sort_keys=False)\nclass Profile(contextlib.ContextDecorator):\n    # Profile class. Usage: @Profile() decorator or 'with Profile():' context manager\n    def __init__(self, t=0.0):\n        self.t = t\n        self.cuda = torch.cuda.is_available()\n    def __enter__(self):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[1]  # root directory\n# Settings\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of multiprocessing threads\nAUTOINSTALL = str(os.getenv('OPENSTEREO_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('OPENSTEREO_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "ROOT = FILE.parents[1]  # root directory\n# Settings\nNUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of multiprocessing threads\nAUTOINSTALL = str(os.getenv('OPENSTEREO_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('OPENSTEREO_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "NUM_THREADS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "NUM_THREADS = min(8, max(1, os.cpu_count() - 1))  # number of multiprocessing threads\nAUTOINSTALL = str(os.getenv('OPENSTEREO_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('OPENSTEREO_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "AUTOINSTALL",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "AUTOINSTALL = str(os.getenv('OPENSTEREO_AUTOINSTALL', True)).lower() == 'true'  # global auto-install mode\nVERBOSE = str(os.getenv('OPENSTEREO_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "VERBOSE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "VERBOSE = str(os.getenv('OPENSTEREO_VERBOSE', True)).lower() == 'true'  # global verbose mode\nTQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "TQDM_BAR_FORMAT",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "TQDM_BAR_FORMAT = '{l_bar}{bar:10}| {n_fmt}/{total_fmt} {elapsed}'  # tqdm bar format\nFONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "FONT",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "FONT = 'Arial.ttf'  # https://ultralytics.com/assets/Arial.ttf\ntorch.set_printoptions(linewidth=320, precision=5, profile='long')\nnp.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\npd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "pd.options.display.max_columns",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "pd.options.display.max_columns = 10\ncv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\nos.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "os.environ['NUMEXPR_MAX_THREADS']",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "os.environ['NUMEXPR_MAX_THREADS'] = str(NUM_THREADS)  # NumExpr max threads\nos.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)\n    else:  # list i.e. img_size=[640, 480]\n        imgsz = list(imgsz)  # convert to list if tuple",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "os.environ['OMP_NUM_THREADS']",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "peekOfCode": "os.environ['OMP_NUM_THREADS'] = '1' if platform.system() == 'darwin' else str(NUM_THREADS)  # OpenMP (PyTorch and SciPy)\ndef check_img_size(imgsz, s=32, floor=0, logger=None):\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    # Verify image size is a multiple of stride s in each dimension\n    if isinstance(imgsz, int):  # integer i.e. img_size=640\n        new_size = max(make_divisible(imgsz, int(s)), floor)\n    else:  # list i.e. img_size=[640, 480]\n        imgsz = list(imgsz)  # convert to list if tuple\n        new_size = [max(make_divisible(x, int(s)), floor) for x in imgsz]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.deploy_utils",
        "documentation": {}
    },
    {
        "label": "export_formats",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def export_formats():\n    # export formats\n    x = [\n        ['PyTorch', '-', '.pt', True, True],\n        ['TorchScript', 'torchscript', '.torchscript', True, True],\n        ['ONNX', 'onnx', '.onnx', True, True],\n        # ['ONNX END2END', 'onnx_end2end', '_end2end.onnx', True, True],\n        ['OpenVINO', 'openvino', '_openvino_model', True, False],\n        ['TensorRT', 'engine', '.engine', False, True],\n        ['CoreML', 'coreml', '.mlmodel', True, False],",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "try_export",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def try_export(inner_func):\n    # export decorator, i..e @try_export\n    inner_args = get_default_args(inner_func)\n    def outer_func(*args, **kwargs):\n        prefix = inner_args['prefix']\n        try:\n            with Profile() as dt:\n                f, model = inner_func(*args, **kwargs)\n            logger.info(f'{prefix} export success  {dt.t:.1f}s, saved as {f} ({file_size(f):.1f} MB)')\n            return f, model",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "export_torchscript",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def export_torchscript(model, inputs, file, optimize, prefix=colorstr('TorchScript:')):\n    logger.info(f'\\n{prefix} starting export with torch {torch.__version__}...')\n    f = Path(file).with_suffix('.torchscript')\n    ts = torch.jit.trace(model, inputs, strict=False)\n    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html\n        inputs['ref_img'].cpu()\n        inputs['tgt_img'].cpu()\n        model.cpu()\n        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f))\n    else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "export_onnx",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def export_onnx(model, inputs, weights, opset, dynamic, simplify, prefix=colorstr('ONNX:')):\n    # ONNX export\n    check_requirements('onnx', logger)\n    import onnx\n    logger.info(f'{prefix} starting export with onnx {onnx.__version__}...')\n    f = Path(weights).with_suffix('.onnx')\n    input_names = ['left_img', 'right_img']\n    output_names =  ['disp_pred']\n    if dynamic:\n        dynamic = {'left_img': {0: 'batch', 2: 'height', 3: 'width'},",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "export_openvino",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def export_openvino(file, half, prefix=colorstr('OpenVINO:')):\n    # OpenVINO export\n    check_requirements('openvino-dev', logger)  # requires openvino-dev: https://pypi.org/project/openvino-dev/\n    import openvino\n    logger.info(f'{prefix} starting export with openvino {openvino.__version__}...')\n    f = str(file).replace('.pt', f'_openvino_model{os.sep}')\n    half_arg = '--compress_to_fp16'\n    half_arg += '=True' if half else '=False'\n    cmd = f\"mo --input_model {Path(file).with_suffix('.onnx')} --output_dir {f} {half_arg}\"\n    subprocess.run(cmd.split(), check=True, env=os.environ)  # export",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "export_engine",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def export_engine(model, inputs, file, half, dynamic, simplify, optimize, workspace=4, verbose=False, prefix=colorstr('TensorRT:')):\n    # TensorRT export https://developer.nvidia.com/tensorrt\n    assert inputs['left'].device.type != 'cpu', 'export running on CPU but must be on GPU, i.e. `python export.py --device 0`'\n    try:\n        import tensorrt as trt\n        import modelopt\n    except Exception:\n        if platform.system() == 'Linux':\n            check_requirements('tensorrt', logger, cmds='-U --index-url https://pypi.ngc.nvidia.com')\n            check_requirements('nvidia-modelopt[all]~=0.11.0', logger, cmds='--extra-index-url https://pypi.nvidia.com')",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "export_paddle",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def export_paddle(model, inputs, file, prefix=colorstr('PaddlePaddle:')):\n    # Paddle export\n    check_requirements(('paddlepaddle', 'x2paddle'), logger, cmds='-i https://pypi.tuna.tsinghua.edu.cn/simple')\n    import x2paddle\n    from x2paddle.convert import pytorch2paddle\n    logger.info(f'{prefix} starting export with X2Paddle {x2paddle.__version__}...')\n    f = str(file).replace('.pt', f'_paddle_model{os.sep}')\n    pytorch2paddle(module=model, save_dir=f, jit_type='trace', input_examples=[inputs])  # export\n    return f, None\n@try_export",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "export_coreml",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def export_coreml(model, im, file, int8, half, prefix=colorstr('CoreML:')):\n    # CoreML export\n    check_requirements('coremltools', logger, cmds='-i https://pypi.tuna.tsinghua.edu.cn/simple')\n    import coremltools as ct\n    logger.info(f'{prefix} starting export with coremltools {ct.__version__}...')\n    f = Path(file).with_suffix('.mlmodel')\n    ts = torch.jit.trace(model, im, strict=False)  # TorchScript model\n    ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=im['left'].shape, scale=1 / 255, bias=[0, 0, 0])])\n    bits, mode = (8, 'kmeans_lut') if int8 else (16, 'linear') if half else (32, None)\n    if bits < 32:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def run(\n        config=ROOT / '../../cfgs/psmnet/psmnet_kitti15.yaml',                                                                 # 'config.yaml path'\n        weights=ROOT / '../../output/KITTI2015/PSMNet/PSMNet_SceneFlow/checkpoints/PSMNet_SceneFlow_epoch_latest.pt',           # weights path\n        imgsz=(256, 512),                   # image (height, width)\n        batch_size=1,                       # batch size\n        device='cpu',                       # cuda device, i.e. 0 or 0,1,2,3 or cpu\n        include=('torchscript', 'onnx'),    # include formats\n        half=False,                         # FP16 half-precision export\n        optimize=False,                     # TorchScript: optimize for mobile\n        int8=False,                         # CoreML INT8 quantization",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "parse_opt",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def parse_opt():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str, default=ROOT / '../../cfgs/psmnet/psmnet_kitti15.yaml', help='<config>.yaml path')\n    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / '../../output/KITTI2015/PSMNet/PSMNet_SceneFlow/checkpoints/PSMNet_SceneFlow_epoch_latest.pt', help='model.pt path(s)')\n    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=[256, 512], help='image size (h, w)')\n    parser.add_argument('--batch-size', type=int, default=1, help='batch size')\n    parser.add_argument('--device', default='cpu', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n    parser.add_argument('--half', action='store_true', help='FP16 half-precision export')\n    parser.add_argument('--optimize', action='store_true', help='TorchScript: optimize for mobile / TensorRT: optimize inference')\n    parser.add_argument('--int8', action='store_true', help='CoreML INT8 quantization')",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "def main(opt):\n    for opt.weights in (opt.weights if isinstance(opt.weights, list) else [opt.weights]):\n        run(**vars(opt))\nif __name__ == \"__main__\":\n    opt = parse_opt()\n    main(opt)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "FILE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "FILE = Path(__file__).resolve()\nROOT = FILE.parents[0]  # root directory\nMODEL_NAME = ''\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != 'Windows':\n    RELATIVAE_ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom deploy_utils import (check_img_size, check_requirements, check_version, colorstr, file_size, get_default_args,\n                          get_format_idx, load_model, print_args, url2file, yaml_save, Profile)\nMACOS = platform.system() == 'Darwin'  # macOS environment",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "ROOT",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "ROOT = FILE.parents[0]  # root directory\nMODEL_NAME = ''\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != 'Windows':\n    RELATIVAE_ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom deploy_utils import (check_img_size, check_requirements, check_version, colorstr, file_size, get_default_args,\n                          get_format_idx, load_model, print_args, url2file, yaml_save, Profile)\nMACOS = platform.system() == 'Darwin'  # macOS environment\nimport os",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "MODEL_NAME",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "MODEL_NAME = ''\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))  # add ROOT to PATH\nif platform.system() != 'Windows':\n    RELATIVAE_ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # relative\nfrom deploy_utils import (check_img_size, check_requirements, check_version, colorstr, file_size, get_default_args,\n                          get_format_idx, load_model, print_args, url2file, yaml_save, Profile)\nMACOS = platform.system() == 'Darwin'  # macOS environment\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "MACOS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "MACOS = platform.system() == 'Darwin'  # macOS environment\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n# Network\nsys.path.append(str(ROOT.parent))  # add ROOT to PATH\nfrom stereo.modeling import models, build_trainer\nfrom stereo.utils.common_utils import config_loader, create_logger, load_params_from_file\nfrom stereo.modeling.models.sttr.sttr import STTR\nfrom stereo.modeling.models.psmnet.psmnet import PSMNet\nfrom stereo.modeling.models.msnet.MSNet2D import MSNet2D",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "os.environ['PYTORCH_CUDA_ALLOC_CONF']",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n# Network\nsys.path.append(str(ROOT.parent))  # add ROOT to PATH\nfrom stereo.modeling import models, build_trainer\nfrom stereo.utils.common_utils import config_loader, create_logger, load_params_from_file\nfrom stereo.modeling.models.sttr.sttr import STTR\nfrom stereo.modeling.models.psmnet.psmnet import PSMNet\nfrom stereo.modeling.models.msnet.MSNet2D import MSNet2D\nfrom stereo.modeling.models.msnet.MSNet3D import MSNet3D\nfrom stereo.modeling.models.igev.igev_stereo import IGEVStereo as IGEV",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "__net__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "__net__ = {\n    'STTR': STTR,\n    'PSMNet': PSMNet,\n    'MSNet2D': MSNet2D,\n    'MSNet3D': MSNet3D,\n    'IGEV': IGEV,\n    'GwcNet': GwcNet,\n    'FADNet': FADNet,\n    'CoExNet': CoEx,\n    # 'AANet': AANet,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "peekOfCode": "logger = logging.getLogger('export')\nlogging.basicConfig(level=logging.DEBUG, \n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\ndef export_formats():\n    # export formats\n    x = [\n        ['PyTorch', '-', '.pt', True, True],\n        ['TorchScript', 'torchscript', '.torchscript', True, True],\n        ['ONNX', 'onnx', '.onnx', True, True],\n        # ['ONNX END2END', 'onnx_end2end', '_end2end.onnx', True, True],",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.deploy.export",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.readpfm",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.readpfm",
        "peekOfCode": "def readpfm(file):\n    file = open(file, 'rb')\n    color = None\n    width = None\n    height = None\n    scale = None\n    endian = None\n    header = file.readline().rstrip()\n    if (sys.version[0]) == '3':\n        header = header.decode('utf-8')",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "Compose",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n    def __call__(self, sample):\n        for t in self.transforms:\n            sample = t(sample)\n        return sample\nclass TransposeImage(object):\n    def __init__(self, config):\n        self.config = config",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "TransposeImage",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class TransposeImage(object):\n    def __init__(self, config):\n        self.config = config\n    def __call__(self, sample):\n        sample['left'] = sample['left'].transpose((2, 0, 1))\n        sample['right'] = sample['right'].transpose((2, 0, 1))\n        return sample\nclass ToTensor(object):\n    def __init__(self, config):\n        self.config = config",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "ToTensor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class ToTensor(object):\n    def __init__(self, config):\n        self.config = config\n    def __call__(self, sample):\n        for k in sample.keys():\n            if isinstance(sample[k], np.ndarray):\n                if k == 'super_pixel_label':\n                    sample[k] = torch.from_numpy(sample[k].copy()).to(torch.int32)\n                elif k in ['occ_mask', 'occ_mask_2']:\n                    sample[k] = torch.from_numpy(sample[k].copy()).to(torch.bool)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "NormalizeImage",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class NormalizeImage(object):\n    def __init__(self, config):\n        self.mean = config.MEAN\n        self.std = config.STD\n    def __call__(self, sample):\n        sample['left'] = normalize(sample['left'] / 255.0, self.mean, self.std)\n        sample['right'] = normalize(sample['right'] / 255.0, self.mean, self.std)\n        return sample\nclass RandomCrop(object):\n    def __init__(self, config):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RandomCrop",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RandomCrop(object):\n    def __init__(self, config):\n        self.crop_size = config.SIZE\n        self.base_size = config.SIZE\n        self.y_jitter = config.get('Y_JITTER', False)\n    def __call__(self, sample):\n        crop_height, crop_width = self.crop_size\n        height, width = sample['left'].shape[:2]  # (H, W, 3)\n        # crop_height = min(height, crop_height)\n        # crop_width = min(width, crop_width)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RandomScale",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RandomScale(object):\n    def __init__(self, config):\n        self.config = config\n        self.crop_size = config.SIZE\n        self.min_scale = config.MIN_SCALE\n        self.max_scale = config.MAX_SCALE\n        self.scale_prob = config.SCALE_PROB\n        self.stretch_prob = config.STRETCH_PROB\n        self.max_stretch = 0.2\n    def __call__(self, sample):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RandomSparseScale",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RandomSparseScale(object):\n    def __init__(self, config):\n        self.crop_size = config.SIZE\n        self.min_scale = config.MIN_SCALE\n        self.max_scale = config.MAX_SCALE\n        self.scale_prob = config.SCALE_PROB\n    def __call__(self, sample):\n        ht, wd = sample['left'].shape[:2]\n        min_scale = np.maximum((self.crop_size[0] + 1) / float(ht), (self.crop_size[1] + 1) / float(wd))\n        scale = 2 ** np.random.uniform(self.min_scale, self.max_scale)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RandomErase",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RandomErase(object):\n    def __init__(self, config):\n        self.eraser_aug_prob = config.PROB\n        self.max_erase_time = config.MAX_TIME\n        self.bounds = config.BOUNDS\n    def __call__(self, sample):\n        img1 = sample['left']\n        img2 = sample['right']\n        ht, wd = img1.shape[:2]\n        if 'super_pixel_label' in sample:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "StereoColorJitter",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class StereoColorJitter(object):\n    def __init__(self, config):\n        self.brightness = list(config.BRIGHTNESS)\n        self.contrast = list(config.CONTRAST)\n        self.saturation = list(config.SATURATION)\n        if isinstance(config.HUE, float):\n            config.HUE = [-config.HUE, config.HUE]\n        self.hue = list(config.HUE)\n        self.asymmetric_color_aug_prob = config.ASYMMETRIC_PROB\n        self.color_jitter = ColorJitter(brightness=self.brightness, contrast=self.contrast,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RightTopPad",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RightTopPad(object):\n    def __init__(self, config):\n        self.size = config.SIZE\n    def __call__(self, sample):\n        h, w = sample['left'].shape[:2]\n        th, tw = self.size\n        h = min(h, th)  # ensure h is within the bounds of the image\n        w = min(w, tw)  # ensure w is within the bounds of the image\n        pad_left = 0\n        pad_right = tw - w",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "DivisiblePad",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class DivisiblePad(object):\n    def __init__(self, config):\n        self.by = config.BY\n        self.mode = config.get('MODE', 'tr')\n    def __call__(self, sample):\n        h, w = sample['left'].shape[:2]\n        if h % self.by != 0:\n            pad_h = self.by - h % self.by\n        else:\n            pad_h = 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RandomFlip",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RandomFlip(object):\n    def __init__(self, config):\n        self.config = config\n        self.flip_type = config.FLIP_TYPE\n        self.prob = config.PROB\n    def __call__(self, sample):\n        img1 = sample['left']\n        img2 = sample['right']\n        disp = sample['disp']\n        disp_right = sample['disp_right']",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RightBottomCrop",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RightBottomCrop(object):\n    def __init__(self, config):\n        self.size = config.SIZE\n    def __call__(self, sample):\n        h, w = sample['left'].shape[:2]\n        crop_h, crop_w = self.size\n        crop_h = min(h, crop_h)\n        crop_w = min(w, crop_w)\n        for k in sample.keys():\n            sample[k] = sample[k][h - crop_h:, w - crop_w:]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "CropOrPad",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class CropOrPad(object):\n    def __init__(self, config):\n        self.size = config.SIZE\n        self.crop_fn = RightBottomCrop(config)\n        self.pad_fn = RightTopPad(config)\n    def __call__(self, sample):\n        h, w = sample['left'].shape[:2]\n        th, tw = self.size\n        if th > h or tw > w:\n            sample = self.pad_fn(sample)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "ArgoverseDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.argoverse_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.argoverse_dataset",
        "peekOfCode": "class ArgoverseDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.argoverse_dataset",
        "documentation": {}
    },
    {
        "label": "CarlaDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.carla_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.carla_dataset",
        "peekOfCode": "class CarlaDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.carla_dataset",
        "documentation": {}
    },
    {
        "label": "CREStereoDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.crestereo_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.crestereo_dataset",
        "peekOfCode": "class CREStereoDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.crestereo_dataset",
        "documentation": {}
    },
    {
        "label": "DatasetTemplate",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "peekOfCode": "class DatasetTemplate(torch_data.Dataset):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__()\n        self.data_info = data_info\n        self.data_cfg = data_cfg\n        self.mode = mode\n        self.root = self.data_info.DATA_PATH\n        if self.mode.upper() in self.data_info.DATA_SPLIT:\n            self.split_file = self.data_info.DATA_SPLIT[self.mode.upper()]\n            transform_config = self.data_cfg.DATA_TRANSFORM[self.mode.upper()]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "build_transform_by_cfg",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "peekOfCode": "def build_transform_by_cfg(transform_config):\n    transform_compose = []\n    for cur_cfg in transform_config:\n        cur_augmentor = getattr(stereo_trans, cur_cfg.NAME)(config=cur_cfg)\n        transform_compose.append(cur_augmentor)\n    return stereo_trans.Compose(transform_compose)\nclass DatasetTemplate(torch_data.Dataset):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__()\n        self.data_info = data_info",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "get_size",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "peekOfCode": "def get_size(base_size, w_range, h_range, random_type):\n    if random_type == 'range':\n        w = random.randint(int(w_range[0] * base_size[1]), int(w_range[1] * base_size[1]))\n        h = random.randint(int(h_range[0] * base_size[0]), int(h_range[1] * base_size[0]))\n    elif random_type == 'choice':\n        w = random.choice(w_range) if isinstance(w_range, list) else w_range\n        h = random.choice(h_range) if isinstance(h_range, list) else h_range\n    else:\n        raise NotImplementedError\n    return int(h), int(w)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "custom_collate",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "peekOfCode": "def custom_collate(data_list, concat_dataset, batch_uniform=False,\n                   random_type=None, h_range=None, w_range=None):\n    if batch_uniform:\n        for each_dataset in concat_dataset.datasets:\n            for cur_t in each_dataset.transform.transforms:\n                if type(cur_t).__name__ == 'RandomCrop':\n                    base_size = cur_t.base_size\n                    cur_t.crop_size = get_size(base_size, w_range, h_range, random_type)\n                    break\n    return torch_data.default_collate(data_list)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "DrivingDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.driving_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.driving_dataset",
        "peekOfCode": "class DrivingDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        self.use_noc = self.data_info.get('USE_NOC', False)\n        # random.shuffle(self.data_list)\n        # self.data_list = self.data_list[:10000]\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.driving_dataset",
        "documentation": {}
    },
    {
        "label": "DynamicReplicaDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dynamic_replica",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dynamic_replica",
        "peekOfCode": "class DynamicReplicaDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.dynamic_replica",
        "documentation": {}
    },
    {
        "label": "ETH3DDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.eth3d_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.eth3d_dataset",
        "peekOfCode": "class ETH3DDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_POS'):\n            self.retrun_pos = self.data_info.RETURN_POS\n        else:\n            self.retrun_pos = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.eth3d_dataset",
        "documentation": {}
    },
    {
        "label": "FallingThingsDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.fallingthings_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.fallingthings_dataset",
        "peekOfCode": "class FallingThingsDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.fallingthings_dataset",
        "documentation": {}
    },
    {
        "label": "FoundationStereoDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.foundationstereo",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.foundationstereo",
        "peekOfCode": "class FoundationStereoDataset(torch_data.Dataset):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__()\n        self.data_info = data_info\n        self.data_cfg = data_cfg\n        self.mode = mode\n        self.root = self.data_info.DATA_PATH\n        self.data_list = []\n        if self.mode.upper() in self.data_info.DATA_SPLIT:\n            transform_config = self.data_cfg.DATA_TRANSFORM[self.mode.upper()]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.foundationstereo",
        "documentation": {}
    },
    {
        "label": "depth_uint8_decoding",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.foundationstereo",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.foundationstereo",
        "peekOfCode": "def depth_uint8_decoding(depth_uint8, scale=1000):\n    depth_uint8 = depth_uint8.astype(float)\n    out = depth_uint8[...,0]*255*255 + depth_uint8[..., 1]*255 + depth_uint8[..., 2]\n    return out/float(scale)\nclass FoundationStereoDataset(torch_data.Dataset):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__()\n        self.data_info = data_info\n        self.data_cfg = data_cfg\n        self.mode = mode",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.foundationstereo",
        "documentation": {}
    },
    {
        "label": "InStereo2KDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.instereo2k_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.instereo2k_dataset",
        "peekOfCode": "class InStereo2KDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.instereo2k_dataset",
        "documentation": {}
    },
    {
        "label": "KittiDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.kitti_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.kitti_dataset",
        "peekOfCode": "class KittiDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        self.use_noc = self.data_info.get('USE_NOC', False)\n        if hasattr(self.data_info, 'RETURN_POS'):\n            self.retrun_pos = self.data_info.RETURN_POS\n        else:\n            self.retrun_pos = False\n    def __getitem__(self, idx):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.kitti_dataset",
        "documentation": {}
    },
    {
        "label": "MiddleburyDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.middlebury_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.middlebury_dataset",
        "peekOfCode": "class MiddleburyDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_POS'):\n            self.retrun_pos = self.data_info.RETURN_POS\n        else:\n            self.retrun_pos = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.middlebury_dataset",
        "documentation": {}
    },
    {
        "label": "SceneFlowDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.sceneflow_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.sceneflow_dataset",
        "peekOfCode": "class SceneFlowDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_POS'):\n            self.retrun_pos = self.data_info.RETURN_POS\n        else:\n            self.retrun_pos = False\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.sceneflow_dataset",
        "documentation": {}
    },
    {
        "label": "FlyingThings3DSubsetDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.sceneflow_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.sceneflow_dataset",
        "peekOfCode": "class FlyingThings3DSubsetDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_occ_mask = self.data_info.RETURN_OCC_MASK\n        self.zeroing_occ = self.data_info.ZEROING_OCC\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item[0:6]]\n        left_img_path, right_img_path, disp_img_path, disp_img_right_path, occ_path, occ_right_path = full_paths\n        left_img = Image.open(left_img_path).convert('RGB')",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.sceneflow_dataset",
        "documentation": {}
    },
    {
        "label": "SintelDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.sintel_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.sintel_dataset",
        "peekOfCode": "class SintelDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.sintel_dataset",
        "documentation": {}
    },
    {
        "label": "SpringDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.spring",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.spring",
        "peekOfCode": "class SpringDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.spring",
        "documentation": {}
    },
    {
        "label": "TartanAirDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.tartanair_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.tartanair_dataset",
        "peekOfCode": "class TartanAirDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.tartanair_dataset",
        "documentation": {}
    },
    {
        "label": "UnrealStereo4KDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.unrealstereo4k_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.unrealstereo4k_dataset",
        "peekOfCode": "class UnrealStereo4KDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.unrealstereo4k_dataset",
        "documentation": {}
    },
    {
        "label": "VirtualKitti2Dataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.vkitti2_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.vkitti2_dataset",
        "peekOfCode": "class VirtualKitti2Dataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.vkitti2_dataset",
        "documentation": {}
    },
    {
        "label": "get_disp",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.vkitti2_dataset",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.vkitti2_dataset",
        "peekOfCode": "def get_disp(file_path, checkinvalid=True):\n    if '.png' in file_path:\n        depth = cv2.imread(file_path, cv2.IMREAD_ANYDEPTH | cv2.IMREAD_ANYCOLOR)\n        invalid = depth >= 65535\n        num_invalid = depth[invalid].shape[0]\n        depth = depth / 100.0\n    else:\n        raise TypeError('only support png and npy format, invalid type found: {}'.format(file_path))\n    f = 725.0087\n    b = 0.532725 # meter",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.datasets.vkitti2_dataset",
        "documentation": {}
    },
    {
        "label": "d1_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.evaluation.metric_per_image",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.evaluation.metric_per_image",
        "peekOfCode": "def d1_metric(disp_pred, disp_gt, mask):\n    E = torch.abs(disp_gt - disp_pred)\n    err_mask = (E > 3) & (E / torch.abs(disp_gt) > 0.05)\n    err_mask = err_mask & mask\n    num_errors = err_mask.sum(dim=[1, 2])\n    num_valid_pixels = mask.sum(dim=[1, 2])\n    d1_per_image = num_errors.float() / num_valid_pixels.float() * 100\n    d1_per_image = torch.where(num_valid_pixels > 0, d1_per_image, torch.zeros_like(d1_per_image))\n    return d1_per_image\ndef threshold_metric(disp_pred, disp_gt, mask, threshold):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "threshold_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.evaluation.metric_per_image",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.evaluation.metric_per_image",
        "peekOfCode": "def threshold_metric(disp_pred, disp_gt, mask, threshold):\n    E = torch.abs(disp_gt - disp_pred)\n    err_mask = E > threshold\n    err_mask = err_mask & mask\n    num_errors = err_mask.sum(dim=[1, 2])\n    num_valid_pixels = mask.sum(dim=[1, 2])\n    bad_per_image = num_errors.float() / num_valid_pixels.float() * 100\n    bad_per_image = torch.where(num_valid_pixels > 0, bad_per_image, torch.zeros_like(bad_per_image))\n    return bad_per_image\ndef epe_metric(disp_pred, disp_gt, mask):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "epe_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.evaluation.metric_per_image",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.evaluation.metric_per_image",
        "peekOfCode": "def epe_metric(disp_pred, disp_gt, mask):\n    E = torch.abs(disp_gt - disp_pred)\n    E_masked = torch.where(mask, E, torch.zeros_like(E))\n    E_sum = E_masked.sum(dim=[1, 2])\n    num_valid_pixels = mask.sum(dim=[1, 2])\n    epe_per_image = E_sum / num_valid_pixels\n    epe_per_image = torch.where(num_valid_pixels > 0, epe_per_image, torch.zeros_like(epe_per_image))\n    return epe_per_image",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "DeformConvFunction",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class DeformConvFunction(Function):\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                weight,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "ModulatedDeformConvFunction",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class ModulatedDeformConvFunction(Function):\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                mask,\n                weight,\n                bias=None,\n                stride=1,\n                padding=0,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "DeformConv",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class DeformConv(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "DeformConvPack",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class DeformConvPack(DeformConv):\n    \"\"\"A Deformable Conv Encapsulation that acts as normal Conv layers.\n    Args:\n        in_channels (int): Same as nn.Conv2d.\n        out_channels (int): Same as nn.Conv2d.\n        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n        stride (int or tuple[int]): Same as nn.Conv2d.\n        padding (int or tuple[int]): Same as nn.Conv2d.\n        dilation (int or tuple[int]): Same as nn.Conv2d.\n        groups (int): Same as nn.Conv2d.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "ModulatedDeformConv",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class ModulatedDeformConv(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "ModulatedDeformConvPack",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class ModulatedDeformConvPack(ModulatedDeformConv):\n    \"\"\"A ModulatedDeformable Conv Encapsulation that acts as normal Conv layers.\n    Args:\n        in_channels (int): Same as nn.Conv2d.\n        out_channels (int): Same as nn.Conv2d.\n        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n        stride (int or tuple[int]): Same as nn.Conv2d.\n        padding (int or tuple[int]): Same as nn.Conv2d.\n        dilation (int or tuple[int]): Same as nn.Conv2d.\n        groups (int): Same as nn.Conv2d.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "deform_conv",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "deform_conv = DeformConvFunction.apply\nmodulated_deform_conv = ModulatedDeformConvFunction.apply\nclass DeformConv(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "modulated_deform_conv",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "modulated_deform_conv = ModulatedDeformConvFunction.apply\nclass DeformConv(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.fadnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.fadnet",
        "peekOfCode": "class ResBlock(nn.Module):\n    def __init__(self, n_in, n_out, stride=1):\n        super(ResBlock, self).__init__()\n        self.conv1 = nn.Conv2d(n_in, n_out, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(n_out)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(n_out, n_out, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(n_out)\n        if stride != 1 or n_out != n_in:\n            self.shortcut = nn.Sequential(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.fadnet",
        "documentation": {}
    },
    {
        "label": "FadnetBackbone",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.fadnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.fadnet",
        "peekOfCode": "class FadnetBackbone(nn.Module):\n    def __init__(self, resblock=True, input_channel=3, encoder_ratio=16, decoder_ratio=16):\n        super(FadnetBackbone, self).__init__()\n        self.input_channel = input_channel\n        self.basicC = 2\n        self.eratio = encoder_ratio\n        self.dratio = decoder_ratio\n        self.basicE = self.basicC * self.eratio\n        self.basicD = self.basicC * self.dratio\n        # shrink and extract features",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.fadnet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.psmnet_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.psmnet_backbone",
        "peekOfCode": "class BasicBlock(nn.Module):\n    def __init__(self, downsample, in_planes, out_planes, stride, padding):\n        super(BasicBlock, self).__init__()\n        self.conv1 = BasicConv2d(in_channels=in_planes, out_channels=out_planes,\n                                 norm_layer=nn.BatchNorm2d,\n                                 act_layer=partial(nn.ReLU, inplace=True),\n                                 kernel_size=3, stride=stride, padding=padding)\n        self.conv2 = BasicConv2d(in_channels=out_planes, out_channels=out_planes,\n                                 norm_layer=nn.BatchNorm2d, act_layer=None,\n                                 kernel_size=3, stride=1, padding=padding)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.psmnet_backbone",
        "documentation": {}
    },
    {
        "label": "PsmnetBackbone",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.psmnet_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.psmnet_backbone",
        "peekOfCode": "class PsmnetBackbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.firstconv = nn.Sequential(\n            BasicConv2d(3, out_channels=32,\n                        norm_layer=nn.BatchNorm2d, act_layer=partial(nn.ReLU, inplace=True),\n                        kernel_size=3, stride=2, padding=1),\n            BasicConv2d(32, out_channels=32,\n                        norm_layer=nn.BatchNorm2d, act_layer=partial(nn.ReLU, inplace=True),\n                        kernel_size=3, stride=1, padding=1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.backbones.psmnet_backbone",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_2d",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_2d",
        "peekOfCode": "class BasicConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, bias=False,\n                 norm_layer=None, act_layer=None, **kwargs):\n        super(BasicConv2d, self).__init__()\n        layers = [nn.Conv2d(in_channels, out_channels,\n                            kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, **kwargs)]\n        if norm_layer is not None:\n            layers.append(norm_layer(out_channels))\n        if act_layer is not None:\n            layers.append(act_layer())",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv2d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_2d",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_2d",
        "peekOfCode": "class BasicDeconv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False,\n                 norm_layer=None, act_layer=None, **kwargs):\n        super(BasicDeconv2d, self).__init__()\n        layers = [nn.ConvTranspose2d(in_channels, out_channels,\n                                     kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, **kwargs)]\n        if norm_layer is not None:\n            layers.append(norm_layer(out_channels))\n        if act_layer is not None:\n            layers.append(act_layer())",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_3d",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_3d",
        "peekOfCode": "class BasicConv3d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, bias=False,\n                 norm_layer=None, act_layer=None, **kwargs):\n        super(BasicConv3d, self).__init__()\n        layers = [nn.Conv3d(in_channels, out_channels,\n                            kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, **kwargs)]\n        if norm_layer is not None:\n            layers.append(norm_layer(out_channels))\n        if act_layer is not None:\n            layers.append(act_layer())",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv3d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_3d",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_3d",
        "peekOfCode": "class BasicDeconv3d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False,\n                 norm_layer=None, act_layer=None, **kwargs):\n        super(BasicDeconv3d, self).__init__()\n        layers = [nn.ConvTranspose3d(in_channels, out_channels,\n                                     kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, **kwargs)]\n        if norm_layer is not None:\n            layers.append(norm_layer(out_channels))\n        if act_layer is not None:\n            layers.append(act_layer())",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BCELoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "peekOfCode": "class BCELoss:  # nn.BCELoss()\n    def __init__(self):\n        self.reduction = 'mean'\n    def __call__(self, inputs, targets):\n        \"\"\"\n        no c dim, because each item means a prob\n        :param inputs: [bz, d1, d2, ...], after softmax or sigmod\n        :param targets: [bz, d1, d2, ...], same shape with inputs, between 0 and 1, such as 0, 0.7, 1\n        :return:\n        t = torch.rand(size=[4, 3])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "documentation": {}
    },
    {
        "label": "BCEWithLogitsLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "peekOfCode": "class BCEWithLogitsLoss:  # nn.BCEWithLogitsLoss()\n    def __call__(self, inputs, targets):\n        \"\"\"\n        t = torch.rand(size=[4, 3])\n        a = torch.randn(size=[4, 3])\n        b = F.sigmoid(a)\n        nn.BCEWithLogitsLoss()(a, t) == BCEWithLogitsLoss()(a, t) == BCELoss()(b, t)\n        \"\"\"\n        inputs = F.sigmoid(inputs)\n        return BCELoss()(inputs, targets)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "peekOfCode": "class CrossEntropyLoss:  # nn.CrossEntropyLoss()\n    def __call__(self, inputs, targets):\n        \"\"\"\n        :param inputs: [bz, c, d1, d2, ...], before softmax\n        :param targets: [bz, d1, d2, ...], value in the range [0, c), or [bz, c, d1, d2, ...], between 0 and 1, such as 0.7\n        :return:\n        \"\"\"\n        inputs = F.softmax(inputs, dim=1)\n        inputs = torch.log(inputs)\n        if targets.shape != inputs.shape:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "documentation": {}
    },
    {
        "label": "KLDivLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "peekOfCode": "class KLDivLoss:\n    def __init__(self):\n        self.reduction = 'mean'\n    def __call__(self, inputs, targets):\n        loss_pointwise = targets * (torch.log(targets) - inputs)\n        if self.reduction == \"mean\":\n            loss = loss_pointwise.mean()\n        elif self.reduction == \"batchmean\":\n            loss = loss_pointwise.sum() / inputs.shape[0]\n        else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.common.loss",
        "documentation": {}
    },
    {
        "label": "Hourglass",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_aggregation.hourglass",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_aggregation.hourglass",
        "peekOfCode": "class Hourglass(nn.Module):\n    def __init__(self, in_channels, backbone_channels=None):\n        super(Hourglass, self).__init__()\n        if backbone_channels is None:\n            backbone_channels = [48, 64, 192, 120]\n        self.conv1 = nn.Sequential(\n            BasicConv3d(in_channels, in_channels * 2,\n                        norm_layer=nn.BatchNorm3d, act_layer=nn.LeakyReLU,\n                        kernel_size=3, padding=1, stride=2, dilation=1),\n            BasicConv3d(in_channels * 2, in_channels * 2,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_aggregation.hourglass",
        "documentation": {}
    },
    {
        "label": "CoExCostVolume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "class CoExCostVolume(nn.Module):\n    def __init__(self, maxdisp, group=1):\n        super(CoExCostVolume, self).__init__()\n        self.maxdisp = maxdisp + 1\n        self.group = group\n        self.unfold = nn.Unfold((1, maxdisp + 1), 1, 0, 1)\n        self.left_pad = nn.ZeroPad2d((maxdisp, 0, 0, 0))\n    def forward(self, x, y):\n        b, c, h, w = x.shape\n        y = self.left_pad(y)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "InterlacedVolume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "class InterlacedVolume(nn.Module):\n    def __init__(self, num_features=8):\n        super(InterlacedVolume, self).__init__()\n        self.num_features = num_features\n        self.conv3d = nn.Sequential(BasicConv3d(in_channels=1, out_channels=16,\n                                                norm_layer=nn.BatchNorm3d, act_layer=nn.ReLU,\n                                                kernel_size=(8, 3, 3), stride=(8, 1, 1), padding=(0, 1, 1)),\n                                    BasicConv3d(in_channels=16, out_channels=32,\n                                                norm_layer=nn.BatchNorm3d, act_layer=nn.ReLU,\n                                                kernel_size=(8, 3, 3), stride=(8, 1, 1), padding=(0, 1, 1)),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "correlation_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def correlation_volume(left_feature, right_feature, max_disp):\n    b, c, h, w = left_feature.size()\n    cost_volume = left_feature.new_zeros(b, max_disp, h, w)\n    for i in range(max_disp):\n        if i > 0:\n            cost_volume[:, i, :, i:] = (left_feature[:, :, :, i:] * right_feature[:, :, :, :-i]).mean(dim=1)\n        else:\n            cost_volume[:, i, :, :] = (left_feature * right_feature).mean(dim=1)\n    cost_volume = cost_volume.contiguous()\n    return cost_volume",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "compute_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def compute_volume(reference_embedding, target_embedding, maxdisp, side='left'):\n    batch, channel, height, width = reference_embedding.size()\n    cost = torch.zeros(batch, channel, maxdisp, height, width, device='cuda').type_as(reference_embedding)\n    cost[:, :, 0, :, :] = reference_embedding - target_embedding\n    for idx in range(1, maxdisp):\n        if side == 'left':\n            cost[:, :, idx, :, idx:] = reference_embedding[:, :, :, idx:] - target_embedding[:, :, :, :-idx]\n        if side == 'right':\n            cost[:, :, idx, :, :-idx] = target_embedding[:, :, :, idx:] - reference_embedding[:, :, :, :-idx]\n    cost = cost.contiguous()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W], requires_grad=False)\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_corr_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def build_corr_volume(img_left, img_right, max_disp):\n    B, C, H, W = img_left.shape\n    volume = img_left.new_zeros([B, max_disp, H, W])\n    for i in range(max_disp):\n        if (i > 0) & (i < W):\n            volume[:, i, :, i:] = (img_left[:, :, :, i:] * img_right[:, :, :, :W-i]).mean(dim=1)\n        else:\n            volume[:, i, :, :] = (img_left[:, :, :, :] * img_right[:, :, :, :]).mean(dim=1)\n    volume = volume.contiguous()\n    return volume",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_sub_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def build_sub_volume(feat_l, feat_r, maxdisp):\n    cost = torch.zeros((feat_l.size()[0], maxdisp, feat_l.size()[2], feat_l.size()[3]), device='cuda')\n    for i in range(maxdisp):\n        cost[:, i, :, :i] = feat_l[:, :, :, :i].abs().sum(1)\n        if i > 0:\n            cost[:, i, :, i:] = torch.norm(feat_l[:, :, :, i:] - feat_r[:, :, :, :-i], 1, 1)\n        else:\n            cost[:, i, :, i:] = torch.norm(feat_l[:, :, :, :] - feat_r[:, :, :, :], 1, 1)\n    return cost.contiguous()\nclass InterlacedVolume(nn.Module):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_pred.disp_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_pred.disp_regression",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=True)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_pred.disp_regression",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "class BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, padding, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = BasicConv2d(in_channels=in_planes, out_channels=out_planes,\n                                 norm_layer=nn.BatchNorm2d,\n                                 act_layer=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True),\n                                 kernel_size=3, stride=stride, padding=padding, dilation=dilation)\n        self.conv2 = BasicConv2d(in_channels=out_planes, out_channels=out_planes,\n                                 norm_layer=nn.BatchNorm2d, act_layer=None,\n                                 kernel_size=3, stride=1, padding=padding, dilation=dilation)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "StereoNetRefinement",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "class StereoNetRefinement(nn.Module):\n    def __init__(self):\n        super(StereoNetRefinement, self).__init__()\n        # Original StereoNet: left, disp\n        self.conv = BasicConv2d(4, 32,\n                                norm_layer=nn.BatchNorm2d,\n                                act_layer=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True),\n                                kernel_size=3, stride=1, padding=1)\n        self.dilation_list = [1, 2, 4, 8, 1, 1]\n        self.dilated_blocks = nn.ModuleList()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "StereoDRNetRefinement",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "class StereoDRNetRefinement(nn.Module):\n    def __init__(self):\n        super(StereoDRNetRefinement, self).__init__()\n        # Left and warped error\n        in_channels = 6\n        self.conv1 = BasicConv2d(in_channels, 16,\n                                norm_layer=nn.BatchNorm2d,\n                                act_layer=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True),\n                                kernel_size=3, stride=1, padding=1)\n        self.conv2 = BasicConv2d(1, 16,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "meshgrid",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "def meshgrid(img, homogeneous=False):\n    \"\"\"Generate meshgrid in image scale\n    Args:\n        img: [B, _, H, W]\n        homogeneous: whether to return homogeneous coordinates\n    Return:\n        grid: [B, 2, H, W]\n    \"\"\"\n    b, _, h, w = img.size()\n    x_range = torch.arange(0, w).view(1, 1, w).expand(1, h, w).type_as(img)  # [1, H, W]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "normalize_coords",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "def normalize_coords(grid):\n    \"\"\"Normalize coordinates of image scale to [-1, 1]\n    Args:\n        grid: [B, 2, H, W]\n    \"\"\"\n    assert grid.size(1) == 2\n    h, w = grid.size()[2:]\n    grid[:, 0, :, :] = 2 * (grid[:, 0, :, :].clone() / (w - 1)) - 1  # x: [-1, 1]\n    grid[:, 1, :, :] = 2 * (grid[:, 1, :, :].clone() / (h - 1)) - 1  # y: [-1, 1]\n    grid = grid.permute((0, 2, 3, 1))  # [B, H, W, 2]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "disp_warp",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "def disp_warp(img, disp, padding_mode='border'):\n    \"\"\"Warping by disparity\n    Args:\n        img: [B, 3, H, W]\n        disp: [B, 1, H, W], positive\n        padding_mode: 'zeros' or 'border'\n    Returns:\n        warped_img: [B, 3, H, W]\n        valid_mask: [B, 3, H, W]\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "context_upsample",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "def context_upsample(disp_low, up_weights, scale_factor=4):\n    # disp_low [b,1,h,w]\n    # up_weights [b,9,4*h,4*w]\n    b, c, h, w = disp_low.shape\n    disp_unfold = F.unfold(disp_low, kernel_size=3, dilation=1, padding=1)  # [bz, 3x3, hxw]\n    disp_unfold = disp_unfold.reshape(b, -1, h, w)  # [bz, 3x3, h, w]\n    disp_unfold = F.interpolate(disp_unfold, (h * scale_factor, w * scale_factor), mode='nearest')  # [bz, 3x3, 4h, 4w]\n    disp = (disp_unfold * up_weights).sum(1)  # # [bz, 4h, 4w]\n    return disp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "MultiBasicEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class MultiBasicEncoder(nn.Module):\n    def __init__(self, output_dim=None, norm_fn='batch', downsample=3):\n        super(MultiBasicEncoder, self).__init__()\n        if output_dim is None:\n            output_dim = [128]\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1 + (downsample > 2), padding=3)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.norm_fn = norm_fn\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "CombinedGeoEncodingVolume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class CombinedGeoEncodingVolume:\n    def __init__(self, init_fmap1, init_fmap2, geo_volume, num_levels=2, radius=4):\n        self.num_levels = num_levels\n        self.radius = radius\n        self.geo_volume_pyramid = []\n        self.init_corr_pyramid = []\n        # all pairs correlation\n        init_corr = self.corr(init_fmap1, init_fmap2)  # [bz, H/4, W/4, 1, W/4]\n        b, h, w, _, w2 = init_corr.shape\n        b, c, d, h, w = geo_volume.shape  # [bz, channel, maxdisp/4, H/4, W/4]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "BasicMotionEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class BasicMotionEncoder(nn.Module):\n    def __init__(self, corr_levels, corr_radius, volume_channel):\n        super(BasicMotionEncoder, self).__init__()\n        cor_planes = corr_levels * (2 * corr_radius + 1) * (volume_channel + 1)\n        self.convc1 = nn.Conv2d(cor_planes, 64, 1, padding=0)\n        self.convc2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.convd1 = nn.Conv2d(1, 64, 7, padding=3)\n        self.convd2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.conv = nn.Conv2d(64 + 64, 128 - 1, 3, padding=1)\n    def forward(self, disp, corr):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "ConvGRU",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class ConvGRU(nn.Module):\n    def __init__(self, hidden_dim, input_dim, kernel_size=3):\n        super(ConvGRU, self).__init__()\n        self.convz = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convq = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n    def forward(self, h, cz, cr, cq, *x_list):\n        x = torch.cat(x_list, dim=1)\n        hx = torch.cat([h, x], dim=1)\n        z = torch.sigmoid(self.convz(hx) + cz)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "DispHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):\n        super(DispHead, self).__init__()\n        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 3, padding=1)\n        self.conv2 = nn.Conv2d(hidden_dim, output_dim, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\ndef pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "BasicMultiUpdateBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []\n        self.n_gru_layers = n_gru_layers\n        self.encoder = BasicMotionEncoder(corr_levels, corr_radius, volume_channel)\n        encoder_output_dim = 128\n        self.gru04 = ConvGRU(hidden_dims[2], encoder_output_dim + hidden_dims[1] * (self.n_gru_layers > 1))\n        self.gru08 = ConvGRU(hidden_dims[1], hidden_dims[0] * (self.n_gru_layers == 3) + hidden_dims[2])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "def bilinear_sampler(img, coords, mask=False):\n    \"\"\" Wrapper for grid_sample, uses pixel coordinates \"\"\"\n    H, W = img.shape[-2:]\n    xgrid, ygrid = coords.split([1, 1], dim=-1)\n    xgrid = 2 * xgrid / (W - 1) - 1\n    assert torch.unique(ygrid).numel() == 1 and H == 1  # This is a stereo problem\n    grid = torch.cat([xgrid, ygrid], dim=-1)\n    img = F.grid_sample(img, grid, align_corners=True)\n    if mask:\n        mask = (xgrid > -1) & (ygrid > -1) & (xgrid < 1) & (ygrid < 1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "pool2x",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "def pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "interp",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "def interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []\n        self.n_gru_layers = n_gru_layers\n        self.encoder = BasicMotionEncoder(corr_levels, corr_radius, volume_channel)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "aanet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.aanet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.aanet",
        "peekOfCode": "class aanet(nn.Module):\n    def __init__(self, model_cfg):\n        super(aanet, self).__init__()\n        self.model_cfg = model_cfg\n        self.max_disp = model_cfg.MAX_DISP\n        self.refinement_type = model_cfg.REFINEMENT_TYPE\n        self.num_downsample = model_cfg.NUM_DOWNSAMPLE\n        self.aggregation_type = model_cfg.AGGREGATION_TYPE\n        self.num_scales = model_cfg.NUM_SCALES\n        self.no_feature_mdconv = model_cfg.NO_FEATURE_MDCONV",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.aanet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, leaky_relu=True):\n        \"\"\"StereoNet uses leaky relu (alpha = 0.2)\"\"\"\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride=stride, dilation=dilation)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class Bottleneck(nn.Module):\n    expansion = 4\n    __constants__ = ['downsample']\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "AANetFeature",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class AANetFeature(nn.Module):\n    def __init__(self, in_channels=32,\n                 zero_init_residual=True,\n                 groups=1,\n                 width_per_group=64,\n                 feature_mdconv=True,\n                 norm_layer=None):\n        super(AANetFeature, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "DeformConv2d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class DeformConv2d(nn.Module):\n    \"\"\"A single (modulated) deformable conv layer\"\"\"\n    def __init__(self, in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 stride=1,\n                 dilation=2,\n                 groups=1,\n                 deformable_groups=2,\n                 modulation=True,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "DeformBottleneck",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class DeformBottleneck(nn.Module):\n    expansion = 4\n    __constants__ = ['downsample']\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(DeformBottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "SimpleBottleneck",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class SimpleBottleneck(nn.Module):\n    \"\"\"Simple bottleneck block without channel expansion\"\"\"\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(SimpleBottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "DeformSimpleBottleneck",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class DeformSimpleBottleneck(nn.Module):\n    \"\"\"Used for cost aggregation\"\"\"\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, norm_layer=None,\n                 mdconv_dilation=2,\n                 deformable_groups=2,\n                 modulation=True,\n                 double_mask=True,\n                 ):\n        super(DeformSimpleBottleneck, self).__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "FeaturePyramidNetwork",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class FeaturePyramidNetwork(nn.Module):\n    def __init__(self, in_channels, out_channels=128,\n                 num_levels=3):\n        # FPN paper uses 256 out channels by default\n        super(FeaturePyramidNetwork, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        for i in range(num_levels):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "CostVolume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class CostVolume(nn.Module):\n    def __init__(self, max_disp, feature_similarity='correlation'):\n        \"\"\"Construct cost volume based on different\n        similarity measures\n        Args:\n            max_disp: max disparity candidate\n            feature_similarity: type of similarity measure\n        \"\"\"\n        super(CostVolume, self).__init__()\n        self.max_disp = max_disp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "CostVolumePyramid",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class CostVolumePyramid(nn.Module):\n    def __init__(self, max_disp, feature_similarity='correlation'):\n        super(CostVolumePyramid, self).__init__()\n        self.max_disp = max_disp\n        self.feature_similarity = feature_similarity\n    def forward(self, left_feature_pyramid, right_feature_pyramid):\n        num_scales = len(left_feature_pyramid)\n        cost_volume_pyramid = []\n        for s in range(num_scales):\n            max_disp = self.max_disp // (2 ** s)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "AdaptiveAggregationModule",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class AdaptiveAggregationModule(nn.Module):\n    def __init__(self, num_scales, num_output_branches, max_disp,\n                 num_blocks=1,\n                 simple_bottleneck=False,\n                 deformable_groups=2,\n                 mdconv_dilation=2):\n        super(AdaptiveAggregationModule, self).__init__()\n        self.num_scales = num_scales\n        self.num_output_branches = num_output_branches\n        self.max_disp = max_disp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "AdaptiveAggregation",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class AdaptiveAggregation(nn.Module):\n    def __init__(self, max_disp, num_scales=3, num_fusions=6,\n                 num_stage_blocks=1,\n                 num_deform_blocks=2,\n                 intermediate_supervision=True,\n                 deformable_groups=2,\n                 mdconv_dilation=2):\n        super(AdaptiveAggregation, self).__init__()\n        self.max_disp = max_disp\n        self.num_scales = num_scales",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "DisparityEstimation",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class DisparityEstimation(nn.Module):\n    def __init__(self, max_disp, match_similarity=True):\n        super(DisparityEstimation, self).__init__()\n        self.max_disp = max_disp\n        self.match_similarity = match_similarity\n    def forward(self, cost_volume):\n        assert cost_volume.dim() == 4\n        # Matching similarity or matching cost\n        cost_volume = cost_volume if self.match_similarity else -cost_volume\n        prob_volume = F.softmax(cost_volume, dim=1)  # [B, D, H, W]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "StereoDRNetRefinement",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class StereoDRNetRefinement(nn.Module):\n    def __init__(self):\n        super(StereoDRNetRefinement, self).__init__()\n        # Left and warped error\n        in_channels = 6\n        self.conv1 = conv2d(in_channels, 16)\n        self.conv2 = conv2d(1, 16)  # on low disparity\n        self.dilation_list = [1, 2, 4, 8, 1, 1]\n        self.dilated_blocks = nn.ModuleList()\n        for dilation in self.dilation_list:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1, with_bn_relu=False, leaky_relu=False):\n    \"\"\"3x3 convolution with padding\"\"\"\n    conv = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n    if with_bn_relu:\n        relu = nn.LeakyReLU(0.2, inplace=True) if leaky_relu else nn.ReLU(inplace=True)\n        conv = nn.Sequential(conv,\n                             nn.BatchNorm2d(out_planes),\n                             relu)\n    return conv",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\ndef conv2d(in_channels, out_channels, kernel_size=3, stride=1, dilation=1, groups=1):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                                   stride=stride, padding=dilation, dilation=dilation,\n                                   bias=False, groups=groups),\n                         nn.BatchNorm2d(out_channels),\n                         nn.LeakyReLU(0.2, inplace=True))\nclass BasicBlock(nn.Module):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "conv2d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def conv2d(in_channels, out_channels, kernel_size=3, stride=1, dilation=1, groups=1):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                                   stride=stride, padding=dilation, dilation=dilation,\n                                   bias=False, groups=groups),\n                         nn.BatchNorm2d(out_channels),\n                         nn.LeakyReLU(0.2, inplace=True))\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, leaky_relu=True):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "normalize_coords",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def normalize_coords(grid):\n    \"\"\"Normalize coordinates of image scale to [-1, 1]\n    Args:\n        grid: [B, 2, H, W]\n    \"\"\"\n    assert grid.size(1) == 2\n    h, w = grid.size()[2:]\n    grid[:, 0, :, :] = 2 * (grid[:, 0, :, :].clone() / (w - 1)) - 1  # x: [-1, 1]\n    grid[:, 1, :, :] = 2 * (grid[:, 1, :, :].clone() / (h - 1)) - 1  # y: [-1, 1]\n    grid = grid.permute((0, 2, 3, 1))  # [B, H, W, 2]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "meshgrid",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def meshgrid(img, homogeneous=False):\n    \"\"\"Generate meshgrid in image scale\n    Args:\n        img: [B, _, H, W]\n        homogeneous: whether to return homogeneous coordinates\n    Return:\n        grid: [B, 2, H, W]\n    \"\"\"\n    b, _, h, w = img.size()\n    x_range = torch.arange(0, w).view(1, 1, w).expand(1, h, w).type_as(img)  # [1, H, W]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "disp_warp",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def disp_warp(img, disp, padding_mode='border'):\n    \"\"\"Warping by disparity\n    Args:\n        img: [B, 3, H, W]\n        disp: [B, 1, H, W], positive\n        padding_mode: 'zeros' or 'border'\n    Returns:\n        warped_img: [B, 3, H, W]\n        valid_mask: [B, 3, H, W]\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.trainer",
        "peekOfCode": "__all__ = {\n    'AANet': aanet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.aanet.trainer",
        "documentation": {}
    },
    {
        "label": "hourglass",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "peekOfCode": "class hourglass(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass, self).__init__()\n        self.conv1 = nn.Sequential(convbn_3d(in_channels, in_channels * 2, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 2, 3, 1, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv3 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 4, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv4 = nn.Sequential(convbn_3d(in_channels * 4, in_channels * 4, 3, 1, 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self, arch_mode=\"nospp\", num_stage=None, concat_feature_channel=12):\n        super(feature_extraction, self).__init__()\n        assert arch_mode in [\"spp\", \"nospp\"]\n        self.arch_mode = arch_mode\n        self.num_stage = num_stage\n        self.inplanes = 32\n        self.concat_feature_channel = concat_feature_channel\n        # strid 1\n        self.firstconv_a = nn.Sequential(convbn(3, 32, 3, 1, 1, 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "CostAggregation",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "peekOfCode": "class CostAggregation(nn.Module):\n    def __init__(self, in_channels, base_channels=32):\n        super(CostAggregation, self).__init__()\n        self.dres0 = nn.Sequential(convbn_3d(in_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(base_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True))\n        self.dres1 = nn.Sequential(convbn_3d(base_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(base_channels, base_channels, 3, 1, 1))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "GetCostVolume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "peekOfCode": "class GetCostVolume(nn.Module):\n    def __init__(self):\n        super(GetCostVolume, self).__init__()\n    def get_warped_feats(self, x, y, disp_range_samples, ndisp):\n        bs, channels, height, width = y.size()\n        mh, mw = torch.meshgrid([torch.arange(0, height, dtype=x.dtype, device=x.device),\n                                 torch.arange(0, width, dtype=x.dtype, device=x.device)])  # (H *W)\n        mh = mh.reshape(1, 1, height, width).repeat(bs, ndisp, 1, 1)\n        mw = mw.reshape(1, 1, height, width).repeat(bs, ndisp, 1, 1)  # (B, D, H, W)\n        cur_disp_coords_y = mh",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "GwcNet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "peekOfCode": "class GwcNet(nn.Module):\n    def __init__(self, cfgs):\n        super(GwcNet, self).__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.ndisps = cfgs.NDISPS\n        self.disp_interval_pixel = cfgs.DISP_INTERVAL_PIXEL\n        self.num_stage = len(cfgs.NDISPS)\n        self.cr_base_chs = cfgs.CR_BASE_CHS\n        self.grad_method = cfgs.GRAD_METHOD\n        self.ns_size = cfgs.NS_SIZE",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "hourglass",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "peekOfCode": "class hourglass(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass, self).__init__()\n        self.conv1 = nn.Sequential(convbn_3d(in_channels, in_channels * 2, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 2, 3, 1, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv3 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 4, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv4 = nn.Sequential(convbn_3d(in_channels * 4, in_channels * 4, 3, 1, 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self, arch_mode=\"fpn\", num_stage=None):\n        super(feature_extraction, self).__init__()\n        assert arch_mode in [\"fpn\"]\n        self.inplanes = 32\n        self.arch_mode = arch_mode\n        self.num_stage = num_stage\n        # TODO: split modifiy\n        self.firstconv_a = nn.Sequential(convbn(3, 32, 3, 1, 1, 1),\n                                         nn.ReLU(inplace=True),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "CostAggregation",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "peekOfCode": "class CostAggregation(nn.Module):\n    def __init__(self, in_channels, base_channels=32):\n        super(CostAggregation, self).__init__()\n        self.dres0 = nn.Sequential(convbn_3d(in_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(base_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True))\n        self.dres1 = nn.Sequential(convbn_3d(base_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(base_channels, base_channels, 3, 1, 1))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "GetCostVolume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "peekOfCode": "class GetCostVolume(nn.Module):\n    def __init__(self):\n        super(GetCostVolume, self).__init__()\n    def forward(self, x, y, disp_range_samples, ndisp):\n        assert (x.is_contiguous() == True)\n        bs, channels, height, width = x.size()\n        cost = x.new().resize_(bs, channels * 2, ndisp, height, width).zero_()\n        # cost = y.unsqueeze(2).repeat(1, 2, ndisp, 1, 1) #(B, 2C, D, H, W)\n        mh, mw = torch.meshgrid([torch.arange(0, height, dtype=x.dtype, device=x.device),\n                                 torch.arange(0, width, dtype=x.dtype, device=x.device)])  # (H *W)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "PSMNet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "peekOfCode": "class PSMNet(nn.Module):\n    def __init__(self, cfgs):\n        super(PSMNet, self).__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.ndisps = cfgs.NDISPS\n        self.disp_interval_pixel = cfgs.DISP_INTERVAL_PIXEL\n        self.num_stage = len(cfgs.NDISPS)\n        self.cr_base_chs = cfgs.CR_BASE_CHS\n        self.grad_method = cfgs.GRAD_METHOD\n        self.ns_size = cfgs.NS_SIZE",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_channels))\ndef convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))\ndef disparity_regression(x, disp_values):\n    assert len(x.shape) == 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))\ndef disparity_regression(x, disp_values):\n    assert len(x.shape) == 4\n    return torch.sum(x * disp_values, 1, keepdim=False)\ndef build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def disparity_regression(x, disp_values):\n    assert len(x.shape) == 4\n    return torch.sum(x * disp_values, 1, keepdim=False)\ndef build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "get_cur_disp_range_samples",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def get_cur_disp_range_samples(cur_disp, ndisp, disp_inteval_pixel, shape, ns_size, using_ns=False, max_disp=192.0):\n    # shape, (B, H, W)\n    # cur_disp: (B, H, W)\n    # return disp_range_samples: (B, D, H, W)\n    if not using_ns:\n        cur_disp_min = (cur_disp - ndisp / 2 * disp_inteval_pixel)  # (B, H, W)\n        cur_disp_max = (cur_disp + ndisp / 2 * disp_inteval_pixel)\n        # cur_disp_min = (cur_disp - ndisp / 2 * disp_inteval_pixel).clamp(min=0.0)   #(B, H, W)\n        # cur_disp_max = (cur_disp_min + (ndisp - 1) * disp_inteval_pixel).clamp(max=max_disp)\n        assert cur_disp.shape == torch.Size(shape), \"cur_disp:{}, input shape:{}\".format(cur_disp.shape, shape)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "get_disp_range_samples",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def get_disp_range_samples(cur_disp, ndisp, disp_inteval_pixel, device, dtype, shape, using_ns, ns_size,\n                           max_disp=192.0):\n    # shape, (B, H, W)\n    # cur_disp: (B, H, W) or float\n    # return disp_range_values: (B, D, H, W)\n    # with torch.no_grad():\n    if cur_disp is None:\n        cur_disp = torch.tensor(0, device=device, dtype=dtype, requires_grad=False).reshape(1, 1, 1).repeat(*shape)\n        cur_disp_min = (cur_disp - ndisp / 2 * disp_inteval_pixel).clamp(min=0.0)  # (B, H, W)\n        cur_disp_max = (cur_disp_min + (ndisp - 1) * disp_inteval_pixel).clamp(max=max_disp)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "Align_Corners",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "Align_Corners = False\nAlign_Corners_Range = False\ndef convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_channels))\ndef convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "Align_Corners_Range",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "peekOfCode": "Align_Corners_Range = False\ndef convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_channels))\ndef convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))\ndef disparity_regression(x, disp_values):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.trainer",
        "peekOfCode": "__all__ = {\n    'CasGwcNet': CasGwcNet,\n    'CasPSMNet': CasPSMNet\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.casnet.trainer",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self, concat_feature=False, concat_feature_channel=12):\n        super(feature_extraction, self).__init__()\n        self.concat_feature = concat_feature\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn(3, 32, 3, 2, 1, 1),\n                                       Mish(),\n                                       convbn(32, 32, 3, 1, 1, 1),\n                                       Mish(),\n                                       convbn(32, 32, 3, 1, 1, 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "hourglassup",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "class hourglassup(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglassup, self).__init__()\n        self.conv1 = nn.Conv3d(in_channels, in_channels * 2, kernel_size=3, stride=2,\n                               padding=1, bias=False)\n        self.conv2 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 2, 3, 1, 1),\n                                   Mish())\n        self.conv3 = nn.Conv3d(in_channels * 2, in_channels * 4, kernel_size=3, stride=2,\n                               padding=1, bias=False)\n        self.conv4 = nn.Sequential(convbn_3d(in_channels * 4, in_channels * 4, 3, 1, 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "hourglass",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "class hourglass(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass, self).__init__()\n        self.conv1 = nn.Sequential(convbn_3d(in_channels, in_channels * 2, 3, 2, 1),\n                                   Mish())\n        self.conv2 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 2, 3, 1, 1),\n                                   Mish())\n        self.conv3 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 4, 3, 2, 1),\n                                   Mish())\n        self.conv4 = nn.Sequential(convbn_3d(in_channels * 4, in_channels * 4, 3, 1, 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "cfnet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "class cfnet(nn.Module):\n    def __init__(self, cfgs, use_concat_volume=False):\n        super(cfnet, self).__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.use_concat_volume = True\n        self.v_scale_s1 = 1\n        self.v_scale_s2 = 2\n        self.v_scale_s3 = 3\n        self.sample_count_s1 = 6\n        self.sample_count_s2 = 10",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "CFNet",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "def CFNet(d, replace_mish=False):\n    net = cfnet(d, use_concat_volume=True)\n    if replace_mish:\n        replace_layers(net, Mish, nn.ReLU(inplace=True))\n        print('replacing', Mish(), '->', nn.ReLU())\n    return net\ndef replace_layers(model, old, new):\n    for n, module in model.named_children():\n        if len(list(module.children())) > 0:\n            ## compound module, go inside it",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "replace_layers",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "def replace_layers(model, old, new):\n    for n, module in model.named_children():\n        if len(list(module.children())) > 0:\n            ## compound module, go inside it\n            replace_layers(module, old, new)\n        if isinstance(module, old):\n            ## simple module\n            setattr(model, n, new)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "pyramidPooling",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class pyramidPooling(nn.Module):\n    def __init__(self, in_channels, pool_sizes, model_name='pspnet', fusion_mode='cat', with_bn=True):\n        super(pyramidPooling, self).__init__()\n        bias = not with_bn\n        self.paths = []\n        if pool_sizes is None:\n            for i in range(4):\n                self.paths.append(conv2DBatchNormRelu(in_channels, in_channels, 1, 1, 0, bias=bias, with_bn=with_bn))\n        else:\n            for i in range(len(pool_sizes)):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv2DBatchNormRelu",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class conv2DBatchNormRelu(nn.Module):\n    def __init__(self, in_channels, n_filters, k_size, stride, padding, bias=True, dilation=1, with_bn=True):\n        super(conv2DBatchNormRelu, self).__init__()\n        if dilation > 1:\n            conv_mod = nn.Conv2d(int(in_channels), int(n_filters), kernel_size=k_size,\n                                 padding=padding, stride=stride, bias=bias, dilation=dilation)\n        else:\n            conv_mod = nn.Conv2d(int(in_channels), int(n_filters), kernel_size=k_size,\n                                 padding=padding, stride=stride, bias=bias, dilation=1)\n        if with_bn:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "Mish",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # print(\"Mish activation loaded...\")\n    def forward(self, x):\n        # save 1 second per epoch with no x= x*() and then return x...just inline it.\n        return x * (torch.tanh(F.softplus(x)))\ndef convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   Mish())\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n        self.downsample = downsample\n        self.stride = stride\n        # self.gc = ContextBlock2d(planes, planes // 8, 'att', ['channel_add'])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "UniformSampler",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class UniformSampler(nn.Module):\n    def __init__(self):\n        super(UniformSampler, self).__init__()\n    def forward(self, min_disparity, max_disparity, number_of_samples=10):\n        \"\"\"\n        Args:\n            :min_disparity: lower bound of disparity search range\n            :max_disparity: upper bound of disparity range predictor\n            :number_of_samples (default:10): number of samples to be genearted.\n        Returns:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "SpatialTransformer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class SpatialTransformer(nn.Module):\n    def __init__(self):\n        super(SpatialTransformer, self).__init__()\n    def forward(self, left_input, right_input, disparity_samples):\n        \"\"\"\n        Disparity Sample Cost Evaluator\n        Description:\n                Given the left image features, right iamge features and the disparity samples, generates:\n                    - Warped right image features\n        Args:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_channels))\ndef convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))\ndef disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))\ndef disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=False)\ndef disparity_variance(x, maxdisp, disparity):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=False)\ndef disparity_variance(x, maxdisp, disparity):\n    # the shape of disparity should be B,1,H,W, return is the variance of the cost volume [B,1,H,W]\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_variance",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def disparity_variance(x, maxdisp, disparity):\n    # the shape of disparity should be B,1,H,W, return is the variance of the cost volume [B,1,H,W]\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    disp_values = (disp_values - disparity) ** 2\n    return torch.sum(x * disp_values, 1, keepdim=True)\ndef disparity_variance_confidence(x, disparity_samples, disparity):\n    # the shape of disparity should be B,1,H,W, return is the uncertainty estimation\n    assert len(x.shape) == 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_variance_confidence",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def disparity_variance_confidence(x, disparity_samples, disparity):\n    # the shape of disparity should be B,1,H,W, return is the uncertainty estimation\n    assert len(x.shape) == 4\n    disp_values = (disparity - disparity_samples) ** 2\n    return torch.sum(x * disp_values, 1, keepdim=True)\ndef build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef groupwise_correlation_4D(fea1, fea2, num_groups):\n    B, C, D, H, W = fea1.shape\n    assert C % num_groups == 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation_4D",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def groupwise_correlation_4D(fea1, fea2, num_groups):\n    B, C, D, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, D, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, D, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_corrleation_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def build_corrleation_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, 2 * maxdisp + 1, H, W])\n    for i in range(-maxdisp, maxdisp + 1):\n        if i > 0:\n            volume[:, :, i + maxdisp, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:],\n                                                                     targetimg_fea[:, :, :, :-i],\n                                                                     num_groups)\n        elif i < 0:\n            volume[:, :, i + maxdisp, :, :-i] = groupwise_correlation(refimg_fea[:, :, :, :-i],",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "warp",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def warp(x, disp):\n    \"\"\"\n    warp an image/tensor (imright) back to imleft, according to the disp\n    x: [B, C, H, W] (imright)\n    disp: [B, 1, H, W] disp\n    \"\"\"\n    B, C, H, W = x.size()\n    # mesh grid\n    xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n    yy = torch.arange(0, H).view(-1, 1).repeat(1, W)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "FMish",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def FMish(x):\n    '''\n    Applies the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n    See additional documentation for mish class.\n    '''\n    return x * torch.tanh(F.softplus(x))\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.trainer",
        "peekOfCode": "__all__ = {\n    'CFNet': cfnet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.cfnet.trainer",
        "documentation": {}
    },
    {
        "label": "CoEx",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex",
        "peekOfCode": "class CoEx(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.max_disp = cfgs.MAX_DISP\n        spixel_branch_channels = cfgs.SPIXEL_BRANCH_CHANNELS\n        chans = cfgs.CHANS\n        matching_weighted = cfgs.MATCHING_WEIGHTED\n        matching_head = cfgs.MATCHING_HEAD\n        gce = cfgs.GCE\n        aggregation_disp_strides = cfgs.AGGREGATION_DISP_STRIDES",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex",
        "documentation": {}
    },
    {
        "label": "FeatUp",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_backbone",
        "peekOfCode": "class FeatUp(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.cfg = cfg['backbone']\n        chans = [16, 24, 32, 96, 160]\n        self.deconv32_16 = Conv2x(chans[4], chans[3], deconv=True, concat=True)\n        self.deconv16_8 = Conv2x(chans[3] * 2, chans[2], deconv=True, concat=True)\n        self.deconv8_4 = Conv2x(chans[2] * 2, chans[1], deconv=True, concat=True)\n        self.conv4 = BasicConv(chans[1] * 2, chans[1] * 2, kernel_size=3, stride=1, padding=1)\n        self.weight_init()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_backbone",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_backbone",
        "peekOfCode": "class Feature(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.type = 'mobilenetv2_100'\n        layers = [1, 2, 3, 5, 6]\n        self.pre_trained = True\n        model = timm.create_model(self.type, pretrained=self.pre_trained, features_only=True)\n        self.conv_stem = model.conv_stem\n        self.bn1 = model.bn1\n        self.block0 = torch.nn.Sequential(*model.blocks[0:layers[0]])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_backbone",
        "documentation": {}
    },
    {
        "label": "CoExBackbone",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_backbone",
        "peekOfCode": "class CoExBackbone(nn.Module):\n    def __init__(self, spixel_branch_channels=[32, 48]):\n        super().__init__()\n        self.feat = Feature()\n        self.up = FeatUp()\n        self.spixel_branch_channels = spixel_branch_channels\n        self.stem_2 = nn.Sequential(\n            BasicConv(3, self.spixel_branch_channels[0], kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(self.spixel_branch_channels[0], self.spixel_branch_channels[0], 3, 1, 1, bias=False),\n            nn.BatchNorm2d(self.spixel_branch_channels[0]),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_backbone",
        "documentation": {}
    },
    {
        "label": "CostVolume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "peekOfCode": "class CostVolume(nn.Module):\n    def __init__(self, maxdisp, glue=False, group=1):\n        super(CostVolume, self).__init__()\n        self.maxdisp = maxdisp + 1\n        self.glue = glue\n        self.group = group\n        self.unfold = nn.Unfold((1, maxdisp + 1), 1, 0, 1)\n        self.left_pad = nn.ZeroPad2d((maxdisp, 0, 0, 0))\n    def forward(self, x, y, v=None):\n        b, c, h, w = x.shape",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "documentation": {}
    },
    {
        "label": "AttentionCostVolume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "peekOfCode": "class AttentionCostVolume(nn.Module):\n    def __init__(self, max_disparity, in_chan, hidden_chan, head=1, weighted=False):\n        super(AttentionCostVolume, self).__init__()\n        self.costVolume = CostVolume(int(max_disparity // 4), False, head)\n        self.conv = BasicConv(in_chan, hidden_chan, kernel_size=3, padding=1, stride=1)\n        self.desc = nn.Conv2d(hidden_chan, hidden_chan, kernel_size=1, padding=0, stride=1)\n        self.head = head\n        self.weighted = weighted\n        if weighted:\n            self.weights = nn.Parameter(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "documentation": {}
    },
    {
        "label": "channelAtt",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "peekOfCode": "class channelAtt(nn.Module):\n    def __init__(self, cv_chan, im_chan, D):\n        super(channelAtt, self).__init__()\n        self.im_att = nn.Sequential(\n            BasicConv(im_chan, im_chan // 2, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(im_chan // 2, cv_chan, 1))\n        self.weight_init()\n    def forward(self, cv, im):\n        channel_att = self.im_att(im).unsqueeze(2)\n        cv = torch.sigmoid(channel_att) * cv",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "documentation": {}
    },
    {
        "label": "Aggregation",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "peekOfCode": "class Aggregation(nn.Module):\n    def __init__(\n            self,\n            max_disparity=192,\n            matching_head=1,\n            gce=True,\n            disp_strides=2,\n            channels=[16, 32, 48],\n            blocks_num=[2, 2, 2],\n            spixel_branch_channels=[32, 48]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "documentation": {}
    },
    {
        "label": "CoExCostProcessor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "peekOfCode": "class CoExCostProcessor(nn.Module):\n    def __init__(\n            self,\n            max_disp=192,\n            gce=True,\n            matching_weighted=False,\n            spixel_branch_channels=[32, 48],\n            matching_head=1,\n            aggregation_disp_strides=2,\n            aggregation_channels=[16, 32, 48],",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_cost_processor",
        "documentation": {}
    },
    {
        "label": "Regression",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_disp_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_disp_processor",
        "peekOfCode": "class Regression(nn.Module):\n    def __init__(self, max_disparity=192, top_k=2):\n        super(Regression, self).__init__()\n        self.D = int(max_disparity // 4)\n        self.top_k = top_k\n        self.ind_init = False\n    def forward(self, cost, spg):\n        b, _, h, w = spg.shape\n        corr, disp = self.topkpool(cost, self.top_k)\n        corr = F.softmax(corr, 2)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_disp_processor",
        "documentation": {}
    },
    {
        "label": "CoExDispProcessor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_disp_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_disp_processor",
        "peekOfCode": "class CoExDispProcessor(nn.Module):\n    def __init__(self, max_disp=192, regression_topk=2, chans =[16, 24, 32, 96, 160]):\n        super().__init__()\n        self.max_disp = max_disp\n        self.regression_topk = regression_topk\n        self.chans = chans\n        self.spixel_branch_channels = [32, 48]\n        self.spx = nn.Sequential(nn.ConvTranspose2d(2 * 32, 9, kernel_size=4, stride=2, padding=1), )\n        self.spx_2 = Conv2x(self.chans[1], 32, True)\n        self.spx_4 = nn.Sequential(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_disp_processor",
        "documentation": {}
    },
    {
        "label": "upfeat",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_disp_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_disp_processor",
        "peekOfCode": "def upfeat(input, prob, up_h=2, up_w=2):\n    b, c, h, w = input.shape\n    feat = F.unfold(input, 3, 1, 1).reshape(b, -1, h, w)\n    feat = F.interpolate(\n        feat, (h * up_h, w * up_w), mode='nearest').reshape(\n        b, -1, 9, h * up_h, w * up_w)\n    feat_sum = (feat * prob.unsqueeze(1)).sum(2)\n    return feat_sum\nclass Regression(nn.Module):\n    def __init__(self, max_disparity=192, top_k=2):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.coex_disp_processor",
        "documentation": {}
    },
    {
        "label": "PSMBasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "class PSMBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(PSMBasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "class BasicConv(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, bn=True, relu=True, **kwargs):\n        super(BasicConv, self).__init__()\n        self.relu = relu\n        self.use_bn = bn\n        if is_3d:\n            if deconv:\n                self.conv = nn.ConvTranspose3d(in_channels, out_channels, bias=False, **kwargs)\n            else:\n                self.conv = nn.Conv3d(in_channels, out_channels, bias=False, **kwargs)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "Conv2x",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "class Conv2x(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, concat=True, keep_concat=True, bn=True,\n                 relu=True, keep_dispc=False):\n        super(Conv2x, self).__init__()\n        self.concat = concat\n        self.is_3d = is_3d\n        if deconv and is_3d:\n            kernel = (4, 4, 4)\n        elif deconv:\n            kernel = 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(\n            self,\n            inplanes: int,\n            planes: int,\n            stride: int = 1,\n            downsample: Optional[nn.Module] = None,\n            groups: int = 1,\n            base_width: int = 64,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "ConvBNReLU3d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "class ConvBNReLU3d(nn.Sequential):\n    def __init__(\n            self,\n            in_planes: int,\n            out_planes: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            groups: int = 1,\n            norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "InvertedResidual3d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "class InvertedResidual3d(nn.Module):\n    def __init__(\n            self,\n            inp: int,\n            oup: int,\n            stride: int,\n            expand_ratio: int,\n            norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(InvertedResidual3d, self).__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "AtrousBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "class AtrousBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, bn=True, relu=True):\n        super(AtrousBlock, self).__init__()\n        dilations = [2, 4, 6]\n        self.conv_1 = BasicConv(in_channels, out_channels // 4, is_3d=True, kernel_size=3, stride=stride, padding=1,\n                                dilation=1)\n        self.conv_2 = BasicConv(in_channels, out_channels // 4, is_3d=True, kernel_size=3, stride=stride,\n                                padding=(1, dilations[0], dilations[0]), dilation=(1, dilations[0], dilations[0]))\n        self.conv_3 = BasicConv(in_channels, out_channels // 4, is_3d=True, kernel_size=3, stride=stride,\n                                padding=(1, dilations[1], dilations[1]), dilation=(1, dilations[1], dilations[1]))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "def convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                  padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n        nn.BatchNorm2d(out_planes)\n    )\nclass PSMBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(PSMBasicBlock, self).__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "def BasicConv2d(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                  padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(inplace=True, negative_slope=0.2),\n    )\ndef BasicTransposeConv2d(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    output_pad = stride + 2 * pad - kernel_size * dilation + dilation - 1\n    return nn.Sequential(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicTransposeConv2d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "def BasicTransposeConv2d(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    output_pad = stride + 2 * pad - kernel_size * dilation + dilation - 1\n    return nn.Sequential(\n        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, pad, output_pad, dilation, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(inplace=True, negative_slope=0.2),\n    )\ndef BasicConv3d(in_channels, out_channels, kernel_size, stride, pad, dilation=1):\n    return nn.Sequential(\n        nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "def BasicConv3d(in_channels, out_channels, kernel_size, stride, pad, dilation=1):\n    return nn.Sequential(\n        nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                  padding=pad, dilation=dilation, bias=False),\n        nn.BatchNorm3d(out_channels),\n        nn.LeakyReLU(inplace=True, negative_slope=0.2),\n    )\ndef BasicTransposeConv3d(in_channels, out_channels, kernel_size, stride, pad, output_pad=0, dilation=1):\n    # output_pad = stride + 2 * pad - kernel_size * dilation + dilation - 1\n    return nn.Sequential(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicTransposeConv3d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "def BasicTransposeConv3d(in_channels, out_channels, kernel_size, stride, pad, output_pad=0, dilation=1):\n    # output_pad = stride + 2 * pad - kernel_size * dilation + dilation - 1\n    return nn.Sequential(\n        nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride,\n                           pad, output_pad, dilation, bias=False),\n        nn.BatchNorm3d(out_channels),\n        nn.LeakyReLU(inplace=True, negative_slope=0.2),\n    )\ndef conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\nclass BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "peekOfCode": "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\nclass BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(\n            self,\n            inplanes: int,\n            planes: int,\n            stride: int = 1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.trainer",
        "peekOfCode": "__all__ = {\n    'CoExNet': CoExNet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.coex.trainer",
        "documentation": {}
    },
    {
        "label": "FADNet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet",
        "peekOfCode": "class FADNet(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        resBlock = cfgs.RESBLOCK\n        input_channel = cfgs.INPUT_CHANNEL\n        encoder_ratio = cfgs.ENCODER_RATIO\n        decoder_ratio = cfgs.DECODER_RATIO\n        in_planes = cfgs.IN_PLANES\n        self.backbone = FadnetBackbone(resBlock=resBlock, maxdisp=self.maxdisp, input_channel=input_channel,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet",
        "documentation": {}
    },
    {
        "label": "FadnetBackbone",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet_backbone",
        "peekOfCode": "class FadnetBackbone(nn.Module):\n    def __init__(self, resBlock=True, maxdisp=192, input_channel=3, encoder_ratio=16, decoder_ratio=16):\n        super(FadnetBackbone, self).__init__()\n        self.input_channel = input_channel\n        self.maxdisp = maxdisp\n        self.relu = nn.ReLU(inplace=False)\n        self.basicC = 2\n        self.eratio = encoder_ratio\n        self.dratio = decoder_ratio\n        self.basicE = self.basicC*self.eratio",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet_backbone",
        "documentation": {}
    },
    {
        "label": "FADAggregator",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet_cost_processor",
        "peekOfCode": "class FADAggregator(nn.Module):\n    def __init__(self, resBlock=True, maxdisp=192, input_channel=3, encoder_ratio=16, decoder_ratio=16):\n        super(FADAggregator, self).__init__()\n        self.input_channel = input_channel\n        self.maxdisp = maxdisp\n        self.relu = nn.ReLU(inplace=False)\n        self.basicC = 2\n        self.eratio = encoder_ratio\n        self.dratio = decoder_ratio\n        self.basicE = self.basicC*self.eratio",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "DispNetRes",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet_disp_predictor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet_disp_predictor",
        "peekOfCode": "class DispNetRes(nn.Module):\n    def __init__(self, in_planes, resBlock=True, input_channel=3, encoder_ratio=16, decoder_ratio=16):\n        super(DispNetRes, self).__init__()\n        self.input_channel = input_channel\n        self.basicC = 2\n        self.eratio = encoder_ratio\n        self.dratio = decoder_ratio\n        self.basicE = self.basicC*self.eratio\n        self.basicD = self.basicC*self.dratio\n        self.resBlock = resBlock",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.fadnet_disp_predictor",
        "documentation": {}
    },
    {
        "label": "HorizontalPoolingPyramid",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class HorizontalPoolingPyramid():\n    \"\"\"\n        Horizontal Pyramid Matching for Person Re-identification\n        Arxiv: https://arxiv.org/abs/1804.05275\n        Github: https://github.com/SHI-Labs/Horizontal-Pyramid-Matching\n    \"\"\"\n    def __init__(self, bin_num=None):\n        if bin_num is None:\n            bin_num = [16, 8, 4, 2, 1]\n        self.bin_num = bin_num",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "SetBlockWrapper",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class SetBlockWrapper(nn.Module):\n    def __init__(self, forward_block):\n        super(SetBlockWrapper, self).__init__()\n        self.forward_block = forward_block\n    def forward(self, x, *args, **kwargs):\n        \"\"\"\n            In  x: [n, c_in, s, h_in, w_in]\n            Out x: [n, c_out, s, h_out, w_out]\n        \"\"\"\n        n, c, s, h, w = x.size()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "PackSequenceWrapper",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class PackSequenceWrapper(nn.Module):\n    def __init__(self, pooling_func):\n        super(PackSequenceWrapper, self).__init__()\n        self.pooling_func = pooling_func\n    def forward(self, seqs, seqL, dim=2, options={}):\n        \"\"\"\n            In  seqs: [n, c, s, ...]\n            Out rets: [n, ...]\n        \"\"\"\n        if seqL is None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class BasicConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n                              stride=stride, padding=padding, bias=False, **kwargs)\n    def forward(self, x):\n        x = self.conv(x)\n        return x\nclass SeparateFCs(nn.Module):\n    def __init__(self, parts_num, in_channels, out_channels, norm=False):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "SeparateFCs",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class SeparateFCs(nn.Module):\n    def __init__(self, parts_num, in_channels, out_channels, norm=False):\n        super(SeparateFCs, self).__init__()\n        self.p = parts_num\n        self.fc_bin = nn.Parameter(\n            nn.init.xavier_uniform_(\n                torch.zeros(parts_num, in_channels, out_channels)))\n        self.norm = norm\n    def forward(self, x):\n        \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "SeparateBNNecks",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class SeparateBNNecks(nn.Module):\n    \"\"\"\n        GaitSet: Bag of Tricks and a Strong Baseline for Deep Person Re-Identification\n        CVPR Workshop:  https://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf\n        Github: https://github.com/michuanhaohao/reid-strong-baseline\n    \"\"\"\n    def __init__(self, parts_num, in_channels, class_num, norm=True, parallel_BN1d=True):\n        super(SeparateBNNecks, self).__init__()\n        self.p = parts_num\n        self.class_num = class_num",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "FocalConv2d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class FocalConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, halving, **kwargs):\n        super(FocalConv2d, self).__init__()\n        self.halving = halving\n        self.conv = nn.Conv2d(in_channels, out_channels,\n                              kernel_size, bias=False, **kwargs)\n    def forward(self, x):\n        if self.halving == 0:\n            z = self.conv(x)\n        else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class BasicConv3d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False, **kwargs):\n        super(BasicConv3d, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size,\n                                stride=stride, padding=padding, bias=bias, **kwargs)\n    def forward(self, ipts):\n        '''\n            ipts: [n, c, s, h, w]\n            outs: [n, c, s, h, w]\n        '''",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "GaitAlign",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class GaitAlign(nn.Module):\n    def __init__(self, H=64, W=44, eps=1, **kwargs):\n        super(GaitAlign, self).__init__()\n        self.H, self.W, self.eps = H, W, eps\n        self.Pad = nn.ZeroPad2d((int(self.W / 2), int(self.W / 2), 0, 0))\n        self.RoiPool = RoIAlign((self.H, self.W), 1, sampling_ratio=-1)\n    def forward(self, feature_map, binary_mask, w_h_ratio):\n        \"\"\"\n           In  sils:         [n, c, h, w]\n               w_h_ratio:    [n, 1]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "DynamicConv2d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class DynamicConv2d(nn.Module):\n    def __init__(self, max_in_channels, max_out_channels, kernel_size=1, stride=1, dilation=1):\n        super(DynamicConv2d, self).__init__()\n        self.max_in_channels = max_in_channels\n        self.max_out_channels = max_out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n        self.conv = nn.Conv2d(\n            self.max_in_channels, self.max_out_channels, self.kernel_size, stride=self.stride, bias=False,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "DyRes",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class DyRes(nn.Module):\n    def __init__(self, max_in=98, max_out=128, stride = 1):\n        super(DyRes, self).__init__()\n        self.conv1 = DynamicConv2d(max_in, max_out, kernel_size = 3, stride = stride)\n        self.bn1 = nn.BatchNorm2d(max_out)\n        self.relu = nn.ReLU(inplace = True)\n        self.conv2 = nn.Conv2d(max_out, max_out, kernel_size = 3, padding = 1)\n        self.bn2 = nn.BatchNorm2d(max_out)\n        self.stride = stride\n        self.max_out = max_out",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class ResBlock(nn.Module):\n    def __init__(self, n_in, n_out, stride = 1):\n        super(ResBlock, self).__init__()\n        self.conv1 = nn.Conv2d(n_in, n_out, kernel_size = 3, stride = stride, padding = 1)\n        self.bn1 = nn.BatchNorm2d(n_out)\n        self.relu = nn.ReLU(inplace = True)\n        self.conv2 = nn.Conv2d(n_out, n_out, kernel_size = 3, padding = 1)\n        self.bn2 = nn.BatchNorm2d(n_out)\n        if stride != 1 or n_out != n_in:\n            self.shortcut = nn.Sequential(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "matchshifted",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class matchshifted(nn.Module):\n    def __init__(self):\n        super(matchshifted, self).__init__()\n    def forward(self, left, right, shift):\n        batch, filters, height, width = left.size()\n        shifted_left  = F.pad(torch.index_select(left,  3, Variable(torch.LongTensor([i for i in range(shift,width)])).cuda()),(shift,0,0,0))\n        shifted_right = F.pad(torch.index_select(right, 3, Variable(torch.LongTensor([i for i in range(width-shift)])).cuda()),(shift,0,0,0))\n        out = torch.cat((shifted_left,shifted_right),1).view(batch,filters*2,1,height,width)\n        return out\nclass disparityregression(nn.Module):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparityregression",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class disparityregression(nn.Module):\n    def __init__(self, maxdisp):\n        super(disparityregression, self).__init__()\n        self.disp = Variable(torch.Tensor(np.reshape(np.array(range(maxdisp)),[1,maxdisp,1,1])).cuda(), requires_grad=False)\n    def forward(self, x):\n        disp = self.disp.repeat(x.size()[0],1,x.size()[2],x.size()[3])\n        out = torch.sum(x*disp,1)\n        return out\ndef disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self):\n        super(feature_extraction, self).__init__()\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn(3, 32, 3, 2, 1, 1),\n                                       nn.ReLU(inplace=True),\n                                       convbn(32, 32, 3, 1, 1, 1),\n                                       nn.ReLU(inplace=True),\n                                       convbn(32, 32, 3, 1, 1, 1),\n                                       nn.ReLU(inplace=True))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "tofp16",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class tofp16(nn.Module):\n    def __init__(self):\n        super(tofp16, self).__init__()\n    def forward(self, input):\n        return input.half()\nclass tofp32(nn.Module):\n    def __init__(self):\n        super(tofp32, self).__init__()\n    def forward(self, input):\n        return input.float()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "tofp32",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class tofp32(nn.Module):\n    def __init__(self):\n        super(tofp32, self).__init__()\n    def forward(self, input):\n        return input.float()\ndef init_deconv_bilinear(weight):\n    f_shape = weight.size()\n    heigh, width = f_shape[-2], f_shape[-1]\n    f = np.ceil(width/2.0)\n    c = (2 * f - 1 - f % 2) / (2.0 * f)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "clones",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def clones(module, N):\n    \"Produce N identical layers.\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\ndef is_list_or_tuple(x):\n    return isinstance(x, (list, tuple))\nclass HorizontalPoolingPyramid():\n    \"\"\"\n        Horizontal Pyramid Matching for Person Re-identification\n        Arxiv: https://arxiv.org/abs/1804.05275\n        Github: https://github.com/SHI-Labs/Horizontal-Pyramid-Matching",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "is_list_or_tuple",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def is_list_or_tuple(x):\n    return isinstance(x, (list, tuple))\nclass HorizontalPoolingPyramid():\n    \"\"\"\n        Horizontal Pyramid Matching for Person Re-identification\n        Arxiv: https://arxiv.org/abs/1804.05275\n        Github: https://github.com/SHI-Labs/Horizontal-Pyramid-Matching\n    \"\"\"\n    def __init__(self, bin_num=None):\n        if bin_num is None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "RmBN2dAffine",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def RmBN2dAffine(model):\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.weight.requires_grad = False\n            m.bias.requires_grad = False\n# freda (todo) : \nclass DynamicConv2d(nn.Module):\n    def __init__(self, max_in_channels, max_out_channels, kernel_size=1, stride=1, dilation=1):\n        super(DynamicConv2d, self).__init__()\n        self.max_in_channels = max_in_channels",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def conv(in_planes, out_planes, kernel_size=3, stride=1, batchNorm=False):\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.LeakyReLU(0.1,inplace=True)\n        )\n    else:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "i_conv",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def i_conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, bias = True):\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=bias),\n            nn.BatchNorm2d(out_planes),\n        )\n    else:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=bias),\n        )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=dilation if dilation > 1 else pad, dilation = dilation, bias=False),\n                         nn.BatchNorm2d(out_planes))\ndef convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),\n                         nn.BatchNorm3d(out_planes))\ndef predict_flow(in_planes, out_planes = 1):\n    return nn.Conv2d(in_planes,out_planes,kernel_size=3,stride=1,padding=1,bias=False)\n#def predict_flow(in_planes):\n#    return nn.Conv2d(in_planes,1,kernel_size=1,stride=1,padding=0,bias=False)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),\n                         nn.BatchNorm3d(out_planes))\ndef predict_flow(in_planes, out_planes = 1):\n    return nn.Conv2d(in_planes,out_planes,kernel_size=3,stride=1,padding=1,bias=False)\n#def predict_flow(in_planes):\n#    return nn.Conv2d(in_planes,1,kernel_size=1,stride=1,padding=0,bias=False)\n#def corr(in_planes, max_disp=40):\n#    return Correlation1d(pad_size=max_disp, kernel_size=1, max_displacement=max_disp, stride1=1, stride2=2, corr_multiply=1)\ndef build_corr(img_left, img_right, max_disp=40, zero_volume=None):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "predict_flow",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def predict_flow(in_planes, out_planes = 1):\n    return nn.Conv2d(in_planes,out_planes,kernel_size=3,stride=1,padding=1,bias=False)\n#def predict_flow(in_planes):\n#    return nn.Conv2d(in_planes,1,kernel_size=1,stride=1,padding=0,bias=False)\n#def corr(in_planes, max_disp=40):\n#    return Correlation1d(pad_size=max_disp, kernel_size=1, max_displacement=max_disp, stride1=1, stride2=2, corr_multiply=1)\ndef build_corr(img_left, img_right, max_disp=40, zero_volume=None):\n    B, C, H, W = img_left.shape\n    if zero_volume is not None:\n        tmp_zero_volume = zero_volume #* 0.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_corr",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def build_corr(img_left, img_right, max_disp=40, zero_volume=None):\n    B, C, H, W = img_left.shape\n    if zero_volume is not None:\n        tmp_zero_volume = zero_volume #* 0.0\n        #print('tmp_zero_volume: ', mean)\n        volume = tmp_zero_volume\n    else:\n        volume = img_left.new_zeros([B, max_disp, H, W])\n    for i in range(max_disp):\n        if (i > 0) & (i < W):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "deconv",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def deconv(in_planes, out_planes):\n    return nn.Sequential(\n        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=False),\n        nn.LeakyReLU(0.1,inplace=True)\n    )\ndef convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=dilation if dilation > 1 else pad, dilation = dilation, bias=False),\n                         nn.BatchNorm2d(out_planes))\ndef convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=dilation if dilation > 1 else pad, dilation = dilation, bias=False),\n                         nn.BatchNorm2d(out_planes))\ndef convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),\n                         nn.BatchNorm3d(out_planes))\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),\n                         nn.BatchNorm3d(out_planes))\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=False)\nclass feature_extraction(nn.Module):\n    def __init__(self):\n        super(feature_extraction, self).__init__()\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn(3, 32, 3, 2, 1, 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "init_deconv_bilinear",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def init_deconv_bilinear(weight):\n    f_shape = weight.size()\n    heigh, width = f_shape[-2], f_shape[-1]\n    f = np.ceil(width/2.0)\n    c = (2 * f - 1 - f % 2) / (2.0 * f)\n    bilinear = np.zeros([heigh, width])\n    for x in range(width):\n        for y in range(heigh):\n            value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n            bilinear[x, y] = value",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "save_grad",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def save_grad(grads, name):\n    def hook(grad):\n        grads[name] = grad\n    return hook\ndef disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=True)\ndef channel_normalize(x):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=True)\ndef channel_normalize(x):\n    return x / (torch.norm(x, 2, dim=1, keepdim=True) + 1e-8)\ndef channel_length(x):\n    return torch.sqrt(torch.sum(torch.pow(x, 2), dim=1, keepdim=True) + 1e-8)\ndef warp_right_to_left(x, disp, warp_grid=None):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "channel_normalize",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def channel_normalize(x):\n    return x / (torch.norm(x, 2, dim=1, keepdim=True) + 1e-8)\ndef channel_length(x):\n    return torch.sqrt(torch.sum(torch.pow(x, 2), dim=1, keepdim=True) + 1e-8)\ndef warp_right_to_left(x, disp, warp_grid=None):\n    #print('size: ', x.size())\n    B, C, H, W = x.size()\n    # mesh grid\n    if warp_grid is not None:\n        xx0, yy = warp_grid",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "channel_length",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def channel_length(x):\n    return torch.sqrt(torch.sum(torch.pow(x, 2), dim=1, keepdim=True) + 1e-8)\ndef warp_right_to_left(x, disp, warp_grid=None):\n    #print('size: ', x.size())\n    B, C, H, W = x.size()\n    # mesh grid\n    if warp_grid is not None:\n        xx0, yy = warp_grid\n        xx = xx0 + disp\n        xx = 2.0*xx / max(W-1,1)-1.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "warp_right_to_left",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def warp_right_to_left(x, disp, warp_grid=None):\n    #print('size: ', x.size())\n    B, C, H, W = x.size()\n    # mesh grid\n    if warp_grid is not None:\n        xx0, yy = warp_grid\n        xx = xx0 + disp\n        xx = 2.0*xx / max(W-1,1)-1.0\n    else:\n        #xx = torch.arange(0, W, device=disp.device).float()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.trainer",
        "peekOfCode": "__all__ = {\n    'FADNet': FADNet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.fadnet.trainer",
        "documentation": {}
    },
    {
        "label": "InputPadder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.utils.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.utils.utils",
        "peekOfCode": "class InputPadder:\n    \"\"\" Pads images such that dimensions are divisible by 8 \"\"\"\n    def __init__(self, dims, mode='sintel', divis_by=8, force_square=False):\n        self.ht, self.wd = dims[-2:]\n        if force_square:\n          max_side = max(self.ht, self.wd)\n          pad_ht = ((max_side // divis_by) + 1) * divis_by - self.ht\n          pad_wd = ((max_side // divis_by) + 1) * divis_by - self.wd\n        else:\n          pad_ht = (((self.ht // divis_by) + 1) * divis_by - self.ht) % divis_by",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.utils.utils",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.utils.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.utils.utils",
        "peekOfCode": "def bilinear_sampler(img, coords, mode='bilinear', mask=False, low_memory=False):\n    \"\"\" Wrapper for grid_sample, uses pixel coordinates \"\"\"\n    H, W = img.shape[-2:]\n    xgrid, ygrid = coords.split([1,1], dim=-1)\n    xgrid = 2*xgrid/(W-1) - 1   # Normalize to [-1,1]\n    assert torch.unique(ygrid).numel() == 1 and H == 1 # This is a stereo problem\n    grid = torch.cat([xgrid, ygrid], dim=-1).to(img.dtype)\n    img = F.grid_sample(img, grid, align_corners=True)\n    if mask:\n        mask = (xgrid > -1) & (ygrid > -1) & (xgrid < 1) & (ygrid < 1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.utils.utils",
        "documentation": {}
    },
    {
        "label": "coords_grid",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.utils.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.utils.utils",
        "peekOfCode": "def coords_grid(batch, ht, wd):\n    coords = torch.meshgrid(torch.arange(ht), torch.arange(wd))\n    coords = torch.stack(coords[::-1], dim=0).float()\n    return coords[None].repeat(batch, 1, 1, 1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.utils.utils",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "MultiBasicEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "class MultiBasicEncoder(nn.Module):\n    def __init__(self, output_dim=[128], norm_fn='batch', dropout=0.0, downsample=3):\n        super(MultiBasicEncoder, self).__init__()\n        self.norm_fn = norm_fn\n        self.downsample = downsample\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n        elif self.norm_fn == 'batch':\n            self.norm1 = nn.BatchNorm2d(64)\n        elif self.norm_fn == 'instance':",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "ContextNetDino",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "class ContextNetDino(MultiBasicEncoder):\n    def __init__(self, args, output_dim=[128], norm_fn='batch', downsample=3):\n        nn.Module.__init__(self)\n        self.args = args\n        self.patch_size = 14\n        self.image_size = 518\n        self.vit_feat_dim = 384\n        code_dir = os.path.dirname(os.path.realpath(__file__))\n        self.out_dims = output_dim\n        self.norm_fn = norm_fn",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "DepthAnythingFeature",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "class DepthAnythingFeature(nn.Module):\n    model_configs = {\n        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]}\n    }\n    def __init__(self, encoder='vits'):\n        super().__init__()\n        from depth_anything.dpt import DepthAnything\n        self.encoder = encoder",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "class Feature(nn.Module):\n    def __init__(self, args):\n        super(Feature, self).__init__()\n        self.args = args\n        model = timm.create_model('edgenext_small', pretrained=True, features_only=False)\n        self.stem = model.stem\n        self.stages = model.stages\n        chans = [48, 96, 160, 304]\n        self.chans = chans\n        self.dino = DepthAnythingFeature(encoder=self.args.vit_size)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom core.submodule import *\nfrom Utils import *\nimport timm\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "hourglass",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "peekOfCode": "class hourglass(nn.Module):\n    def __init__(self, cfg, in_channels, feat_dims=None):\n        super().__init__()\n        self.cfg = cfg\n        self.conv1 = nn.Sequential(BasicConv(in_channels, in_channels*2, is_3d=True, bn=True, relu=True, kernel_size=3,\n                                             padding=1, stride=2, dilation=1),\n                                   Conv3dNormActReduced(in_channels*2, in_channels*2, kernel_size=3, kernel_disp=17))\n        self.conv2 = nn.Sequential(BasicConv(in_channels*2, in_channels*4, is_3d=True, bn=True, relu=True, kernel_size=3,\n                                             padding=1, stride=2, dilation=1),\n                                   Conv3dNormActReduced(in_channels*4, in_channels*4, kernel_size=3, kernel_disp=17))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "documentation": {}
    },
    {
        "label": "FoundationStereo",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "peekOfCode": "class FoundationStereo(nn.Module, huggingface_hub.PyTorchModelHubMixin):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        context_dims = args.hidden_dims\n        self.cv_group = 8\n        volume_dim = 28\n        self.max_disp = args.max_disp\n        self.cnet = ContextNetDino(args, output_dim=[args.hidden_dims, context_dims], downsample=args.n_downsample)\n        self.update_block = BasicSelectiveMultiUpdateBlock(self.args, self.args.hidden_dims[0], volume_dim=volume_dim)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "documentation": {}
    },
    {
        "label": "normalize_image",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "peekOfCode": "def normalize_image(img):\n    '''\n    @img: (B,C,H,W) in range 0-255, RGB order\n    '''\n    tf = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n    return tf(img/255.0).contiguous()\nclass hourglass(nn.Module):\n    def __init__(self, cfg, in_channels, feat_dims=None):\n        super().__init__()\n        self.cfg = cfg",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom stereo.modeling.models.foundationstereo.core.update import *\nfrom stereo.modeling.models.foundationstereo.core.extractor import *\nfrom stereo.modeling.models.foundationstereo.core.geometry import Combined_Geo_Encoding_Volume\nfrom stereo.modeling.models.foundationstereo.core.submodule import *\nfrom stereo.modeling.models.foundationstereo.core.utils.utils import *\nfrom stereo.modeling.models.foundationstereo.Utils import *\nimport time,huggingface_hub\ntry:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "documentation": {}
    },
    {
        "label": "Combined_Geo_Encoding_Volume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.geometry",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.geometry",
        "peekOfCode": "class Combined_Geo_Encoding_Volume:\n    def __init__(self, init_fmap1, init_fmap2, geo_volume, num_levels=2, dx=None):\n        self.num_levels = num_levels\n        self.geo_volume_pyramid = []\n        self.init_corr_pyramid = []\n        self.dx = dx\n        # all pairs correlation\n        init_corr = Combined_Geo_Encoding_Volume.corr(init_fmap1, init_fmap2)\n        b, h, w, _, w2 = init_corr.shape\n        b, c, d, h, w = geo_volume.shape",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.geometry",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.geometry",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.geometry",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom Utils import *\nclass Combined_Geo_Encoding_Volume:\n    def __init__(self, init_fmap1, init_fmap2, geo_volume, num_levels=2, dx=None):\n        self.num_levels = num_levels\n        self.geo_volume_pyramid = []\n        self.init_corr_pyramid = []\n        self.dx = dx\n        # all pairs correlation",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.geometry",
        "documentation": {}
    },
    {
        "label": "LayerNorm2d",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class LayerNorm2d(nn.LayerNorm):\n    r\"\"\" https://huggingface.co/spaces/Roll20/pet_score/blob/b258ef28152ab0d5b377d9142a23346f863c1526/lib/timm/models/convnext.py#L85\n    LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).\n    \"\"\"\n    def __init__(self, normalized_shape, eps=1e-6):\n        super().__init__(normalized_shape, eps=eps)\n    def forward(self, x) -> torch.Tensor:\n        \"\"\"\n        @x: (B,C,H,W)\n        \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class BasicConv(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, bn=True, relu=True, norm='batch', **kwargs):\n        super(BasicConv, self).__init__()\n        self.relu = relu\n        self.use_bn = bn\n        self.bn = nn.Identity()\n        if is_3d:\n            if deconv:\n                self.conv = nn.ConvTranspose3d(in_channels, out_channels, bias=False, **kwargs)\n            else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "Conv3dNormActReduced",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class Conv3dNormActReduced(nn.Module):\n    def __init__(self, C_in, C_out, hidden=None, kernel_size=3, kernel_disp=None, stride=1, norm=nn.BatchNorm3d):\n        super().__init__()\n        if kernel_disp is None:\n          kernel_disp = kernel_size\n        if hidden is None:\n            hidden = C_out\n        self.conv1 = nn.Sequential(\n            nn.Conv3d(C_in, hidden, kernel_size=(1,kernel_size,kernel_size), padding=(0, kernel_size//2, kernel_size//2), stride=(1, stride, stride)),\n            norm(hidden),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "ResnetBasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class ResnetBasicBlock(nn.Module):\n  def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=nn.BatchNorm2d, bias=False):\n    super().__init__()\n    self.norm_layer = norm_layer\n    if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n    if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n    # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=kernel_size, stride=stride, bias=bias, padding=padding)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "ResnetBasicBlock3D",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class ResnetBasicBlock3D(nn.Module):\n  def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=nn.BatchNorm3d, bias=False):\n    super().__init__()\n    self.norm_layer = norm_layer\n    if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n    if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n    self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=kernel_size, stride=stride, bias=bias, padding=padding)\n    if self.norm_layer is not None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "FlashMultiheadAttention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class FlashMultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "FlashAttentionTransformerEncoderLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class FlashAttentionTransformerEncoderLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, dim_feedforward, dropout=0.1, act=nn.GELU, norm=nn.LayerNorm):\n        super().__init__()\n        self.self_attn = FlashMultiheadAttention(embed_dim, num_heads)\n        self.act = act()\n        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n        self.norm1 = norm(embed_dim)\n        self.norm2 = norm(embed_dim)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "UpsampleConv",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class UpsampleConv(nn.Module):\n    def __init__(self, C_in, C_out, is_3d=False, kernel_size=3, bias=True, stride=1, padding=1):\n        super().__init__()\n        self.is_3d = is_3d\n        if is_3d:\n          self.conv = nn.Conv3d(C_in, C_out, kernel_size=kernel_size, stride=1, padding=kernel_size//2, bias=bias)\n        else:\n          self.conv = nn.Conv2d(C_in, C_out, kernel_size=kernel_size, stride=1, padding=kernel_size//2, bias=bias)\n    def forward(self, x):\n        if self.is_3d:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "Conv2x",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class Conv2x(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, concat=True, keep_concat=True, bn=True, relu=True, keep_dispc=False):\n        super(Conv2x, self).__init__()\n        self.concat = concat\n        self.is_3d = is_3d\n        if deconv and is_3d:\n            kernel = (4, 4, 4)\n        elif deconv:\n            kernel = 4\n        else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv_IN",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class BasicConv_IN(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, IN=True, relu=True, **kwargs):\n        super(BasicConv_IN, self).__init__()\n        self.relu = relu\n        self.use_in = IN\n        if is_3d:\n            if deconv:\n                self.conv = nn.ConvTranspose3d(in_channels, out_channels, bias=False, **kwargs)\n            else:\n                self.conv = nn.Conv3d(in_channels, out_channels, bias=False, **kwargs)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "Conv2x_IN",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class Conv2x_IN(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, concat=True, keep_concat=True, IN=True, relu=True, keep_dispc=False):\n        super(Conv2x_IN, self).__init__()\n        self.concat = concat\n        self.is_3d = is_3d\n        if deconv and is_3d:\n            kernel = (4, 4, 4)\n        elif deconv:\n            kernel = 4\n        else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "FeatureAtt",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class FeatureAtt(nn.Module):\n    def __init__(self, cv_chan, feat_chan):\n        super(FeatureAtt, self).__init__()\n        self.feat_att = nn.Sequential(\n            BasicConv(feat_chan, feat_chan//2, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(feat_chan//2, cv_chan, 1)\n            )\n    def forward(self, cv, feat):\n        '''\n        @cv: cost volume (B,C,D,H,W)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "PositionalEmbedding",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class PositionalEmbedding(nn.Module):\n  def __init__(self, d_model, max_len=512):\n    super().__init__()\n    # Compute the positional encodings once in log space.\n    pe = torch.zeros(max_len, d_model).float()\n    pe.require_grad = False\n    position = torch.arange(0, max_len).float().unsqueeze(1)  #(N,1)\n    div_term = (torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model)).exp()[None]\n    pe[:, 0::2] = torch.sin(position * div_term)  #(N, d_model/2)\n    pe[:, 1::2] = torch.cos(position * div_term)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "CostVolumeDisparityAttention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class CostVolumeDisparityAttention(nn.Module):\n  def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1, act=nn.GELU, norm_first=False, num_transformer=6, max_len=512, resize_embed=False):\n    super().__init__()\n    self.resize_embed = resize_embed\n    self.sa = nn.ModuleList([])\n    for _ in range(num_transformer):\n      self.sa.append(FlashAttentionTransformerEncoderLayer(embed_dim=d_model, num_heads=nhead, dim_feedforward=dim_feedforward, act=act, dropout=dropout))\n    self.pos_embed0 = PositionalEmbedding(d_model, max_len=max_len)\n  def forward(self, cv, window_size=(-1,-1)):\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "ChannelAttentionEnhancement",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class ChannelAttentionEnhancement(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttentionEnhancement, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "SpatialAttentionExtractor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class SpatialAttentionExtractor(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttentionExtractor, self).__init__()\n        self.samconv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.samconv(x)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "EdgeNextConvEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class EdgeNextConvEncoder(nn.Module):\n    def __init__(self, dim, layer_scale_init_value=1e-6, expan_ratio=4, kernel_size=7, norm='layer'):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=kernel_size, padding=kernel_size // 2, groups=dim)\n        if norm=='layer':\n          self.norm = LayerNorm2d(dim, eps=1e-6)\n        else:\n          self.norm = nn.Identity()\n        self.pwconv1 = nn.Linear(dim, expan_ratio * dim)\n        self.act = nn.GELU()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0, f\"C:{C}, num_groups:{num_groups}\"\n    channels_per_group = C // num_groups\n    fea1 = fea1.reshape(B, num_groups, channels_per_group, H, W)\n    fea2 = fea2.reshape(B, num_groups, channels_per_group, H, W)\n    with torch.cuda.amp.autocast(enabled=False):\n      cost = (F.normalize(fea1.float(), dim=2) * F.normalize(fea2.float(), dim=2)).sum(dim=2)  #!NOTE Divide first for numerical stability\n    assert cost.shape == (B, num_groups, H, W)\n    return cost",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups, stride=1):\n    \"\"\"\n    @refimg_fea: left image feature\n    @targetimg_fea: right image feature\n    \"\"\"\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i], num_groups)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, :] = refimg_fea[:, :, :, :]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.reshape(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=True)\nclass FeatureAtt(nn.Module):\n    def __init__(self, cv_chan, feat_chan):\n        super(FeatureAtt, self).__init__()\n        self.feat_att = nn.Sequential(\n            BasicConv(feat_chan, feat_chan//2, kernel_size=1, stride=1, padding=0),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "context_upsample",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "def context_upsample(disp_low, up_weights):\n    \"\"\"\n    @disp_low: (b,1,h,w)  1/4 resolution\n    @up_weights: (b,9,4*h,4*w)  Image resolution\n    \"\"\"\n    b, c, h, w = disp_low.shape\n    disp_unfold = F.unfold(disp_low.reshape(b,c,h,w),3,1,1).reshape(b,-1,h,w)\n    disp_unfold = F.interpolate(disp_unfold,(h*4,w*4),mode='nearest').reshape(b,9,h*4,w*4)\n    disp = (disp_unfold*up_weights).sum(1)\n    return disp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom Utils import *\nfrom flash_attn import flash_attn_qkvpacked_func, flash_attn_func\ndef _is_contiguous(tensor: torch.Tensor) -> bool:\n    if torch.jit.is_scripting():\n        return tensor.is_contiguous()\n    else:\n        return tensor.is_contiguous(memory_format=torch.contiguous_format)\nclass LayerNorm2d(nn.LayerNorm):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "DispHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):\n        super(DispHead, self).__init__()\n        self.conv = nn.Sequential(\n          nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1),\n          nn.ReLU(),\n          EdgeNextConvEncoder(input_dim, expan_ratio=4, kernel_size=7, norm=None),\n          EdgeNextConvEncoder(input_dim, expan_ratio=4, kernel_size=7, norm=None),\n          nn.Conv2d(input_dim, output_dim, 3, padding=1),\n        )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "ConvGRU",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class ConvGRU(nn.Module):\n    def __init__(self, hidden_dim, input_dim, kernel_size=3):\n        super(ConvGRU, self).__init__()\n        self.convz = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size//2)\n        self.convr = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size//2)\n        self.convq = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size//2)\n    def forward(self, h, cz, cr, cq, *x_list):\n        x = torch.cat(x_list, dim=1)\n        hx = torch.cat([h, x], dim=1)\n        z = torch.sigmoid(self.convz(hx) + cz)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "BasicMotionEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class BasicMotionEncoder(nn.Module):\n    def __init__(self, args, ngroup=8):\n        super(BasicMotionEncoder, self).__init__()\n        self.args = args\n        cor_planes = args.corr_levels * (2*args.corr_radius + 1) * (ngroup+1)\n        self.convc1 = nn.Conv2d(cor_planes, 256, 1, padding=0)\n        self.convc2 = nn.Conv2d(256, 256, 3, padding=1)\n        self.convd1 = nn.Conv2d(1, 64, 7, padding=3)\n        self.convd2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.conv = nn.Conv2d(64+256, 128-1, 3, padding=1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "RaftConvGRU",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class RaftConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=256, kernel_size=3):\n        super().__init__()\n        self.convz = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convq = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n    def forward(self, h, x, hx):\n        z = torch.sigmoid(self.convz(hx))\n        r = torch.sigmoid(self.convr(hx))\n        q = torch.tanh(self.convq(torch.cat([r*h, x], dim=1)))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "SelectiveConvGRU",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class SelectiveConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=256, small_kernel_size=1, large_kernel_size=3, patch_size=None):\n        super(SelectiveConvGRU, self).__init__()\n        self.conv0 = nn.Sequential(\n            nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1),\n            nn.ReLU(),\n        )\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(input_dim+hidden_dim, input_dim+hidden_dim, kernel_size=3, padding=1),\n            nn.ReLU(),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "BasicSelectiveMultiUpdateBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class BasicSelectiveMultiUpdateBlock(nn.Module):\n    def __init__(self, args, hidden_dim=128, volume_dim=8):\n        super().__init__()\n        self.args = args\n        self.encoder = BasicMotionEncoder(args, volume_dim)\n        if args.n_gru_layers == 3:\n            self.gru16 = SelectiveConvGRU(hidden_dim, hidden_dim * 2)\n        if args.n_gru_layers >= 2:\n            self.gru08 = SelectiveConvGRU(hidden_dim, hidden_dim * (args.n_gru_layers == 3) + hidden_dim * 2)\n        self.gru04 = SelectiveConvGRU(hidden_dim, hidden_dim * (args.n_gru_layers > 1) + hidden_dim * 2)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "pool2x",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "def pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)\ndef pool4x(x):\n    return F.avg_pool2d(x, 5, stride=4, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass RaftConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=256, kernel_size=3):\n        super().__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "pool4x",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "def pool4x(x):\n    return F.avg_pool2d(x, 5, stride=4, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass RaftConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=256, kernel_size=3):\n        super().__init__()\n        self.convz = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "interp",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "def interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass RaftConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=256, kernel_size=3):\n        super().__init__()\n        self.convz = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convq = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n    def forward(self, h, x, hx):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom core.submodule import *\nfrom core.extractor import *\nclass DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):\n        super(DispHead, self).__init__()\n        self.conv = nn.Sequential(\n          nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1),\n          nn.ReLU(),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "Resize",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "peekOfCode": "class Resize(object):\n    \"\"\"Resize sample to given size (width, height).\n    \"\"\"\n    def __init__(\n        self,\n        width,\n        height,\n        resize_target=True,\n        keep_aspect_ratio=False,\n        ensure_multiple_of=1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "documentation": {}
    },
    {
        "label": "NormalizeImage",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "peekOfCode": "class NormalizeImage(object):\n    \"\"\"Normlize image by given mean and std.\n    \"\"\"\n    def __init__(self, mean, std):\n        self.__mean = mean\n        self.__std = std\n    def __call__(self, sample):\n        sample[\"image\"] = (sample[\"image\"] - self.__mean) / self.__std\n        return sample\nclass PrepareForNet(object):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "documentation": {}
    },
    {
        "label": "PrepareForNet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "peekOfCode": "class PrepareForNet(object):\n    \"\"\"Prepare sample for usage as network input.\n    \"\"\"\n    def __init__(self):\n        pass\n    def __call__(self, sample):\n        image = np.transpose(sample[\"image\"], (2, 0, 1))\n        sample[\"image\"] = np.ascontiguousarray(image).astype(np.float32)\n        if \"mask\" in sample:\n            sample[\"mask\"] = sample[\"mask\"].astype(np.float32)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "documentation": {}
    },
    {
        "label": "apply_min_size",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "peekOfCode": "def apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):\n    \"\"\"Rezise the sample to ensure the given size. Keeps aspect ratio.\n    Args:\n        sample (dict): sample\n        size (tuple): image size\n    Returns:\n        tuple: new size\n    \"\"\"\n    shape = list(sample[\"disparity\"].shape)\n    if shape[0] >= size[0] and shape[1] >= size[1]:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "documentation": {}
    },
    {
        "label": "ResidualConvUnit",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "peekOfCode": "class ResidualConvUnit(nn.Module):\n    \"\"\"Residual convolution module.\n    \"\"\"\n    def __init__(self, features, activation, bn):\n        \"\"\"Init.\n        Args:\n            features (int): number of features\n        \"\"\"\n        super().__init__()\n        self.bn = bn",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "peekOfCode": "class FeatureFusionBlock(nn.Module):\n    \"\"\"Feature fusion block.\n    \"\"\"\n    def __init__(self, features, activation, deconv=False, bn=False, expand=False, align_corners=True, size=None):\n        \"\"\"Init.\n        Args:\n            features (int): number of features\n        \"\"\"\n        super(FeatureFusionBlock, self).__init__()\n        self.deconv = deconv",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "peekOfCode": "class DPTHead(nn.Module):\n    def __init__(self, nclass, in_channels, features=256, use_bn=False, out_channels=[256, 512, 1024, 1024], use_clstoken=False):\n        super(DPTHead, self).__init__()\n        self.nclass = nclass\n        self.use_clstoken = use_clstoken\n        self.projects = nn.ModuleList([\n            nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=out_channel,\n                kernel_size=1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "documentation": {}
    },
    {
        "label": "DPT_DINOv2",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "peekOfCode": "class DPT_DINOv2(nn.Module):\n    def __init__(self, encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024], use_bn=False, use_clstoken=False, pretrained_dino=False):\n        super(DPT_DINOv2, self).__init__()\n        assert encoder in ['vits', 'vitb', 'vitl']\n        # in case the Internet connection is not stable, please load the DINOv2 locally\n        # if localhub:\n        #     self.pretrained = torch.hub.load('torchhub/facebookresearch_dinov2_main', 'dinov2_{:}14'.format(encoder), source='local', pretrained=False)\n        # else:\n        # self.pretrained = torch.hub.load('facebookresearch/dinov2', 'dinov2_{:}14'.format(encoder), pretrained=pretrained_dino, skip_validation=True)\n        self.pretrained = torch.hub.load('/file_system/vepfs/algorithm/chenming.zhang/.cache/torch/hub/facebookresearch_dinov2_main', 'dinov2_{:}14'.format(encoder),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnything",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "peekOfCode": "class DepthAnything(DPT_DINOv2):\n    def __init__(self, config):\n        super().__init__(**config)\n    def forward(self, x):\n        h, w = x.shape[-2:]\n        features = self.pretrained.get_intermediate_layers(x, 4, return_class_token=True)\n        patch_size = self.pretrained.patch_size\n        patch_h, patch_w = h // patch_size, w // patch_size\n        depth = self.depth_head(features, patch_h, patch_w, patch_size=patch_size)\n        depth = F.interpolate(depth, size=(h, w), mode=\"bilinear\", align_corners=True)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom dinov2.models.vision_transformer import vit_small,vit_base,vit_large\nfrom depth_anything.blocks import FeatureFusionBlock, _make_scratch\ndef _make_fusion_block(features, use_bn, size = None):\n    return FeatureFusionBlock(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "documentation": {}
    },
    {
        "label": "DinoVisionTransformer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.backbones.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.backbones.vision_transformer",
        "peekOfCode": "class DinoVisionTransformer(BaseModule):\n    \"\"\"Vision Transformer.\"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.backbones.vision_transformer",
        "documentation": {}
    },
    {
        "label": "DepthBaseDecodeHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.decode_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.decode_head",
        "peekOfCode": "class DepthBaseDecodeHead(BaseModule, metaclass=ABCMeta):\n    \"\"\"Base class for BaseDecodeHead.\n    Args:\n        in_channels (List): Input channels.\n        channels (int): Channels after modules, before conv_depth.\n        conv_cfg (dict|None): Config of conv layers. Default: None.\n        act_cfg (dict): Config of activation layers.\n            Default: dict(type='ReLU')\n        loss_decode (dict): Config of decode loss.\n            Default: dict(type='SigLoss').",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.decode_head",
        "documentation": {}
    },
    {
        "label": "Interpolate",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class Interpolate(nn.Module):\n    def __init__(self, scale_factor, mode, align_corners=False):\n        super(Interpolate, self).__init__()\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.align_corners = align_corners\n    def forward(self, x):\n        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n        return x",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "HeadDepth",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class HeadDepth(nn.Module):\n    def __init__(self, features):\n        super(HeadDepth, self).__init__()\n        self.head = nn.Sequential(\n            nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(features // 2, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n        )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "ReassembleBlocks",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class ReassembleBlocks(BaseModule):\n    \"\"\"ViTPostProcessBlock, process cls_token in ViT backbone output and\n    rearrange the feature vector to feature map.\n    Args:\n        in_channels (int): ViT feature channels. Default: 768.\n        out_channels (List): output channels of each stage.\n            Default: [96, 192, 384, 768].\n        readout_type (str): Type of readout operation. Default: 'ignore'.\n        patch_size (int): The patch size. Default: 16.\n        init_cfg (dict, optional): Initialization config dict. Default: None.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "PreActResidualConvUnit",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class PreActResidualConvUnit(BaseModule):\n    \"\"\"ResidualConvUnit, pre-activate residual unit.\n    Args:\n        in_channels (int): number of channels in the input feature map.\n        act_cfg (dict): dictionary to construct and config activation layer.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        stride (int): stride of the first block. Default: 1\n        dilation (int): dilation rate for convs layers. Default: 1.\n        init_cfg (dict, optional): Initialization config dict. Default: None.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class FeatureFusionBlock(BaseModule):\n    \"\"\"FeatureFusionBlock, merge feature map from different stages.\n    Args:\n        in_channels (int): Input channels.\n        act_cfg (dict): The activation config for ResidualConvUnit.\n        norm_cfg (dict): Config dict for normalization layer.\n        expand (bool): Whether expand the channels in post process block.\n            Default: False.\n        align_corners (bool): align_corner setting for bilinear upsample.\n            Default: True.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class DPTHead(DepthBaseDecodeHead):\n    \"\"\"Vision Transformers for Dense Prediction.\n    This head is implemented of `DPT <https://arxiv.org/abs/2103.13413>`_.\n    Args:\n        embed_dims (int): The embed dimension of the ViT backbone.\n            Default: 768.\n        post_process_channels (List): Out channels of post process conv\n            layers. Default: [96, 192, 384, 768].\n        readout_type (str): Type of readout operation. Default: 'ignore'.\n        patch_size (int): The patch size. Default: 16.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "BNHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.linear_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.linear_head",
        "peekOfCode": "class BNHead(DepthBaseDecodeHead):\n    \"\"\"Just a batchnorm.\"\"\"\n    def __init__(self, input_transform=\"resize_concat\", in_index=(0, 1, 2, 3), upsample=1, **kwargs):\n        super().__init__(**kwargs)\n        self.input_transform = input_transform\n        self.in_index = in_index\n        self.upsample = upsample\n        # self.bn = nn.SyncBatchNorm(self.in_channels)\n        if self.classify:\n            self.conv_depth = nn.Conv2d(self.channels, self.n_bins, kernel_size=1, padding=0, stride=1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.linear_head",
        "documentation": {}
    },
    {
        "label": "BaseDepther",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.base",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.base",
        "peekOfCode": "class BaseDepther(BaseModule, metaclass=ABCMeta):\n    \"\"\"Base class for depther.\"\"\"\n    def __init__(self, init_cfg=None):\n        super(BaseDepther, self).__init__(init_cfg)\n        self.fp16_enabled = False\n    @property\n    def with_neck(self):\n        \"\"\"bool: whether the depther has neck\"\"\"\n        return hasattr(self, \"neck\") and self.neck is not None\n    @property",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.base",
        "documentation": {}
    },
    {
        "label": "DepthEncoderDecoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "peekOfCode": "class DepthEncoderDecoder(BaseDepther):\n    \"\"\"Encoder Decoder depther.\n    EncoderDecoder typically consists of backbone, (neck) and decode_head.\n    \"\"\"\n    def __init__(self, backbone, decode_head, neck=None, train_cfg=None, test_cfg=None, pretrained=None, init_cfg=None):\n        super(DepthEncoderDecoder, self).__init__(init_cfg)\n        if pretrained is not None:\n            assert backbone.get(\"pretrained\") is None, \"both backbone and depther set pretrained weight\"\n            backbone.pretrained = pretrained\n        self.backbone = builder.build_backbone(backbone)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "documentation": {}
    },
    {
        "label": "add_prefix",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "peekOfCode": "def add_prefix(inputs, prefix):\n    \"\"\"Add prefix for dict.\n    Args:\n        inputs (dict): The input dict with str keys.\n        prefix (str): The prefix to add.\n    Returns:\n        dict: The dict with keys updated with ``prefix``.\n    \"\"\"\n    outputs = dict()\n    for name, value in inputs.items():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "documentation": {}
    },
    {
        "label": "GradientLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.gradientloss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.gradientloss",
        "peekOfCode": "class GradientLoss(nn.Module):\n    \"\"\"GradientLoss.\n    Adapted from https://www.cs.cornell.edu/projects/megadepth/\n    Args:\n        valid_mask (bool): Whether filter invalid gt (gt > 0). Default: True.\n        loss_weight (float): Weight of the loss. Default: 1.0.\n        max_depth (int): When filtering invalid gt, set a max threshold. Default: None.\n    \"\"\"\n    def __init__(self, valid_mask=True, loss_weight=1.0, max_depth=None, loss_name=\"loss_grad\"):\n        super(GradientLoss, self).__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.gradientloss",
        "documentation": {}
    },
    {
        "label": "SigLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.sigloss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.sigloss",
        "peekOfCode": "class SigLoss(nn.Module):\n    \"\"\"SigLoss.\n        This follows `AdaBins <https://arxiv.org/abs/2011.14141>`_.\n    Args:\n        valid_mask (bool): Whether filter invalid gt (gt > 0). Default: True.\n        loss_weight (float): Weight of the loss. Default: 1.0.\n        max_depth (int): When filtering invalid gt, set a max threshold. Default: None.\n        warm_up (bool): A simple warm up stage to help convergence. Default: False.\n        warm_iter (int): The number of warm up stage. Default: 100.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.sigloss",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "def build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return HEADS.build(cfg)\ndef build_loss(cfg):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "build_neck",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "def build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return HEADS.build(cfg)\ndef build_loss(cfg):\n    \"\"\"Build loss.\"\"\"\n    return LOSSES.build(cfg)\ndef build_depther(cfg, train_cfg=None, test_cfg=None):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "build_head",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "def build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return HEADS.build(cfg)\ndef build_loss(cfg):\n    \"\"\"Build loss.\"\"\"\n    return LOSSES.build(cfg)\ndef build_depther(cfg, train_cfg=None, test_cfg=None):\n    \"\"\"Build depther.\"\"\"\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\"train_cfg and test_cfg is deprecated, \" \"please specify them in model\", UserWarning)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "build_loss",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "def build_loss(cfg):\n    \"\"\"Build loss.\"\"\"\n    return LOSSES.build(cfg)\ndef build_depther(cfg, train_cfg=None, test_cfg=None):\n    \"\"\"Build depther.\"\"\"\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\"train_cfg and test_cfg is deprecated, \" \"please specify them in model\", UserWarning)\n    assert cfg.get(\"train_cfg\") is None or train_cfg is None, \"train_cfg specified in both outer field and model field \"\n    assert cfg.get(\"test_cfg\") is None or test_cfg is None, \"test_cfg specified in both outer field and model field \"\n    return DEPTHER.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "build_depther",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "def build_depther(cfg, train_cfg=None, test_cfg=None):\n    \"\"\"Build depther.\"\"\"\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\"train_cfg and test_cfg is deprecated, \" \"please specify them in model\", UserWarning)\n    assert cfg.get(\"train_cfg\") is None or train_cfg is None, \"train_cfg specified in both outer field and model field \"\n    assert cfg.get(\"test_cfg\") is None or test_cfg is None, \"test_cfg specified in both outer field and model field \"\n    return DEPTHER.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "MODELS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "MODELS = Registry(\"models\", parent=MMCV_MODELS)\nATTENTION = Registry(\"attention\", parent=MMCV_ATTENTION)\nBACKBONES = MODELS\nNECKS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "ATTENTION",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "ATTENTION = Registry(\"attention\", parent=MMCV_ATTENTION)\nBACKBONES = MODELS\nNECKS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "BACKBONES",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "BACKBONES = MODELS\nNECKS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "NECKS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "NECKS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "HEADS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "HEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\ndef build_head(cfg):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "LOSSES",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "LOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "DEPTHER",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "DEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return HEADS.build(cfg)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.ops.wrappers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.ops.wrappers",
        "peekOfCode": "def resize(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None, warning=False):\n    if warning:\n        if size is not None and align_corners:\n            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n            output_h, output_w = tuple(int(x) for x in size)\n            if output_h > input_h or output_w > output_h:\n                if (\n                    (output_h > 1 and output_w > 1 and input_h > 1 and input_w > 1)\n                    and (output_h - 1) % (input_h - 1)\n                    and (output_w - 1) % (input_w - 1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.depth.ops.wrappers",
        "documentation": {}
    },
    {
        "label": "DistOptimizerHook",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.hooks.optimizer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.hooks.optimizer",
        "peekOfCode": "class DistOptimizerHook(OptimizerHook):\n    \"\"\"Optimizer hook for distributed training.\"\"\"\n    def __init__(self, update_interval=1, grad_clip=None, coalesce=True, bucket_size_mb=-1, use_fp16=False):\n        self.grad_clip = grad_clip\n        self.coalesce = coalesce\n        self.bucket_size_mb = bucket_size_mb\n        self.update_interval = update_interval\n        self.use_fp16 = use_fp16\n    def before_run(self, runner):\n        runner.optimizer.zero_grad()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.hooks.optimizer",
        "documentation": {}
    },
    {
        "label": "DinoVisionTransformer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.backbones.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.backbones.vision_transformer",
        "peekOfCode": "class DinoVisionTransformer(BaseModule):\n    \"\"\"Vision Transformer.\"\"\"\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.backbones.vision_transformer",
        "documentation": {}
    },
    {
        "label": "BNHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.decode_heads.linear_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.decode_heads.linear_head",
        "peekOfCode": "class BNHead(BaseDecodeHead):\n    \"\"\"Just a batchnorm.\"\"\"\n    def __init__(self, resize_factors=None, **kwargs):\n        super().__init__(**kwargs)\n        assert self.in_channels == self.channels\n        self.bn = nn.SyncBatchNorm(self.in_channels)\n        self.resize_factors = resize_factors\n    def _forward_feature(self, inputs):\n        \"\"\"Forward function for feature maps before classifying each pixel with\n        ``self.cls_seg`` fc.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.decode_heads.linear_head",
        "documentation": {}
    },
    {
        "label": "ADE20K_COLORMAP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "peekOfCode": "ADE20K_COLORMAP = [\n    (0, 0, 0),\n    (120, 120, 120),\n    (180, 120, 120),\n    (6, 230, 230),\n    (80, 50, 50),\n    (4, 200, 3),\n    (120, 120, 80),\n    (140, 140, 140),\n    (204, 5, 255),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "documentation": {}
    },
    {
        "label": "ADE20K_CLASS_NAMES",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "peekOfCode": "ADE20K_CLASS_NAMES = [\n    \"\",\n    \"wall\",\n    \"building;edifice\",\n    \"sky\",\n    \"floor;flooring\",\n    \"tree\",\n    \"ceiling\",\n    \"road;route\",\n    \"bed\",",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "documentation": {}
    },
    {
        "label": "VOC2012_COLORMAP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "peekOfCode": "VOC2012_COLORMAP = [\n    (0, 0, 0),\n    (128, 0, 0),\n    (0, 128, 0),\n    (128, 128, 0),\n    (0, 0, 128),\n    (128, 0, 128),\n    (0, 128, 128),\n    (128, 128, 128),\n    (64, 0, 0),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "documentation": {}
    },
    {
        "label": "VOC2012_CLASS_NAMES",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "peekOfCode": "VOC2012_CLASS_NAMES = [\n    \"\",\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "documentation": {}
    },
    {
        "label": "build_prior_generator",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "peekOfCode": "def build_prior_generator(cfg, default_args=None):\n    return build_from_cfg(cfg, PRIOR_GENERATORS, default_args)\ndef build_anchor_generator(cfg, default_args=None):\n    warnings.warn(\"``build_anchor_generator`` would be deprecated soon, please use \" \"``build_prior_generator`` \")\n    return build_prior_generator(cfg, default_args=default_args)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "documentation": {}
    },
    {
        "label": "build_anchor_generator",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "peekOfCode": "def build_anchor_generator(cfg, default_args=None):\n    warnings.warn(\"``build_anchor_generator`` would be deprecated soon, please use \" \"``build_prior_generator`` \")\n    return build_prior_generator(cfg, default_args=default_args)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "documentation": {}
    },
    {
        "label": "PRIOR_GENERATORS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "peekOfCode": "PRIOR_GENERATORS = Registry(\"Generator for anchors and points\")\nANCHOR_GENERATORS = PRIOR_GENERATORS\ndef build_prior_generator(cfg, default_args=None):\n    return build_from_cfg(cfg, PRIOR_GENERATORS, default_args)\ndef build_anchor_generator(cfg, default_args=None):\n    warnings.warn(\"``build_anchor_generator`` would be deprecated soon, please use \" \"``build_prior_generator`` \")\n    return build_prior_generator(cfg, default_args=default_args)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "documentation": {}
    },
    {
        "label": "ANCHOR_GENERATORS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "peekOfCode": "ANCHOR_GENERATORS = PRIOR_GENERATORS\ndef build_prior_generator(cfg, default_args=None):\n    return build_from_cfg(cfg, PRIOR_GENERATORS, default_args)\ndef build_anchor_generator(cfg, default_args=None):\n    warnings.warn(\"``build_anchor_generator`` would be deprecated soon, please use \" \"``build_prior_generator`` \")\n    return build_prior_generator(cfg, default_args=default_args)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "documentation": {}
    },
    {
        "label": "MlvlPointGenerator",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.point_generator",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.point_generator",
        "peekOfCode": "class MlvlPointGenerator:\n    \"\"\"Standard points generator for multi-level (Mlvl) feature maps in 2D\n    points-based detectors.\n    Args:\n        strides (list[int] | list[tuple[int, int]]): Strides of anchors\n            in multiple feature levels in order (w, h).\n        offset (float): The offset of points, the value is normalized with\n            corresponding stride. Defaults to 0.5.\n    \"\"\"\n    def __init__(self, strides, offset=0.5):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.point_generator",
        "documentation": {}
    },
    {
        "label": "BaseSampler",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.base_sampler",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.base_sampler",
        "peekOfCode": "class BaseSampler(metaclass=ABCMeta):\n    \"\"\"Base class of samplers.\"\"\"\n    def __init__(self, num, pos_fraction, neg_pos_ub=-1, add_gt_as_proposals=True, **kwargs):\n        self.num = num\n        self.pos_fraction = pos_fraction\n        self.neg_pos_ub = neg_pos_ub\n        self.add_gt_as_proposals = add_gt_as_proposals\n        self.pos_sampler = self\n        self.neg_sampler = self\n    @abstractmethod",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.base_sampler",
        "documentation": {}
    },
    {
        "label": "MaskPseudoSampler",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_pseudo_sampler",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_pseudo_sampler",
        "peekOfCode": "class MaskPseudoSampler(BaseSampler):\n    \"\"\"A pseudo sampler that does not do sampling actually.\"\"\"\n    def __init__(self, **kwargs):\n        pass\n    def _sample_pos(self, **kwargs):\n        \"\"\"Sample positive samples.\"\"\"\n        raise NotImplementedError\n    def _sample_neg(self, **kwargs):\n        \"\"\"Sample negative samples.\"\"\"\n        raise NotImplementedError",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_pseudo_sampler",
        "documentation": {}
    },
    {
        "label": "MaskSamplingResult",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_sampling_result",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_sampling_result",
        "peekOfCode": "class MaskSamplingResult(SamplingResult):\n    \"\"\"Mask sampling result.\"\"\"\n    def __init__(self, pos_inds, neg_inds, masks, gt_masks, assign_result, gt_flags):\n        self.pos_inds = pos_inds\n        self.neg_inds = neg_inds\n        self.pos_masks = masks[pos_inds]\n        self.neg_masks = masks[neg_inds]\n        self.pos_is_gt = gt_flags[pos_inds]\n        self.num_gts = gt_masks.shape[0]\n        self.pos_assigned_gt_inds = assign_result.gt_inds[pos_inds] - 1",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_sampling_result",
        "documentation": {}
    },
    {
        "label": "SamplingResult",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.sampling_result",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.sampling_result",
        "peekOfCode": "class SamplingResult:\n    \"\"\"Bbox sampling result.\n    Example:\n        >>> # xdoctest: +IGNORE_WANT\n        >>> from mmdet.core.bbox.samplers.sampling_result import *  # NOQA\n        >>> self = SamplingResult.random(rng=10)\n        >>> print(f'self = {self}')\n        self = <SamplingResult({\n            'neg_bboxes': torch.Size([12, 4]),\n            'neg_inds': tensor([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12]),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.sampling_result",
        "documentation": {}
    },
    {
        "label": "build_sampler",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "peekOfCode": "def build_sampler(cfg, **default_args):\n    \"\"\"Builder of box sampler.\"\"\"\n    return build_from_cfg(cfg, BBOX_SAMPLERS, default_args)\ndef build_bbox_coder(cfg, **default_args):\n    \"\"\"Builder of box coder.\"\"\"\n    return build_from_cfg(cfg, BBOX_CODERS, default_args)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "documentation": {}
    },
    {
        "label": "build_bbox_coder",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "peekOfCode": "def build_bbox_coder(cfg, **default_args):\n    \"\"\"Builder of box coder.\"\"\"\n    return build_from_cfg(cfg, BBOX_CODERS, default_args)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "documentation": {}
    },
    {
        "label": "BBOX_SAMPLERS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "peekOfCode": "BBOX_SAMPLERS = Registry(\"bbox_sampler\")\nBBOX_CODERS = Registry(\"bbox_coder\")\ndef build_sampler(cfg, **default_args):\n    \"\"\"Builder of box sampler.\"\"\"\n    return build_from_cfg(cfg, BBOX_SAMPLERS, default_args)\ndef build_bbox_coder(cfg, **default_args):\n    \"\"\"Builder of box coder.\"\"\"\n    return build_from_cfg(cfg, BBOX_CODERS, default_args)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "documentation": {}
    },
    {
        "label": "BBOX_CODERS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "peekOfCode": "BBOX_CODERS = Registry(\"bbox_coder\")\ndef build_sampler(cfg, **default_args):\n    \"\"\"Builder of box sampler.\"\"\"\n    return build_from_cfg(cfg, BBOX_SAMPLERS, default_args)\ndef build_bbox_coder(cfg, **default_args):\n    \"\"\"Builder of box coder.\"\"\"\n    return build_from_cfg(cfg, BBOX_CODERS, default_args)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "documentation": {}
    },
    {
        "label": "reduce_mean",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.dist_utils",
        "peekOfCode": "def reduce_mean(tensor):\n    \"\"\" \"Obtain the mean of tensor on different GPUs.\"\"\"\n    if not (dist.is_available() and dist.is_initialized()):\n        return tensor\n    tensor = tensor.clone()\n    dist.all_reduce(tensor.div_(dist.get_world_size()), op=dist.ReduceOp.SUM)\n    return tensor",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "multi_apply",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "peekOfCode": "def multi_apply(func, *args, **kwargs):\n    \"\"\"Apply function to a list of arguments.\n    Note:\n        This function applies the ``func`` to multiple inputs and\n        map the multiple outputs of the ``func`` into different\n        list. Each list contains the same type of outputs corresponding\n        to different inputs.\n    Args:\n        func (Function): A function that will be applied to a list of\n            arguments",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "documentation": {}
    },
    {
        "label": "add_prefix",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "peekOfCode": "def add_prefix(inputs, prefix):\n    \"\"\"Add prefix for dict.\n    Args:\n        inputs (dict): The input dict with str keys.\n        prefix (str): The prefix to add.\n    Returns:\n        dict: The dict with keys updated with ``prefix``.\n    \"\"\"\n    outputs = dict()\n    for name, value in inputs.items():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "documentation": {}
    },
    {
        "label": "ConvFFN",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class ConvFFN(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "DWConv",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class DWConv(nn.Module):\n    def __init__(self, dim=768):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        n = N // 21\n        x1 = x[:, 0 : 16 * n, :].transpose(1, 2).view(B, C, H * 2, W * 2).contiguous()\n        x2 = x[:, 16 * n : 20 * n, :].transpose(1, 2).view(B, C, H, W).contiguous()\n        x3 = x[:, 20 * n :, :].transpose(1, 2).view(B, C, H // 2, W // 2).contiguous()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "Extractor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class Extractor(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=6,\n        n_points=4,\n        n_levels=1,\n        deform_ratio=1.0,\n        with_cffn=True,\n        cffn_ratio=0.25,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "Injector",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class Injector(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=6,\n        n_points=4,\n        n_levels=1,\n        deform_ratio=1.0,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=0.0,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "InteractionBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class InteractionBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=6,\n        n_points=4,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        drop=0.0,\n        drop_path=0.0,\n        with_cffn=True,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "InteractionBlockWithCls",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class InteractionBlockWithCls(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=6,\n        n_points=4,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        drop=0.0,\n        drop_path=0.0,\n        with_cffn=True,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "SpatialPriorModule",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class SpatialPriorModule(nn.Module):\n    def __init__(self, inplanes=64, embed_dim=384, with_cp=False):\n        super().__init__()\n        self.with_cp = with_cp\n        self.stem = nn.Sequential(\n            *[\n                nn.Conv2d(3, inplanes, kernel_size=3, stride=2, padding=1, bias=False),\n                nn.SyncBatchNorm(inplanes),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inplanes, inplanes, kernel_size=3, stride=1, padding=1, bias=False),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "get_reference_points",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "def get_reference_points(spatial_shapes, device):\n    reference_points_list = []\n    for lvl, (H_, W_) in enumerate(spatial_shapes):\n        ref_y, ref_x = torch.meshgrid(\n            torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n            torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device),\n        )\n        ref_y = ref_y.reshape(-1)[None] / H_\n        ref_x = ref_x.reshape(-1)[None] / W_\n        ref = torch.stack((ref_x, ref_y), -1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "deform_inputs",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "def deform_inputs(x, patch_size):\n    bs, c, h, w = x.shape\n    spatial_shapes = torch.as_tensor(\n        [(h // 8, w // 8), (h // 16, w // 16), (h // 32, w // 32)], dtype=torch.long, device=x.device\n    )\n    level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))\n    reference_points = get_reference_points([(h // patch_size, w // patch_size)], x.device)\n    deform_inputs1 = [reference_points, spatial_shapes, level_start_index]\n    spatial_shapes = torch.as_tensor([(h // patch_size, w // patch_size)], dtype=torch.long, device=x.device)\n    level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n    def __init__(self, drop_prob: float = 0.0):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor\nclass DropPath(nn.Module):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFN",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class SwiGLUFFN(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = None,\n        drop: float = 0.0,\n    ) -> None:\n        super().__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"2D Image to Patch Embedding.\"\"\"\n    def __init__(\n        self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True, bias=True\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0.0, proj_drop=0.0):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "MemEffAttention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class MemEffAttention(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -> None:\n        super().__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "WindowedAttention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class WindowedAttention(nn.Module):\n    def __init__(\n        self, dim, num_heads=8, qkv_bias=False, attn_drop=0.0, proj_drop=0.0, window_size=14, pad_mode=\"constant\"\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "TIMMVisionTransformer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class TIMMVisionTransformer(BaseModule):\n    \"\"\"Vision Transformer.\n    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n        - https://arxiv.org/abs/2010.11929\n    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`\n        - https://arxiv.org/abs/2012.12877\n    \"\"\"\n    def __init__(\n        self,\n        img_size=224,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "def to_2tuple(x):\n    return tuple(repeat(x, 2))\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        drop: float = 0.0,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "def window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "ViTAdapter",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit_adapter",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit_adapter",
        "peekOfCode": "class ViTAdapter(TIMMVisionTransformer):\n    def __init__(\n        self,\n        pretrain_size=224,\n        num_heads=12,\n        conv_inplane=64,\n        n_points=4,\n        deform_num_heads=6,\n        init_values=0.0,\n        interaction_indexes=None,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit_adapter",
        "documentation": {}
    },
    {
        "label": "Mask2FormerHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.decode_heads.mask2former_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.decode_heads.mask2former_head",
        "peekOfCode": "class Mask2FormerHead(BaseDecodeHead):\n    \"\"\"Implements the Mask2Former head.\n    See `Masked-attention Mask Transformer for Universal Image\n    Segmentation <https://arxiv.org/pdf/2112.01527>`_ for details.\n    Args:\n        in_channels (list[int]): Number of channels in the input feature map.\n        feat_channels (int): Number of channels for features.\n        out_channels (int): Number of channels for output.\n        num_things_classes (int): Number of things.\n        num_stuff_classes (int): Number of stuff.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.decode_heads.mask2former_head",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "peekOfCode": "class CrossEntropyLoss(nn.Module):\n    \"\"\"CrossEntropyLoss.\n    Args:\n        use_sigmoid (bool, optional): Whether the prediction uses sigmoid\n            of softmax. Defaults to False.\n        use_mask (bool, optional): Whether to use mask cross entropy loss.\n            Defaults to False.\n        reduction (str, optional): . Defaults to 'mean'.\n            Options are \"none\", \"mean\" and \"sum\".\n        class_weight (list[float] | str, optional): Weight of each class. If in",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "documentation": {}
    },
    {
        "label": "cross_entropy",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "peekOfCode": "def cross_entropy(\n    pred,\n    label,\n    weight=None,\n    class_weight=None,\n    reduction=\"mean\",\n    avg_factor=None,\n    ignore_index=-100,\n    avg_non_ignore=False,\n):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "documentation": {}
    },
    {
        "label": "binary_cross_entropy",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "peekOfCode": "def binary_cross_entropy(\n    pred,\n    label,\n    weight=None,\n    reduction=\"mean\",\n    avg_factor=None,\n    class_weight=None,\n    ignore_index=-100,\n    avg_non_ignore=False,\n    **kwargs,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "documentation": {}
    },
    {
        "label": "mask_cross_entropy",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "peekOfCode": "def mask_cross_entropy(\n    pred, target, label, reduction=\"mean\", avg_factor=None, class_weight=None, ignore_index=None, **kwargs\n):\n    \"\"\"Calculate the CrossEntropy loss for masks.\n    Args:\n        pred (torch.Tensor): The prediction with shape (N, C), C is the number\n            of classes.\n        target (torch.Tensor): The learning label of the prediction.\n        label (torch.Tensor): ``label`` indicates the class label of the mask'\n            corresponding object. This will be used to select the mask in the",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "documentation": {}
    },
    {
        "label": "DiceLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "peekOfCode": "class DiceLoss(nn.Module):\n    def __init__(self, use_sigmoid=True, activate=True, reduction=\"mean\", naive_dice=False, loss_weight=1.0, eps=1e-3):\n        \"\"\"Dice Loss, there are two forms of dice loss is supported:\n            - the one proposed in `V-Net: Fully Convolutional Neural\n                Networks for Volumetric Medical Image Segmentation\n                <https://arxiv.org/abs/1606.04797>`_.\n            - the dice loss in which the power of the number in the\n                denominator is the first power instead of the second\n                power.\n        Args:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "documentation": {}
    },
    {
        "label": "dice_loss",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "peekOfCode": "def dice_loss(pred, target, weight=None, eps=1e-3, reduction=\"mean\", avg_factor=None):\n    \"\"\"Calculate dice loss, which is proposed in\n    `V-Net: Fully Convolutional Neural Networks for Volumetric\n    Medical Image Segmentation <https://arxiv.org/abs/1606.04797>`_.\n    Args:\n        pred (torch.Tensor): The prediction, has a shape (n, *)\n        target (torch.Tensor): The learning label of the prediction,\n            shape (n, *), same shape of pred.\n        weight (torch.Tensor, optional): The weight of loss for each\n            prediction, has a shape (n,). Defaults to None.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "documentation": {}
    },
    {
        "label": "naive_dice_loss",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "peekOfCode": "def naive_dice_loss(pred, target, weight=None, eps=1e-3, reduction=\"mean\", avg_factor=None):\n    \"\"\"Calculate naive dice loss, the coefficient in the denominator is the\n    first power instead of the second power.\n    Args:\n        pred (torch.Tensor): The prediction, has a shape (n, *)\n        target (torch.Tensor): The learning label of the prediction,\n            shape (n, *), same shape of pred.\n        weight (torch.Tensor, optional): The weight of loss for each\n            prediction, has a shape (n,). Defaults to None.\n        eps (float): Avoid dividing by zero. Default: 1e-3.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "documentation": {}
    },
    {
        "label": "ClassificationCost",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "peekOfCode": "class ClassificationCost:\n    \"\"\"ClsSoftmaxCost.Borrow from\n    mmdet.core.bbox.match_costs.match_cost.ClassificationCost.\n     Args:\n         weight (int | float, optional): loss_weight\n     Examples:\n         >>> import torch\n         >>> self = ClassificationCost()\n         >>> cls_pred = torch.rand(4, 3)\n         >>> gt_labels = torch.tensor([0, 1, 2])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "documentation": {}
    },
    {
        "label": "DiceCost",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "peekOfCode": "class DiceCost:\n    \"\"\"Cost of mask assignments based on dice losses.\n    Args:\n        weight (int | float, optional): loss_weight. Defaults to 1.\n        pred_act (bool, optional): Whether to apply sigmoid to mask_pred.\n            Defaults to False.\n        eps (float, optional): default 1e-12.\n    \"\"\"\n    def __init__(self, weight=1.0, pred_act=False, eps=1e-3):\n        self.weight = weight",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLossCost",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "peekOfCode": "class CrossEntropyLossCost:\n    \"\"\"CrossEntropyLossCost.\n    Args:\n        weight (int | float, optional): loss weight. Defaults to 1.\n        use_sigmoid (bool, optional): Whether the prediction uses sigmoid\n                of softmax. Defaults to True.\n    \"\"\"\n    def __init__(self, weight=1.0, use_sigmoid=True):\n        assert use_sigmoid, \"use_sigmoid = False is not supported yet.\"\n        self.weight = weight",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "documentation": {}
    },
    {
        "label": "MSDeformAttnPixelDecoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.plugins.msdeformattn_pixel_decoder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.plugins.msdeformattn_pixel_decoder",
        "peekOfCode": "class MSDeformAttnPixelDecoder(BaseModule):\n    \"\"\"Pixel decoder with multi-scale deformable attention.\n    Args:\n        in_channels (list[int] | tuple[int]): Number of channels in the\n            input feature maps.\n        strides (list[int] | tuple[int]): Output strides of feature from\n            backbone.\n        feat_channels (int): Number of channels for feature.\n        out_channels (int): Number of channels for output.\n        num_outs (int): Number of output scales.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.plugins.msdeformattn_pixel_decoder",
        "documentation": {}
    },
    {
        "label": "EncoderDecoderMask2Former",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.segmentors.encoder_decoder_mask2former",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.segmentors.encoder_decoder_mask2former",
        "peekOfCode": "class EncoderDecoderMask2Former(BaseSegmentor):\n    \"\"\"Encoder Decoder segmentors.\n    EncoderDecoder typically consists of backbone, decode_head, auxiliary_head.\n    Note that auxiliary_head is only used for deep supervision during training,\n    which could be dumped during inference.\n    \"\"\"\n    def __init__(\n        self,\n        backbone,\n        decode_head,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.segmentors.encoder_decoder_mask2former",
        "documentation": {}
    },
    {
        "label": "AssignResult",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "peekOfCode": "class AssignResult(metaclass=ABCMeta):\n    \"\"\"Collection of assign results.\"\"\"\n    def __init__(self, num_gts, gt_inds, labels):\n        self.num_gts = num_gts\n        self.gt_inds = gt_inds\n        self.labels = labels\n    @property\n    def info(self):\n        info = {\n            \"num_gts\": self.num_gts,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "documentation": {}
    },
    {
        "label": "BaseAssigner",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "peekOfCode": "class BaseAssigner(metaclass=ABCMeta):\n    \"\"\"Base assigner that assigns boxes to ground truth boxes.\"\"\"\n    @abstractmethod\n    def assign(self, masks, gt_masks, gt_masks_ignore=None, gt_labels=None):\n        \"\"\"Assign boxes to either a ground truth boxes or a negative boxes.\"\"\"\n        pass\n@MASK_ASSIGNERS.register_module()\nclass MaskHungarianAssigner(BaseAssigner):\n    \"\"\"Computes one-to-one matching between predictions and ground truth for\n    mask.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "documentation": {}
    },
    {
        "label": "MaskHungarianAssigner",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "peekOfCode": "class MaskHungarianAssigner(BaseAssigner):\n    \"\"\"Computes one-to-one matching between predictions and ground truth for\n    mask.\n    This class computes an assignment between the targets and the predictions\n    based on the costs. The costs are weighted sum of three components:\n    classification cost, regression L1 cost and regression iou cost. The\n    targets don't include the no_object, so generally there are more\n    predictions than targets. After the one-to-one matching, the un-matched\n    are treated as backgrounds. Thus each query prediction will be assigned\n    with `0` or a positive integer indicating the ground truth index:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "documentation": {}
    },
    {
        "label": "get_uncertainty",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "peekOfCode": "def get_uncertainty(mask_pred, labels):\n    \"\"\"Estimate uncertainty based on pred logits.\n    We estimate uncertainty as L1 distance between 0.0 and the logits\n    prediction in 'mask_pred' for the foreground class in `classes`.\n    Args:\n        mask_pred (Tensor): mask predication logits, shape (num_rois,\n            num_classes, mask_height, mask_width).\n        labels (list[Tensor]): Either predicted or ground truth label for\n            each predicted mask, of length num_rois.\n    Returns:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "documentation": {}
    },
    {
        "label": "get_uncertain_point_coords_with_randomness",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "peekOfCode": "def get_uncertain_point_coords_with_randomness(\n    mask_pred, labels, num_points, oversample_ratio, importance_sample_ratio\n):\n    \"\"\"Get ``num_points`` most uncertain points with random points during\n    train.\n    Sample points in [0, 1] x [0, 1] coordinate space based on their\n    uncertainty. The uncertainties are calculated for each point using\n    'get_uncertainty()' function that takes point's logit prediction as\n    input.\n    Args:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "documentation": {}
    },
    {
        "label": "SinePositionalEncoding",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "peekOfCode": "class SinePositionalEncoding(BaseModule):\n    \"\"\"Position encoding with sine and cosine functions.\n    See `End-to-End Object Detection with Transformers\n    <https://arxiv.org/pdf/2005.12872>`_ for details.\n    Args:\n        num_feats (int): The feature dimension for each position\n            along x-axis or y-axis. Note the final returned dimension\n            for each position is 2 times of this value.\n        temperature (int, optional): The temperature used for scaling\n            the position embedding. Defaults to 10000.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "documentation": {}
    },
    {
        "label": "LearnedPositionalEncoding",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "peekOfCode": "class LearnedPositionalEncoding(BaseModule):\n    \"\"\"Position embedding with learnable embedding weights.\n    Args:\n        num_feats (int): The feature dimension for each position\n            along x-axis or y-axis. The final returned dimension for\n            each position is 2 times of this value.\n        row_num_embed (int, optional): The dictionary size of row embeddings.\n            Default 50.\n        col_num_embed (int, optional): The dictionary size of col embeddings.\n            Default 50.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "documentation": {}
    },
    {
        "label": "AdaptivePadding",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class AdaptivePadding(nn.Module):\n    \"\"\"Applies padding to input (if needed) so that input can get fully covered\n    by filter you specified. It support two modes \"same\" and \"corner\". The\n    \"same\" mode is same with \"SAME\" padding mode in TensorFlow, pad zero around\n    input. The \"corner\"  mode would pad zero to bottom right.\n    Args:\n        kernel_size (int | tuple): Size of the kernel:\n        stride (int | tuple): Stride of the filter. Default: 1:\n        dilation (int | tuple): Spacing between kernel elements.\n            Default: 1",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class PatchMerging(BaseModule):\n    \"\"\"Merge patch feature map.\n    This layer groups feature map by kernel_size, and applies norm and linear\n    layers to the grouped feature map. Our implementation uses `nn.Unfold` to\n    merge patch, which is about 25% faster than original implementation.\n    Instead, we need to modify pretrained models for compatibility.\n    Args:\n        in_channels (int): The num of input channels.\n            to gets fully covered by filter and stride you specified..\n            Default: True.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "FFN",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class FFN(BaseModule):\n    \"\"\"Implements feed-forward networks (FFNs) with identity connection.\n    Args:\n        embed_dims (int): The feature dimension. Same as\n            `MultiheadAttention`. Defaults: 256.\n        feedforward_channels (int): The hidden dimension of FFNs.\n            Defaults: 1024.\n        num_fcs (int, optional): The number of fully-connected layers in\n            FFNs. Default: 2.\n        act_cfg (dict, optional): The activation config for FFNs.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DetrTransformerDecoderLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DetrTransformerDecoderLayer(BaseTransformerLayer):\n    \"\"\"Implements decoder layer in DETR transformer.\n    Args:\n        attn_cfgs (list[`mmcv.ConfigDict`] | list[dict] | dict )):\n            Configs for self_attention or cross_attention, the order\n            should be consistent with it in `operation_order`. If it is\n            a dict, it would be expand to the number of attention in\n            `operation_order`.\n        feedforward_channels (int): The hidden dimension for FFNs.\n        ffn_dropout (float): Probability of an element to be zeroed",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DetrTransformerEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DetrTransformerEncoder(TransformerLayerSequence):\n    \"\"\"TransformerEncoder of DETR.\n    Args:\n        post_norm_cfg (dict): Config of last normalization layer. Default\n            `LN`. Only used when `self.pre_norm` is `True`\n    \"\"\"\n    def __init__(self, *args, post_norm_cfg=dict(type=\"LN\"), **kwargs):\n        super(DetrTransformerEncoder, self).__init__(*args, **kwargs)\n        if post_norm_cfg is not None:\n            self.post_norm = build_norm_layer(post_norm_cfg, self.embed_dims)[1] if self.pre_norm else None",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DetrTransformerDecoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DetrTransformerDecoder(TransformerLayerSequence):\n    \"\"\"Implements the decoder in DETR transformer.\n    Args:\n        return_intermediate (bool): Whether to return intermediate outputs.\n        post_norm_cfg (dict): Config of last normalization layer. Default\n            `LN`.\n    \"\"\"\n    def __init__(self, *args, post_norm_cfg=dict(type=\"LN\"), return_intermediate=False, **kwargs):\n        super(DetrTransformerDecoder, self).__init__(*args, **kwargs)\n        self.return_intermediate = return_intermediate",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class Transformer(BaseModule):\n    \"\"\"Implements the DETR transformer.\n    Following the official DETR implementation, this module copy-paste\n    from torch.nn.Transformer with modifications:\n        * positional encodings are passed in MultiheadAttention\n        * extra LN at the end of encoder is removed\n        * decoder returns a stack of activations from all decoding layers\n    See `paper: End-to-End Object Detection with Transformers\n    <https://arxiv.org/pdf/2005.12872>`_ for details.\n    Args:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DeformableDetrTransformerDecoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DeformableDetrTransformerDecoder(TransformerLayerSequence):\n    \"\"\"Implements the decoder in DETR transformer.\n    Args:\n        return_intermediate (bool): Whether to return intermediate outputs.\n        coder_norm_cfg (dict): Config of last normalization layer. Default\n            `LN`.\n    \"\"\"\n    def __init__(self, *args, return_intermediate=False, **kwargs):\n        super(DeformableDetrTransformerDecoder, self).__init__(*args, **kwargs)\n        self.return_intermediate = return_intermediate",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DeformableDetrTransformer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DeformableDetrTransformer(Transformer):\n    \"\"\"Implements the DeformableDETR transformer.\n    Args:\n        as_two_stage (bool): Generate query from encoder features.\n            Default: False.\n        num_feature_levels (int): Number of feature maps from FPN:\n            Default: 4.\n        two_stage_num_proposals (int): Number of proposals when set\n            `as_two_stage` as True. Default: 300.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DynamicConv",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DynamicConv(BaseModule):\n    \"\"\"Implements Dynamic Convolution.\n    This module generate parameters for each sample and\n    use bmm to implement 1*1 convolution. Code is modified\n    from the `official github repo <https://github.com/PeizeSun/\n    SparseR-CNN/blob/main/projects/SparseRCNN/sparsercnn/head.py#L258>`_ .\n    Args:\n        in_channels (int): The input feature channel.\n            Defaults to 256.\n        feat_channels (int): The inner feature channel.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "def inverse_sigmoid(x, eps=1e-5):\n    \"\"\"Inverse function of sigmoid.\n    Args:\n        x (Tensor): The tensor to do the\n            inverse.\n        eps (float): EPS avoid numerical\n            overflow. Defaults 1e-5.\n    Returns:\n        Tensor: The x has passed the inverse\n            function of sigmoid, has same",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "build_match_cost",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "def build_match_cost(cfg):\n    \"\"\"Build Match Cost.\"\"\"\n    return MATCH_COST.build(cfg)\ndef build_assigner(cfg):\n    \"\"\"Build Assigner.\"\"\"\n    return MASK_ASSIGNERS.build(cfg)\ndef build_transformer(cfg):\n    \"\"\"Build Transformer.\"\"\"\n    return TRANSFORMER.build(cfg)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "build_assigner",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "def build_assigner(cfg):\n    \"\"\"Build Assigner.\"\"\"\n    return MASK_ASSIGNERS.build(cfg)\ndef build_transformer(cfg):\n    \"\"\"Build Transformer.\"\"\"\n    return TRANSFORMER.build(cfg)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "build_transformer",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "def build_transformer(cfg):\n    \"\"\"Build Transformer.\"\"\"\n    return TRANSFORMER.build(cfg)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "TRANSFORMER",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "TRANSFORMER = Registry(\"Transformer\")\nMASK_ASSIGNERS = Registry(\"mask_assigner\")\nMATCH_COST = Registry(\"match_cost\")\ndef build_match_cost(cfg):\n    \"\"\"Build Match Cost.\"\"\"\n    return MATCH_COST.build(cfg)\ndef build_assigner(cfg):\n    \"\"\"Build Assigner.\"\"\"\n    return MASK_ASSIGNERS.build(cfg)\ndef build_transformer(cfg):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "MASK_ASSIGNERS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "MASK_ASSIGNERS = Registry(\"mask_assigner\")\nMATCH_COST = Registry(\"match_cost\")\ndef build_match_cost(cfg):\n    \"\"\"Build Match Cost.\"\"\"\n    return MATCH_COST.build(cfg)\ndef build_assigner(cfg):\n    \"\"\"Build Assigner.\"\"\"\n    return MASK_ASSIGNERS.build(cfg)\ndef build_transformer(cfg):\n    \"\"\"Build Transformer.\"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "MATCH_COST",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "MATCH_COST = Registry(\"match_cost\")\ndef build_match_cost(cfg):\n    \"\"\"Build Match Cost.\"\"\"\n    return MATCH_COST.build(cfg)\ndef build_assigner(cfg):\n    \"\"\"Build Assigner.\"\"\"\n    return MASK_ASSIGNERS.build(cfg)\ndef build_transformer(cfg):\n    \"\"\"Build Transformer.\"\"\"\n    return TRANSFORMER.build(cfg)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "MSDeformAttnFunction",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "peekOfCode": "class MSDeformAttnFunction(Function):\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float32)\n    def forward(\n        ctx, value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, im2col_step\n    ):\n        output = ms_deform_attn_core_pytorch(\n            value,\n            value_spatial_shapes,\n            #  value_level_start_index,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "MSDeformAttn",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "peekOfCode": "class MSDeformAttn(nn.Module):\n    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4, ratio=1.0):\n        \"\"\"Multi-Scale Deformable Attention Module.\n        :param d_model      hidden dimension\n        :param n_levels     number of feature levels\n        :param n_heads      number of attention heads\n        :param n_points     number of sampling points per attention head per feature level\n        \"\"\"\n        super().__init__()\n        if d_model % n_heads != 0:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "ms_deform_attn_core_pytorch",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "peekOfCode": "def ms_deform_attn_core_pytorch(value, value_spatial_shapes, sampling_locations, attention_weights):\n    # for debug and test only,\n    # need to use cuda version instead\n    N_, S_, M_, D_ = value.shape\n    _, Lq_, M_, L_, P_, _ = sampling_locations.shape\n    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)\n    sampling_grids = 2 * sampling_locations - 1\n    sampling_value_list = []\n    for lid_, (H_, W_) in enumerate(value_spatial_shapes):\n        # N_, H_*W_, M_, D_ -> N_, H_*W_, M_*D_ -> N_, M_*D_, H_*W_ -> N_*M_, D_, H_, W_",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "KnnModule",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "class KnnModule(torch.nn.Module):\n    \"\"\"\n    Gets knn of test features from all processes on a chunk of the train features\n    Each rank gets a chunk of the train features as well as a chunk of the test features.\n    In `compute_neighbors`, for each rank one after the other, its chunk of test features\n    is sent to all devices, partial knns are computed with each chunk of train features\n    then collated back on the original device.\n    \"\"\"\n    def __init__(self, train_features, train_labels, nb_knn, T, device, num_classes=1000):\n        super().__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "DictKeysModule",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "class DictKeysModule(torch.nn.Module):\n    def __init__(self, keys):\n        super().__init__()\n        self.keys = keys\n    def forward(self, features_dict, targets):\n        for k in self.keys:\n            features_dict = features_dict[k]\n        return {\"preds\": features_dict, \"target\": targets}\ndef create_module_dict(*, module, n_per_class_list, n_tries, nb_knn, train_features, train_labels):\n    modules = {}",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "ModuleDictWithForward",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "class ModuleDictWithForward(torch.nn.ModuleDict):\n    def forward(self, *args, **kwargs):\n        return {k: module(*args, **kwargs) for k, module in self._modules.items()}\ndef eval_knn(\n    model,\n    train_dataset,\n    val_dataset,\n    accuracy_averaging,\n    nb_knn,\n    temperature,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(\n        description=description,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "create_module_dict",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def create_module_dict(*, module, n_per_class_list, n_tries, nb_knn, train_features, train_labels):\n    modules = {}\n    mapping = create_class_indices_mapping(train_labels)\n    for npc in n_per_class_list:\n        if npc < 0:  # Only one try needed when using the full data\n            full_module = module(\n                train_features=train_features,\n                train_labels=train_labels,\n                nb_knn=nb_knn,\n            )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "filter_train",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def filter_train(mapping, n_per_class, seed):\n    torch.manual_seed(seed)\n    final_indices = []\n    for k in mapping.keys():\n        index = torch.randperm(len(mapping[k]))[:n_per_class]\n        final_indices.append(mapping[k][index])\n    return torch.cat(final_indices).squeeze()\ndef create_class_indices_mapping(labels):\n    unique_labels, inverse = torch.unique(labels, return_inverse=True)\n    mapping = {unique_labels[i]: (inverse == i).nonzero() for i in range(len(unique_labels))}",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "create_class_indices_mapping",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def create_class_indices_mapping(labels):\n    unique_labels, inverse = torch.unique(labels, return_inverse=True)\n    mapping = {unique_labels[i]: (inverse == i).nonzero() for i in range(len(unique_labels))}\n    return mapping\nclass ModuleDictWithForward(torch.nn.ModuleDict):\n    def forward(self, *args, **kwargs):\n        return {k: module(*args, **kwargs) for k, module in self._modules.items()}\ndef eval_knn(\n    model,\n    train_dataset,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "eval_knn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def eval_knn(\n    model,\n    train_dataset,\n    val_dataset,\n    accuracy_averaging,\n    nb_knn,\n    temperature,\n    batch_size,\n    num_workers,\n    gather_on_cpu,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "eval_knn_with_model",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def eval_knn_with_model(\n    model,\n    output_dir,\n    train_dataset_str=\"ImageNet:split=TRAIN\",\n    val_dataset_str=\"ImageNet:split=VAL\",\n    nb_knn=(10, 20, 100, 200),\n    temperature=0.07,\n    autocast_dtype=torch.float,\n    accuracy_averaging=AccuracyAveraging.MEAN_ACCURACY,\n    transform=None,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def main(args):\n    model, autocast_dtype = setup_and_build_model(args)\n    eval_knn_with_model(\n        model=model,\n        output_dir=args.output_dir,\n        train_dataset_str=args.train_dataset_str,\n        val_dataset_str=args.val_dataset_str,\n        nb_knn=args.nb_knn,\n        temperature=args.temperature,\n        autocast_dtype=autocast_dtype,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "LinearClassifier",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "class LinearClassifier(nn.Module):\n    \"\"\"Linear layer to train on top of frozen features\"\"\"\n    def __init__(self, out_dim, use_n_blocks, use_avgpool, num_classes=1000):\n        super().__init__()\n        self.out_dim = out_dim\n        self.use_n_blocks = use_n_blocks\n        self.use_avgpool = use_avgpool\n        self.num_classes = num_classes\n        self.linear = nn.Linear(out_dim, num_classes)\n        self.linear.weight.data.normal_(mean=0.0, std=0.01)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "AllClassifiers",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "class AllClassifiers(nn.Module):\n    def __init__(self, classifiers_dict):\n        super().__init__()\n        self.classifiers_dict = nn.ModuleDict()\n        self.classifiers_dict.update(classifiers_dict)\n    def forward(self, inputs):\n        return {k: v.forward(inputs) for k, v in self.classifiers_dict.items()}\n    def __len__(self):\n        return len(self.classifiers_dict)\nclass LinearPostprocessor(nn.Module):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "LinearPostprocessor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "class LinearPostprocessor(nn.Module):\n    def __init__(self, linear_classifier, class_mapping=None):\n        super().__init__()\n        self.linear_classifier = linear_classifier\n        self.register_buffer(\"class_mapping\", None if class_mapping is None else torch.LongTensor(class_mapping))\n    def forward(self, samples, targets):\n        preds = self.linear_classifier(samples)\n        return {\n            \"preds\": preds[:, self.class_mapping] if self.class_mapping is not None else preds,\n            \"target\": targets,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(\n        description=description,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "has_ddp_wrapper",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def has_ddp_wrapper(m: nn.Module) -> bool:\n    return isinstance(m, DistributedDataParallel)\ndef remove_ddp_wrapper(m: nn.Module) -> nn.Module:\n    return m.module if has_ddp_wrapper(m) else m\ndef _pad_and_collate(batch):\n    maxlen = max(len(targets) for image, targets in batch)\n    padded_batch = [\n        (image, np.pad(targets, (0, maxlen - len(targets)), constant_values=-1)) for image, targets in batch\n    ]\n    return torch.utils.data.default_collate(padded_batch)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "remove_ddp_wrapper",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def remove_ddp_wrapper(m: nn.Module) -> nn.Module:\n    return m.module if has_ddp_wrapper(m) else m\ndef _pad_and_collate(batch):\n    maxlen = max(len(targets) for image, targets in batch)\n    padded_batch = [\n        (image, np.pad(targets, (0, maxlen - len(targets)), constant_values=-1)) for image, targets in batch\n    ]\n    return torch.utils.data.default_collate(padded_batch)\ndef create_linear_input(x_tokens_list, use_n_blocks, use_avgpool):\n    intermediate_output = x_tokens_list[-use_n_blocks:]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "create_linear_input",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def create_linear_input(x_tokens_list, use_n_blocks, use_avgpool):\n    intermediate_output = x_tokens_list[-use_n_blocks:]\n    output = torch.cat([class_token for _, class_token in intermediate_output], dim=-1)\n    if use_avgpool:\n        output = torch.cat(\n            (\n                output,\n                torch.mean(intermediate_output[-1][0], dim=1),  # patch tokens\n            ),\n            dim=-1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "scale_lr",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def scale_lr(learning_rates, batch_size):\n    return learning_rates * (batch_size * distributed.get_global_size()) / 256.0\ndef setup_linear_classifiers(sample_output, n_last_blocks_list, learning_rates, batch_size, num_classes=1000):\n    linear_classifiers_dict = nn.ModuleDict()\n    optim_param_groups = []\n    for n in n_last_blocks_list:\n        for avgpool in [False, True]:\n            for _lr in learning_rates:\n                lr = scale_lr(_lr, batch_size)\n                out_dim = create_linear_input(sample_output, use_n_blocks=n, use_avgpool=avgpool).shape[1]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "setup_linear_classifiers",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def setup_linear_classifiers(sample_output, n_last_blocks_list, learning_rates, batch_size, num_classes=1000):\n    linear_classifiers_dict = nn.ModuleDict()\n    optim_param_groups = []\n    for n in n_last_blocks_list:\n        for avgpool in [False, True]:\n            for _lr in learning_rates:\n                lr = scale_lr(_lr, batch_size)\n                out_dim = create_linear_input(sample_output, use_n_blocks=n, use_avgpool=avgpool).shape[1]\n                linear_classifier = LinearClassifier(\n                    out_dim, use_n_blocks=n, use_avgpool=avgpool, num_classes=num_classes",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "evaluate_linear_classifiers",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def evaluate_linear_classifiers(\n    feature_model,\n    linear_classifiers,\n    data_loader,\n    metric_type,\n    metrics_file_path,\n    training_num_classes,\n    iteration,\n    prefixstring=\"\",\n    class_mapping=None,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "eval_linear",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def eval_linear(\n    *,\n    feature_model,\n    linear_classifiers,\n    train_data_loader,\n    val_data_loader,\n    metrics_file_path,\n    optimizer,\n    scheduler,\n    output_dir,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "make_eval_data_loader",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def make_eval_data_loader(test_dataset_str, batch_size, num_workers, metric_type):\n    test_dataset = make_dataset(\n        dataset_str=test_dataset_str,\n        transform=make_classification_eval_transform(),\n    )\n    test_data_loader = make_data_loader(\n        dataset=test_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        sampler_type=SamplerType.DISTRIBUTED,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "test_on_datasets",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def test_on_datasets(\n    feature_model,\n    linear_classifiers,\n    test_dataset_strs,\n    batch_size,\n    num_workers,\n    test_metric_types,\n    metrics_file_path,\n    training_num_classes,\n    iteration,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "run_eval_linear",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def run_eval_linear(\n    model,\n    output_dir,\n    train_dataset_str,\n    val_dataset_str,\n    batch_size,\n    epochs,\n    epoch_length,\n    num_workers,\n    save_checkpoint_frequency,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def main(args):\n    model, autocast_dtype = setup_and_build_model(args)\n    run_eval_linear(\n        model=model,\n        output_dir=args.output_dir,\n        train_dataset_str=args.train_dataset_str,\n        val_dataset_str=args.val_dataset_str,\n        test_dataset_strs=args.test_dataset_strs,\n        batch_size=args.batch_size,\n        epochs=args.epochs,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "LogRegModule",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "class LogRegModule(nn.Module):\n    def __init__(\n        self,\n        C,\n        max_iter=DEFAULT_MAX_ITER,\n        dtype=torch.float64,\n        device=_CPU_DEVICE,\n    ):\n        super().__init__()\n        self.dtype = dtype",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(\n        description=description,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def evaluate_model(*, logreg_model, logreg_metric, test_data_loader, device):\n    postprocessors = {\"metrics\": logreg_model}\n    metrics = {\"metrics\": logreg_metric}\n    return evaluate(nn.Identity(), test_data_loader, postprocessors, metrics, device)\ndef train_for_C(*, C, max_iter, train_features, train_labels, dtype=torch.float64, device=_CPU_DEVICE):\n    logreg_model = LogRegModule(C, max_iter=max_iter, dtype=dtype, device=device)\n    logreg_model.fit(train_features, train_labels)\n    return logreg_model\ndef train_and_evaluate(\n    *,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "train_for_C",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def train_for_C(*, C, max_iter, train_features, train_labels, dtype=torch.float64, device=_CPU_DEVICE):\n    logreg_model = LogRegModule(C, max_iter=max_iter, dtype=dtype, device=device)\n    logreg_model.fit(train_features, train_labels)\n    return logreg_model\ndef train_and_evaluate(\n    *,\n    C,\n    max_iter,\n    train_features,\n    train_labels,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "train_and_evaluate",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def train_and_evaluate(\n    *,\n    C,\n    max_iter,\n    train_features,\n    train_labels,\n    logreg_metric,\n    test_data_loader,\n    train_dtype=torch.float64,\n    train_features_device,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "sweep_C_values",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def sweep_C_values(\n    *,\n    train_features,\n    train_labels,\n    test_data_loader,\n    metric_type,\n    num_classes,\n    train_dtype=torch.float64,\n    train_features_device=_CPU_DEVICE,\n    max_train_iters=DEFAULT_MAX_ITER,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "eval_log_regression",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def eval_log_regression(\n    *,\n    model,\n    train_dataset,\n    val_dataset,\n    finetune_dataset,\n    metric_type,\n    batch_size,\n    num_workers,\n    finetune_on_val=False,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "eval_log_regression_with_model",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def eval_log_regression_with_model(\n    model,\n    train_dataset_str=\"ImageNet:split=TRAIN\",\n    val_dataset_str=\"ImageNet:split=VAL\",\n    finetune_dataset_str=None,\n    autocast_dtype=torch.float,\n    finetune_on_val=False,\n    metric_type=MetricType.MEAN_ACCURACY,\n    train_dtype=torch.float64,\n    train_features_device=_CPU_DEVICE,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def main(args):\n    model, autocast_dtype = setup_and_build_model(args)\n    eval_log_regression_with_model(\n        model=model,\n        train_dataset_str=args.train_dataset_str,\n        val_dataset_str=args.val_dataset_str,\n        finetune_dataset_str=args.finetune_dataset_str,\n        autocast_dtype=autocast_dtype,\n        finetune_on_val=args.finetune_on_val,\n        metric_type=args.metric_type,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nDEFAULT_MAX_ITER = 1_000\nC_POWER_RANGE = torch.linspace(-6, 5, 45)\n_CPU_DEVICE = torch.device(\"cpu\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MAX_ITER",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "DEFAULT_MAX_ITER = 1_000\nC_POWER_RANGE = torch.linspace(-6, 5, 45)\n_CPU_DEVICE = torch.device(\"cpu\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "C_POWER_RANGE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "C_POWER_RANGE = torch.linspace(-6, 5, 45)\n_CPU_DEVICE = torch.device(\"cpu\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "_CPU_DEVICE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "_CPU_DEVICE = torch.device(\"cpu\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "MetricType",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "class MetricType(Enum):\n    MEAN_ACCURACY = \"mean_accuracy\"\n    MEAN_PER_CLASS_ACCURACY = \"mean_per_class_accuracy\"\n    PER_CLASS_ACCURACY = \"per_class_accuracy\"\n    IMAGENET_REAL_ACCURACY = \"imagenet_real_accuracy\"\n    @property\n    def accuracy_averaging(self):\n        return getattr(AccuracyAveraging, self.name, None)\n    def __str__(self):\n        return self.value",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "AccuracyAveraging",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "class AccuracyAveraging(Enum):\n    MEAN_ACCURACY = \"micro\"\n    MEAN_PER_CLASS_ACCURACY = \"macro\"\n    PER_CLASS_ACCURACY = \"none\"\n    def __str__(self):\n        return self.value\ndef build_metric(metric_type: MetricType, *, num_classes: int, ks: Optional[tuple] = None):\n    if metric_type.accuracy_averaging is not None:\n        return build_topk_accuracy_metric(\n            average_type=metric_type.accuracy_averaging,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "ImageNetReaLAccuracy",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "class ImageNetReaLAccuracy(Metric):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = None\n    full_state_update: bool = False\n    def __init__(\n        self,\n        num_classes: int,\n        top_k: int = 1,\n        **kwargs: Any,\n    ) -> None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "def build_metric(metric_type: MetricType, *, num_classes: int, ks: Optional[tuple] = None):\n    if metric_type.accuracy_averaging is not None:\n        return build_topk_accuracy_metric(\n            average_type=metric_type.accuracy_averaging,\n            num_classes=num_classes,\n            ks=(1, 5) if ks is None else ks,\n        )\n    elif metric_type == MetricType.IMAGENET_REAL_ACCURACY:\n        return build_topk_imagenet_real_accuracy_metric(\n            num_classes=num_classes,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_topk_accuracy_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "def build_topk_accuracy_metric(average_type: AccuracyAveraging, num_classes: int, ks: tuple = (1, 5)):\n    metrics: Dict[str, Metric] = {\n        f\"top-{k}\": MulticlassAccuracy(top_k=k, num_classes=int(num_classes), average=average_type.value) for k in ks\n    }\n    return MetricCollection(metrics)\ndef build_topk_imagenet_real_accuracy_metric(num_classes: int, ks: tuple = (1, 5)):\n    metrics: Dict[str, Metric] = {f\"top-{k}\": ImageNetReaLAccuracy(top_k=k, num_classes=int(num_classes)) for k in ks}\n    return MetricCollection(metrics)\nclass ImageNetReaLAccuracy(Metric):\n    is_differentiable: bool = False",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_topk_imagenet_real_accuracy_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "def build_topk_imagenet_real_accuracy_metric(num_classes: int, ks: tuple = (1, 5)):\n    metrics: Dict[str, Metric] = {f\"top-{k}\": ImageNetReaLAccuracy(top_k=k, num_classes=int(num_classes)) for k in ks}\n    return MetricCollection(metrics)\nclass ImageNetReaLAccuracy(Metric):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = None\n    full_state_update: bool = False\n    def __init__(\n        self,\n        num_classes: int,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass MetricType(Enum):\n    MEAN_ACCURACY = \"mean_accuracy\"\n    MEAN_PER_CLASS_ACCURACY = \"mean_per_class_accuracy\"\n    PER_CLASS_ACCURACY = \"per_class_accuracy\"\n    IMAGENET_REAL_ACCURACY = \"imagenet_real_accuracy\"\n    @property\n    def accuracy_averaging(self):\n        return getattr(AccuracyAveraging, self.name, None)\n    def __str__(self):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "peekOfCode": "def get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parser = argparse.ArgumentParser(\n        description=description,\n        parents=parents or [],\n        add_help=add_help,\n    )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "get_autocast_dtype",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "peekOfCode": "def get_autocast_dtype(config):\n    teacher_dtype_str = config.compute_precision.teacher.backbone.mixed_precision.param_dtype\n    if teacher_dtype_str == \"fp16\":\n        return torch.half\n    elif teacher_dtype_str == \"bf16\":\n        return torch.bfloat16\n    else:\n        return torch.float\ndef build_model_for_eval(config, pretrained_weights):\n    model, _ = build_model_from_cfg(config, only_teacher=True)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "build_model_for_eval",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "peekOfCode": "def build_model_for_eval(config, pretrained_weights):\n    model, _ = build_model_from_cfg(config, only_teacher=True)\n    dinov2_utils.load_pretrained_weights(model, pretrained_weights, \"teacher\")\n    model.eval()\n    model.cuda()\n    return model\ndef setup_and_build_model(args) -> Tuple[Any, torch.dtype]:\n    cudnn.benchmark = True\n    config = setup(args)\n    model = build_model_for_eval(config, args.pretrained_weights)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "setup_and_build_model",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "peekOfCode": "def setup_and_build_model(args) -> Tuple[Any, torch.dtype]:\n    cudnn.benchmark = True\n    config = setup(args)\n    model = build_model_for_eval(config, args.pretrained_weights)\n    autocast_dtype = get_autocast_dtype(config)\n    return model, autocast_dtype",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "ModelWithNormalize",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "class ModelWithNormalize(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n    def forward(self, samples):\n        return nn.functional.normalize(self.model(samples), dim=1, p=2)\nclass ModelWithIntermediateLayers(nn.Module):\n    def __init__(self, feature_model, n_last_blocks, autocast_ctx):\n        super().__init__()\n        self.feature_model = feature_model",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "ModelWithIntermediateLayers",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "class ModelWithIntermediateLayers(nn.Module):\n    def __init__(self, feature_model, n_last_blocks, autocast_ctx):\n        super().__init__()\n        self.feature_model = feature_model\n        self.feature_model.eval()\n        self.n_last_blocks = n_last_blocks\n        self.autocast_ctx = autocast_ctx\n    def forward(self, images):\n        with torch.inference_mode():\n            with self.autocast_ctx():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "def evaluate(\n    model: nn.Module,\n    data_loader,\n    postprocessors: Dict[str, nn.Module],\n    metrics: Dict[str, MetricCollection],\n    device: torch.device,\n    criterion: Optional[nn.Module] = None,\n):\n    model.eval()\n    if criterion is not None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "all_gather_and_flatten",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "def all_gather_and_flatten(tensor_rank):\n    tensor_all_ranks = torch.empty(\n        distributed.get_global_size(),\n        *tensor_rank.shape,\n        dtype=tensor_rank.dtype,\n        device=tensor_rank.device,\n    )\n    tensor_list = list(tensor_all_ranks.unbind(0))\n    torch.distributed.all_gather(tensor_list, tensor_rank.contiguous())\n    return tensor_all_ranks.flatten(end_dim=1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "def extract_features(model, dataset, batch_size, num_workers, gather_on_cpu=False):\n    dataset_with_enumerated_targets = DatasetWithEnumeratedTargets(dataset)\n    sample_count = len(dataset_with_enumerated_targets)\n    data_loader = make_data_loader(\n        dataset=dataset_with_enumerated_targets,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        sampler_type=SamplerType.DISTRIBUTED,\n        drop_last=False,\n        shuffle=False,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "extract_features_with_dataloader",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "def extract_features_with_dataloader(model, data_loader, sample_count, gather_on_cpu=False):\n    gather_device = torch.device(\"cpu\") if gather_on_cpu else torch.device(\"cuda\")\n    metric_logger = MetricLogger(delimiter=\"  \")\n    features, all_labels = None, None\n    for samples, (index, labels_rank) in metric_logger.log_every(data_loader, 10):\n        samples = samples.cuda(non_blocking=True)\n        labels_rank = labels_rank.cuda(non_blocking=True)\n        index = index.cuda(non_blocking=True)\n        features_rank = model(samples).float()\n        # init storage feature matrix",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass ModelWithNormalize(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n    def forward(self, samples):\n        return nn.functional.normalize(self.model(samples), dim=1, p=2)\nclass ModelWithIntermediateLayers(nn.Module):\n    def __init__(self, feature_model, n_last_blocks, autocast_ctx):\n        super().__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "DepthBaseDecodeHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class DepthBaseDecodeHead(nn.Module):\n    \"\"\"Base class for BaseDecodeHead.\n    Args:\n        in_channels (List): Input channels.\n        channels (int): Channels after modules, before conv_depth.\n        conv_layer (nn.Module): Conv layers. Default: None.\n        act_layer (nn.Module): Activation layers. Default: nn.ReLU.\n        loss_decode (dict): Config of decode loss.\n            Default: ().\n        sampler (dict|None): The config of depth map sampler.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "BNHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class BNHead(DepthBaseDecodeHead):\n    \"\"\"Just a batchnorm.\"\"\"\n    def __init__(self, input_transform=\"resize_concat\", in_index=(0, 1, 2, 3), upsample=1, **kwargs):\n        super().__init__(**kwargs)\n        self.input_transform = input_transform\n        self.in_index = in_index\n        self.upsample = upsample\n        # self.bn = nn.SyncBatchNorm(self.in_channels)\n        if self.classify:\n            self.conv_depth = nn.Conv2d(self.channels, self.n_bins, kernel_size=1, padding=0, stride=1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class ConvModule(nn.Module):\n    \"\"\"A conv block that bundles conv/norm/activation layers.\n    This block simplifies the usage of convolution layers, which are commonly\n    used with a norm layer (e.g., BatchNorm) and activation layer (e.g., ReLU).\n    It is based upon three build methods: `build_conv_layer()`,\n    `build_norm_layer()` and `build_activation_layer()`.\n    Besides, we add some additional features in this module.\n    1. Automatically set `bias` of the conv layer.\n    2. Spectral norm is supported.\n    3. More padding modes are supported. Before PyTorch 1.5, nn.Conv2d only",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "Interpolate",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class Interpolate(nn.Module):\n    def __init__(self, scale_factor, mode, align_corners=False):\n        super(Interpolate, self).__init__()\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.align_corners = align_corners\n    def forward(self, x):\n        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n        return x",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "HeadDepth",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class HeadDepth(nn.Module):\n    def __init__(self, features):\n        super(HeadDepth, self).__init__()\n        self.head = nn.Sequential(\n            nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(features // 2, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n        )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "ReassembleBlocks",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class ReassembleBlocks(nn.Module):\n    \"\"\"ViTPostProcessBlock, process cls_token in ViT backbone output and\n    rearrange the feature vector to feature map.\n    Args:\n        in_channels (int): ViT feature channels. Default: 768.\n        out_channels (List): output channels of each stage.\n            Default: [96, 192, 384, 768].\n        readout_type (str): Type of readout operation. Default: 'ignore'.\n        patch_size (int): The patch size. Default: 16.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "PreActResidualConvUnit",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class PreActResidualConvUnit(nn.Module):\n    \"\"\"ResidualConvUnit, pre-activate residual unit.\n    Args:\n        in_channels (int): number of channels in the input feature map.\n        act_layer (nn.Module): activation layer.\n        norm_layer (nn.Module): norm layer.\n        stride (int): stride of the first block. Default: 1\n        dilation (int): dilation rate for convs layers. Default: 1.\n    \"\"\"\n    def __init__(self, in_channels, act_layer, norm_layer, stride=1, dilation=1):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class FeatureFusionBlock(nn.Module):\n    \"\"\"FeatureFusionBlock, merge feature map from different stages.\n    Args:\n        in_channels (int): Input channels.\n        act_layer (nn.Module): activation layer for ResidualConvUnit.\n        norm_layer (nn.Module): normalization layer.\n        expand (bool): Whether expand the channels in post process block.\n            Default: False.\n        align_corners (bool): align_corner setting for bilinear upsample.\n            Default: True.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class DPTHead(DepthBaseDecodeHead):\n    \"\"\"Vision Transformers for Dense Prediction.\n    This head is implemented of `DPT <https://arxiv.org/abs/2103.13413>`_.\n    Args:\n        embed_dims (int): The embed dimension of the ViT backbone.\n            Default: 768.\n        post_process_channels (List): Out channels of post process conv\n            layers. Default: [96, 192, 384, 768].\n        readout_type (str): Type of readout operation. Default: 'ignore'.\n        patch_size (int): The patch size. Default: 16.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "DepthEncoderDecoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "peekOfCode": "class DepthEncoderDecoder(nn.Module):\n    \"\"\"Encoder Decoder depther.\n    EncoderDecoder typically consists of backbone and decode_head.\n    \"\"\"\n    def __init__(self, backbone, decode_head):\n        super(DepthEncoderDecoder, self).__init__()\n        self.backbone = backbone\n        self.decode_head = decode_head\n        self.align_corners = self.decode_head.align_corners\n    def extract_feat(self, img):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "documentation": {}
    },
    {
        "label": "add_prefix",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "peekOfCode": "def add_prefix(inputs, prefix):\n    \"\"\"Add prefix for dict.\n    Args:\n        inputs (dict): The input dict with str keys.\n        prefix (str): The prefix to add.\n    Returns:\n        dict: The dict with keys updated with ``prefix``.\n    \"\"\"\n    outputs = dict()\n    for name, value in inputs.items():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.ops",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.ops",
        "peekOfCode": "def resize(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None, warning=False):\n    if warning:\n        if size is not None and align_corners:\n            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n            output_h, output_w = tuple(int(x) for x in size)\n            if output_h > input_h or output_w > output_h:\n                if (\n                    (output_h > 1 and output_w > 1 and input_h > 1 and input_w > 1)\n                    and (output_h - 1) % (input_h - 1)\n                    and (output_w - 1) % (input_w - 1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depth.ops",
        "documentation": {}
    },
    {
        "label": "Weights",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "class Weights(Enum):\n    LVD142M = \"LVD142M\"\ndef _make_dinov2_model(\n    *,\n    arch_name: str = \"vit_large\",\n    img_size: int = 518,\n    patch_size: int = 14,\n    init_values: float = 1.0,\n    ffn_layer: str = \"mlp\",\n    block_chunks: int = 0,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vits14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-S/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(arch_name=\"vit_small\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitb14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-B/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(arch_name=\"vit_base\", pretrained=pretrained, weights=weights, **kwargs)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitb14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-B/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(arch_name=\"vit_base\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitl14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-L/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(arch_name=\"vit_large\", pretrained=pretrained, weights=weights, **kwargs)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitl14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-L/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(arch_name=\"vit_large\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitg14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-g/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitg14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-g/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(\n        arch_name=\"vit_giant2\",\n        ffn_layer=\"swiglufused\",\n        weights=weights,\n        pretrained=pretrained,\n        **kwargs,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14_reg",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vits14_reg(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-S/14 model with registers (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(\n        arch_name=\"vit_small\",\n        pretrained=pretrained,\n        weights=weights,\n        num_register_tokens=4,\n        interpolate_antialias=True,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14_reg",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitb14_reg(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-B/14 model with registers (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(\n        arch_name=\"vit_base\",\n        pretrained=pretrained,\n        weights=weights,\n        num_register_tokens=4,\n        interpolate_antialias=True,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14_reg",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitl14_reg(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-L/14 model with registers (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(\n        arch_name=\"vit_large\",\n        pretrained=pretrained,\n        weights=weights,\n        num_register_tokens=4,\n        interpolate_antialias=True,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14_reg",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitg14_reg(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-g/14 model with registers (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(\n        arch_name=\"vit_giant2\",\n        ffn_layer=\"swiglufused\",\n        weights=weights,\n        pretrained=pretrained,\n        num_register_tokens=4,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "Weights",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "class Weights(Enum):\n    IMAGENET1K = \"IMAGENET1K\"\ndef _make_dinov2_linear_classification_head(\n    *,\n    arch_name: str = \"vit_large\",\n    patch_size: int = 14,\n    embed_dim: int = 1024,\n    layers: int = 4,\n    pretrained: bool = True,\n    weights: Union[Weights, str] = Weights.IMAGENET1K,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "_LinearClassifierWrapper",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "class _LinearClassifierWrapper(nn.Module):\n    def __init__(self, *, backbone: nn.Module, linear_head: nn.Module, layers: int = 4):\n        super().__init__()\n        self.backbone = backbone\n        self.linear_head = linear_head\n        self.layers = layers\n    def forward(self, x):\n        if self.layers == 1:\n            x = self.backbone.forward_features(x)\n            cls_token = x[\"x_norm_clstoken\"]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14_lc",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vits14_lc(\n    *,\n    layers: int = 4,\n    pretrained: bool = True,\n    weights: Union[Weights, str] = Weights.IMAGENET1K,\n    **kwargs,\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-S/14 backbone (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14_lc",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitb14_lc(\n    *,\n    layers: int = 4,\n    pretrained: bool = True,\n    weights: Union[Weights, str] = Weights.IMAGENET1K,\n    **kwargs,\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-B/14 backbone (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14_lc",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitl14_lc(\n    *,\n    layers: int = 4,\n    pretrained: bool = True,\n    weights: Union[Weights, str] = Weights.IMAGENET1K,\n    **kwargs,\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-L/14 backbone (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14_lc",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitg14_lc(\n    *,\n    layers: int = 4,\n    pretrained: bool = True,\n    weights: Union[Weights, str] = Weights.IMAGENET1K,\n    **kwargs,\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-g/14 backbone (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14_reg_lc",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vits14_reg_lc(\n    *, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.IMAGENET1K, **kwargs\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-S/14 backbone with registers (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"\n    return _make_dinov2_linear_classifier(\n        arch_name=\"vit_small\",\n        layers=layers,\n        pretrained=pretrained,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14_reg_lc",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitb14_reg_lc(\n    *, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.IMAGENET1K, **kwargs\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-B/14 backbone with registers (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"\n    return _make_dinov2_linear_classifier(\n        arch_name=\"vit_base\",\n        layers=layers,\n        pretrained=pretrained,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14_reg_lc",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitl14_reg_lc(\n    *, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.IMAGENET1K, **kwargs\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-L/14 backbone with registers (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"\n    return _make_dinov2_linear_classifier(\n        arch_name=\"vit_large\",\n        layers=layers,\n        pretrained=pretrained,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14_reg_lc",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitg14_reg_lc(\n    *, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.IMAGENET1K, **kwargs\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-g/14 backbone with registers (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"\n    return _make_dinov2_linear_classifier(\n        arch_name=\"vit_giant2\",\n        layers=layers,\n        ffn_layer=\"swiglufused\",",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "Weights",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "class Weights(Enum):\n    NYU = \"NYU\"\n    KITTI = \"KITTI\"\ndef _get_depth_range(pretrained: bool, weights: Weights = Weights.NYU) -> Tuple[float, float]:\n    if not pretrained:  # Default\n        return (0.001, 10.0)\n    # Pretrained, set according to the training dataset for the provided weights\n    if weights == Weights.KITTI:\n        return (0.001, 80.0)\n    if weights == Weights.NYU:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14_ld",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vits14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_small\", layers=layers, pretrained=pretrained, weights=weights, **kwargs\n    )\ndef dinov2_vitb14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_base\", layers=layers, pretrained=pretrained, weights=weights, **kwargs\n    )\ndef dinov2_vitl14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14_ld",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitb14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_base\", layers=layers, pretrained=pretrained, weights=weights, **kwargs\n    )\ndef dinov2_vitl14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_large\", layers=layers, pretrained=pretrained, weights=weights, **kwargs\n    )\ndef dinov2_vitg14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14_ld",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitl14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_large\", layers=layers, pretrained=pretrained, weights=weights, **kwargs\n    )\ndef dinov2_vitg14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_giant2\", layers=layers, ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )\ndef _make_dinov2_dpt_depth_head(*, embed_dim: int, min_depth: float, max_depth: float):\n    return DPTHead(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14_ld",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitg14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_giant2\", layers=layers, ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )\ndef _make_dinov2_dpt_depth_head(*, embed_dim: int, min_depth: float, max_depth: float):\n    return DPTHead(\n        in_channels=[embed_dim] * 4,\n        channels=256,\n        embed_dims=embed_dim,\n        post_process_channels=[embed_dim // 2 ** (3 - i) for i in range(4)],",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14_dd",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vits14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_small\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitb14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_base\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitl14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_large\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitg14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(\n        arch_name=\"vit_giant2\", ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14_dd",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitb14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_base\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitl14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_large\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitg14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(\n        arch_name=\"vit_giant2\", ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14_dd",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitl14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_large\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitg14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(\n        arch_name=\"vit_giant2\", ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14_dd",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitg14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(\n        arch_name=\"vit_giant2\", ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "CenterPadding",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "peekOfCode": "class CenterPadding(nn.Module):\n    def __init__(self, multiple):\n        super().__init__()\n        self.multiple = multiple\n    def _get_pad(self, size):\n        new_size = math.ceil(size / self.multiple) * self.multiple\n        pad_size = new_size - size\n        pad_size_left = pad_size // 2\n        pad_size_right = pad_size - pad_size_left\n        return pad_size_left, pad_size_right",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "documentation": {}
    },
    {
        "label": "_DINOV2_BASE_URL",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "peekOfCode": "_DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\ndef _make_dinov2_model_name(arch_name: str, patch_size: int, num_register_tokens: int = 0) -> str:\n    compact_arch_name = arch_name.replace(\"_\", \"\")[:4]\n    registers_suffix = f\"_reg{num_register_tokens}\" if num_register_tokens else \"\"\n    return f\"dinov2_{compact_arch_name}{patch_size}{registers_suffix}\"\nclass CenterPadding(nn.Module):\n    def __init__(self, multiple):\n        super().__init__()\n        self.multiple = multiple\n    def _get_pad(self, size):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        proj_bias: bool = True,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -> None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "documentation": {}
    },
    {
        "label": "MemEffAttention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "peekOfCode": "class MemEffAttention(Attention):\n    def forward(self, x: Tensor, attn_bias=None) -> Tensor:\n        if not XFORMERS_AVAILABLE:\n            if attn_bias is not None:\n                raise AssertionError(\"xFormers is required for using nested tensors\")\n            return super().forward(x)\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n        q, k, v = unbind(qkv, 2)\n        x = memory_efficient_attention(q, k, v, attn_bias=attn_bias)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nXFORMERS_ENABLED = os.environ.get(\"XFORMERS_DISABLED\") is None\ntry:\n    if XFORMERS_ENABLED:\n        from xformers.ops import memory_efficient_attention, unbind\n        XFORMERS_AVAILABLE = True\n        warnings.warn(\"xFormers is available (Attention)\")\n    else:\n        warnings.warn(\"xFormers is disabled (Attention)\")\n        raise ImportError",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "documentation": {}
    },
    {
        "label": "XFORMERS_ENABLED",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "peekOfCode": "XFORMERS_ENABLED = os.environ.get(\"XFORMERS_DISABLED\") is None\ntry:\n    if XFORMERS_ENABLED:\n        from xformers.ops import memory_efficient_attention, unbind\n        XFORMERS_AVAILABLE = True\n        warnings.warn(\"xFormers is available (Attention)\")\n    else:\n        warnings.warn(\"xFormers is disabled (Attention)\")\n        raise ImportError\nexcept ImportError:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        proj_bias: bool = True,\n        ffn_bias: bool = True,\n        drop: float = 0.0,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "NestedTensorBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "class NestedTensorBlock(Block):\n    def forward_nested(self, x_list: List[Tensor]) -> List[Tensor]:\n        \"\"\"\n        x_list contains a list of tensors to nest together and run\n        \"\"\"\n        assert isinstance(self.attn, MemEffAttention)\n        if self.training and self.sample_drop_ratio > 0.0:\n            def attn_residual_func(x: Tensor, attn_bias=None) -> Tensor:\n                return self.attn(self.norm1(x), attn_bias=attn_bias)\n            def ffn_residual_func(x: Tensor, attn_bias=None) -> Tensor:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "drop_add_residual_stochastic_depth",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "def drop_add_residual_stochastic_depth(\n    x: Tensor,\n    residual_func: Callable[[Tensor], Tensor],\n    sample_drop_ratio: float = 0.0,\n) -> Tensor:\n    # 1) extract subset using permutation\n    b, n, d = x.shape\n    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)\n    brange = (torch.randperm(b, device=x.device))[:sample_subset_size]\n    x_subset = x[brange]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "get_branges_scales",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "def get_branges_scales(x, sample_drop_ratio=0.0):\n    b, n, d = x.shape\n    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)\n    brange = (torch.randperm(b, device=x.device))[:sample_subset_size]\n    residual_scale_factor = b / sample_subset_size\n    return brange, residual_scale_factor\ndef add_residual(x, brange, residual, residual_scale_factor, scaling_vector=None):\n    if scaling_vector is None:\n        x_flat = x.flatten(1)\n        residual = residual.flatten(1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "add_residual",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "def add_residual(x, brange, residual, residual_scale_factor, scaling_vector=None):\n    if scaling_vector is None:\n        x_flat = x.flatten(1)\n        residual = residual.flatten(1)\n        x_plus_residual = torch.index_add(x_flat, 0, brange, residual.to(dtype=x.dtype), alpha=residual_scale_factor)\n    else:\n        x_plus_residual = scaled_index_add(\n            x, brange, residual.to(dtype=x.dtype), scaling=scaling_vector, alpha=residual_scale_factor\n        )\n    return x_plus_residual",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "get_attn_bias_and_cat",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "def get_attn_bias_and_cat(x_list, branges=None):\n    \"\"\"\n    this will perform the index select, cat the tensors, and provide the attn_bias from cache\n    \"\"\"\n    batch_sizes = [b.shape[0] for b in branges] if branges is not None else [x.shape[0] for x in x_list]\n    all_shapes = tuple((b, x.shape[1]) for b, x in zip(batch_sizes, x_list))\n    if all_shapes not in attn_bias_cache.keys():\n        seqlens = []\n        for b, x in zip(batch_sizes, x_list):\n            for _ in range(b):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "drop_add_residual_stochastic_depth_list",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "def drop_add_residual_stochastic_depth_list(\n    x_list: List[Tensor],\n    residual_func: Callable[[Tensor, Any], Tensor],\n    sample_drop_ratio: float = 0.0,\n    scaling_vector=None,\n) -> Tensor:\n    # 1) generate random set of indices for dropping samples in the batch\n    branges_scales = [get_branges_scales(x, sample_drop_ratio=sample_drop_ratio) for x in x_list]\n    branges = [s[0] for s in branges_scales]\n    residual_scale_factors = [s[1] for s in branges_scales]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nXFORMERS_ENABLED = os.environ.get(\"XFORMERS_DISABLED\") is None\ntry:\n    if XFORMERS_ENABLED:\n        from xformers.ops import fmha, scaled_index_add, index_select_cat\n        XFORMERS_AVAILABLE = True\n        warnings.warn(\"xFormers is available (Block)\")\n    else:\n        warnings.warn(\"xFormers is disabled (Block)\")\n        raise ImportError",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "XFORMERS_ENABLED",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "XFORMERS_ENABLED = os.environ.get(\"XFORMERS_DISABLED\") is None\ntry:\n    if XFORMERS_ENABLED:\n        from xformers.ops import fmha, scaled_index_add, index_select_cat\n        XFORMERS_AVAILABLE = True\n        warnings.warn(\"xFormers is available (Block)\")\n    else:\n        warnings.warn(\"xFormers is disabled (Block)\")\n        raise ImportError\nexcept ImportError:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "DINOHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.dino_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.dino_head",
        "peekOfCode": "class DINOHead(nn.Module):\n    def __init__(\n        self,\n        in_dim,\n        out_dim,\n        use_bn=False,\n        nlayers=3,\n        hidden_dim=2048,\n        bottleneck_dim=256,\n        mlp_bias=True,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.dino_head",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0:\n        random_tensor.div_(keep_prob)\n    output = x * random_tensor\n    return output",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "documentation": {}
    },
    {
        "label": "LayerScale",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.layer_scale",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.layer_scale",
        "peekOfCode": "class LayerScale(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        init_values: Union[float, Tensor] = 1e-5,\n        inplace: bool = False,\n    ) -> None:\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.layer_scale",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.mlp",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.mlp",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.mlp",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"\n    2D image to patch embedding: (B,C,H,W) -> (B,N,D)\n    Args:\n        img_size: Image size.\n        patch_size: Patch token size.\n        in_chans: Number of input image channels.\n        embed_dim: Number of linear projection output channels.\n        norm_layer: Normalization layer.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "documentation": {}
    },
    {
        "label": "make_2tuple",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "peekOfCode": "def make_2tuple(x):\n    if isinstance(x, tuple):\n        assert len(x) == 2\n        return x\n    assert isinstance(x, int)\n    return (x, x)\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    2D image to patch embedding: (B,C,H,W) -> (B,N,D)\n    Args:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFN",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "peekOfCode": "class SwiGLUFFN(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = None,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFNFused",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "peekOfCode": "class SwiGLUFFNFused(SwiGLU):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = None,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "documentation": {}
    },
    {
        "label": "XFORMERS_ENABLED",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "peekOfCode": "XFORMERS_ENABLED = os.environ.get(\"XFORMERS_DISABLED\") is None\ntry:\n    if XFORMERS_ENABLED:\n        from xformers.ops import SwiGLU\n        XFORMERS_AVAILABLE = True\n        warnings.warn(\"xFormers is available (SwiGLU)\")\n    else:\n        warnings.warn(\"xFormers is disabled (SwiGLU)\")\n        raise ImportError\nexcept ImportError:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "peekOfCode": "class MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\", output_file=None):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n        self.output_file = output_file\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "documentation": {}
    },
    {
        "label": "SmoothedValue",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "peekOfCode": "class SmoothedValue:\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\", output_file=None):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n        self.output_file = output_file\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "documentation": {}
    },
    {
        "label": "DINOLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.dino_clstoken_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.dino_clstoken_loss",
        "peekOfCode": "class DINOLoss(nn.Module):\n    def __init__(\n        self,\n        out_dim,\n        student_temp=0.1,\n        center_momentum=0.9,\n    ):\n        super().__init__()\n        self.student_temp = student_temp\n        self.center_momentum = center_momentum",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.dino_clstoken_loss",
        "documentation": {}
    },
    {
        "label": "iBOTPatchLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "peekOfCode": "class iBOTPatchLoss(nn.Module):\n    def __init__(self, patch_out_dim, student_temp=0.1, center_momentum=0.9):\n        super().__init__()\n        self.student_temp = student_temp\n        self.center_momentum = center_momentum\n        self.register_buffer(\"center\", torch.zeros(1, 1, patch_out_dim))\n        self.updated = True\n        self.reduce_handle = None\n        self.len_teacher_patch_tokens = None\n        self.async_batch_center = None",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ntry:\n    from xformers.ops import cross_entropy\n    def lossfunc(t, s, temp):\n        s = s.float()\n        t = t.float()\n        if s.ndim == 2:\n            return -cross_entropy(s.unsqueeze(0), t.unsqueeze(0), temp, bw_inplace=True).squeeze(0)\n        elif s.ndim == 3:\n            return -cross_entropy(s, t, temp, bw_inplace=True)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "documentation": {}
    },
    {
        "label": "KoLeoLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "peekOfCode": "class KoLeoLoss(nn.Module):\n    \"\"\"Kozachenko-Leonenko entropic loss regularizer from Sablayrolles et al. - 2018 - Spreading vectors for similarity search\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.pdist = nn.PairwiseDistance(2, eps=1e-8)\n    def pairwise_NNs_inner(self, x):\n        \"\"\"\n        Pairwise nearest neighbors for L2-normalized vectors.\n        Uses Torch rather than Faiss to remain on GPU.\n        \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass KoLeoLoss(nn.Module):\n    \"\"\"Kozachenko-Leonenko entropic loss regularizer from Sablayrolles et al. - 2018 - Spreading vectors for similarity search\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.pdist = nn.PairwiseDistance(2, eps=1e-8)\n    def pairwise_NNs_inner(self, x):\n        \"\"\"\n        Pairwise nearest neighbors for L2-normalized vectors.\n        Uses Torch rather than Faiss to remain on GPU.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "documentation": {}
    },
    {
        "label": "BlockChunk",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "class BlockChunk(nn.ModuleList):\n    def forward(self, x):\n        for b in self:\n            x = b(x)\n        return x\nclass DinoVisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "DinoVisionTransformer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "class DinoVisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "named_apply",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for child_name, child_module in module.named_children():\n        child_name = \".\".join((name, child_name)) if name else child_name\n        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        fn(module=module, name=name)\n    return module\nclass BlockChunk(nn.ModuleList):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "init_weights_vit_timm",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def init_weights_vit_timm(module: nn.Module, name: str = \"\"):\n    \"\"\"ViT weight initialization, original timm impl (for reproducibility)\"\"\"\n    if isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=0.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\ndef vit_small(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=384,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_small",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def vit_small(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_base",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def vit_base(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_large",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def vit_large(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_giant2",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def vit_giant2(patch_size=16, num_register_tokens=0, **kwargs):\n    \"\"\"\n    Close to ViT-giant, with embed-dim 1536 and 24 heads => embed-dim per head 64\n    \"\"\"\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1536,\n        depth=40,\n        num_heads=24,\n        mlp_ratio=4,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for child_name, child_module in module.named_children():\n        child_name = \".\".join((name, child_name)) if name else child_name\n        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        fn(module=module, name=name)\n    return module",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "Evaluator",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "peekOfCode": "class Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.knn import main as knn_main\n        self._setup_args()\n        knn_main(self.args)\n    def checkpoint(self):\n        import submitit\n        logger.info(f\"Requeuing {self.args}\")",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "peekOfCode": "def main():\n    description = \"Submitit launcher for DINOv2 k-NN evaluation\"\n    knn_args_parser = get_knn_args_parser(add_help=False)\n    parents = [knn_args_parser]\n    args_parser = get_args_parser(description=description, parents=parents)\n    args = args_parser.parse_args()\n    setup_logging()\n    assert os.path.exists(args.config_file), \"Configuration file does not exist!\"\n    submit_jobs(Evaluator, args, name=\"dinov2:knn\")\n    return 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.knn import main as knn_main\n        self._setup_args()\n        knn_main(self.args)\n    def checkpoint(self):\n        import submitit",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "documentation": {}
    },
    {
        "label": "Evaluator",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "peekOfCode": "class Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.linear import main as linear_main\n        self._setup_args()\n        linear_main(self.args)\n    def checkpoint(self):\n        import submitit\n        logger.info(f\"Requeuing {self.args}\")",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "peekOfCode": "def main():\n    description = \"Submitit launcher for DINOv2 linear evaluation\"\n    linear_args_parser = get_linear_args_parser(add_help=False)\n    parents = [linear_args_parser]\n    args_parser = get_args_parser(description=description, parents=parents)\n    args = args_parser.parse_args()\n    setup_logging()\n    assert os.path.exists(args.config_file), \"Configuration file does not exist!\"\n    submit_jobs(Evaluator, args, name=\"dinov2:linear\")\n    return 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.linear import main as linear_main\n        self._setup_args()\n        linear_main(self.args)\n    def checkpoint(self):\n        import submitit",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "documentation": {}
    },
    {
        "label": "Evaluator",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "peekOfCode": "class Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.log_regression import main as log_regression_main\n        self._setup_args()\n        log_regression_main(self.args)\n    def checkpoint(self):\n        import submitit\n        logger.info(f\"Requeuing {self.args}\")",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "peekOfCode": "def main():\n    description = \"Submitit launcher for DINOv2 logistic evaluation\"\n    log_regression_args_parser = get_log_regression_args_parser(add_help=False)\n    parents = [log_regression_args_parser]\n    args_parser = get_args_parser(description=description, parents=parents)\n    args = args_parser.parse_args()\n    setup_logging()\n    assert os.path.exists(args.config_file), \"Configuration file does not exist!\"\n    submit_jobs(Evaluator, args, name=\"dinov2:logreg\")\n    return 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.log_regression import main as log_regression_main\n        self._setup_args()\n        log_regression_main(self.args)\n    def checkpoint(self):\n        import submitit",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "peekOfCode": "class Trainer(object):\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.train import main as train_main\n        self._setup_args()\n        train_main(self.args)\n    def checkpoint(self):\n        import submitit\n        logger.info(f\"Requeuing {self.args}\")",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "peekOfCode": "def main():\n    description = \"Submitit launcher for DINOv2 training\"\n    train_args_parser = get_train_args_parser(add_help=False)\n    parents = [train_args_parser]\n    args_parser = get_args_parser(description=description, parents=parents)\n    args = args_parser.parse_args()\n    setup_logging()\n    assert os.path.exists(args.config_file), \"Configuration file does not exist!\"\n    submit_jobs(Trainer, args, name=\"dinov2:train\")\n    return 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.train import main as train_main\n        self._setup_args()\n        train_main(self.args)\n    def checkpoint(self):\n        import submitit",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "peekOfCode": "def get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n) -> argparse.ArgumentParser:\n    parents = parents or []\n    slurm_partition = get_slurm_partition()\n    parser = argparse.ArgumentParser(\n        description=description,\n        parents=parents,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "get_shared_folder",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "peekOfCode": "def get_shared_folder() -> Path:\n    user_checkpoint_path = get_user_checkpoint_path()\n    if user_checkpoint_path is None:\n        raise RuntimeError(\"Path to user checkpoint cannot be determined\")\n    path = user_checkpoint_path / \"experiments\"\n    path.mkdir(exist_ok=True)\n    return path\ndef submit_jobs(task_class, args, name: str):\n    if not args.output_dir:\n        args.output_dir = str(get_shared_folder() / \"%j\")",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "submit_jobs",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "peekOfCode": "def submit_jobs(task_class, args, name: str):\n    if not args.output_dir:\n        args.output_dir = str(get_shared_folder() / \"%j\")\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n    executor = submitit.AutoExecutor(folder=args.output_dir, slurm_max_num_timeout=30)\n    kwargs = {}\n    if args.use_volta32:\n        kwargs[\"slurm_constraint\"] = \"volta32gb\"\n    if args.comment:\n        kwargs[\"slurm_comment\"] = args.comment",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n) -> argparse.ArgumentParser:\n    parents = parents or []\n    slurm_partition = get_slurm_partition()\n    parser = argparse.ArgumentParser(\n        description=description,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "SSLMetaArch",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "peekOfCode": "class SSLMetaArch(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.fp16_scaler = ShardedGradScaler() if cfg.compute_precision.grad_scaler else None\n        student_model_dict = dict()\n        teacher_model_dict = dict()\n        student_backbone, teacher_backbone, embed_dim = build_model_from_cfg(cfg)\n        student_model_dict[\"backbone\"] = student_backbone\n        teacher_model_dict[\"backbone\"] = teacher_backbone",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass SSLMetaArch(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.fp16_scaler = ShardedGradScaler() if cfg.compute_precision.grad_scaler else None\n        student_model_dict = dict()\n        teacher_model_dict = dict()\n        student_backbone, teacher_backbone, embed_dim = build_model_from_cfg(cfg)\n        student_model_dict[\"backbone\"] = student_backbone",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def get_args_parser(add_help: bool = True):\n    parser = argparse.ArgumentParser(\"DINOv2 training\", add_help=add_help)\n    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--no-resume\",\n        action=\"store_true\",\n        help=\"Whether to not attempt to resume from the checkpoint directory. \",\n    )\n    parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"perform evaluation only\")\n    parser.add_argument(\"--eval\", type=str, default=\"\", help=\"Eval type to perform\")",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "build_optimizer",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def build_optimizer(cfg, params_groups):\n    return torch.optim.AdamW(params_groups, betas=(cfg.optim.adamw_beta1, cfg.optim.adamw_beta2))\ndef build_schedulers(cfg):\n    OFFICIAL_EPOCH_LENGTH = cfg.train.OFFICIAL_EPOCH_LENGTH\n    lr = dict(\n        base_value=cfg.optim[\"lr\"],\n        final_value=cfg.optim[\"min_lr\"],\n        total_iters=cfg.optim[\"epochs\"] * OFFICIAL_EPOCH_LENGTH,\n        warmup_iters=cfg.optim[\"warmup_epochs\"] * OFFICIAL_EPOCH_LENGTH,\n        start_warmup_value=0,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "build_schedulers",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def build_schedulers(cfg):\n    OFFICIAL_EPOCH_LENGTH = cfg.train.OFFICIAL_EPOCH_LENGTH\n    lr = dict(\n        base_value=cfg.optim[\"lr\"],\n        final_value=cfg.optim[\"min_lr\"],\n        total_iters=cfg.optim[\"epochs\"] * OFFICIAL_EPOCH_LENGTH,\n        warmup_iters=cfg.optim[\"warmup_epochs\"] * OFFICIAL_EPOCH_LENGTH,\n        start_warmup_value=0,\n    )\n    wd = dict(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "apply_optim_scheduler",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def apply_optim_scheduler(optimizer, lr, wd, last_layer_lr):\n    for param_group in optimizer.param_groups:\n        is_last_layer = param_group[\"is_last_layer\"]\n        lr_multiplier = param_group[\"lr_multiplier\"]\n        wd_multiplier = param_group[\"wd_multiplier\"]\n        param_group[\"weight_decay\"] = wd * wd_multiplier\n        param_group[\"lr\"] = (last_layer_lr if is_last_layer else lr) * lr_multiplier\ndef do_test(cfg, model, iteration):\n    new_state_dict = model.teacher.state_dict()\n    if distributed.is_main_process():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "do_test",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def do_test(cfg, model, iteration):\n    new_state_dict = model.teacher.state_dict()\n    if distributed.is_main_process():\n        iterstring = str(iteration)\n        eval_dir = os.path.join(cfg.train.output_dir, \"eval\", iterstring)\n        os.makedirs(eval_dir, exist_ok=True)\n        # save teacher checkpoint\n        teacher_ckp_path = os.path.join(eval_dir, \"teacher_checkpoint.pth\")\n        torch.save({\"teacher\": new_state_dict}, teacher_ckp_path)\ndef do_train(cfg, model, resume=False):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "do_train",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def do_train(cfg, model, resume=False):\n    model.train()\n    inputs_dtype = torch.half\n    fp16_scaler = model.fp16_scaler  # for mixed precision training\n    # setup optimizer\n    optimizer = build_optimizer(cfg, model.get_params_groups())\n    (\n        lr_schedule,\n        wd_schedule,\n        momentum_schedule,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def main(args):\n    cfg = setup(args)\n    model = SSLMetaArch(cfg).to(torch.device(\"cuda\"))\n    model.prepare_for_distributed_training()\n    logger.info(\"Model:\\n{}\".format(model))\n    if args.eval_only:\n        iteration = (\n            FSDPCheckpointer(model, save_dir=cfg.train.output_dir)\n            .resume_or_load(cfg.MODEL.WEIGHTS, resume=not args.no_resume)\n            .get(\"iteration\", -1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True  # PyTorch 1.12 sets this to False by default\nlogger = logging.getLogger(\"dinov2\")\ndef get_args_parser(add_help: bool = True):\n    parser = argparse.ArgumentParser(\"DINOv2 training\", add_help=add_help)\n    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--no-resume\",\n        action=\"store_true\",\n        help=\"Whether to not attempt to resume from the checkpoint directory. \",\n    )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef get_args_parser(add_help: bool = True):\n    parser = argparse.ArgumentParser(\"DINOv2 training\", add_help=add_help)\n    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--no-resume\",\n        action=\"store_true\",\n        help=\"Whether to not attempt to resume from the checkpoint directory. \",\n    )\n    parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"perform evaluation only\")",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "ClusterType",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "class ClusterType(Enum):\n    AWS = \"aws\"\n    FAIR = \"fair\"\n    RSC = \"rsc\"\ndef _guess_cluster_type() -> ClusterType:\n    uname = os.uname()\n    if uname.sysname == \"Linux\":\n        if uname.release.endswith(\"-aws\"):\n            # Linux kernel versions on AWS instances are of the form \"5.4.0-1051-aws\"\n            return ClusterType.AWS",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_cluster_type",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "def get_cluster_type(cluster_type: Optional[ClusterType] = None) -> Optional[ClusterType]:\n    if cluster_type is None:\n        return _guess_cluster_type()\n    return cluster_type\ndef get_checkpoint_path(cluster_type: Optional[ClusterType] = None) -> Optional[Path]:\n    cluster_type = get_cluster_type(cluster_type)\n    if cluster_type is None:\n        return None\n    CHECKPOINT_DIRNAMES = {\n        ClusterType.AWS: \"checkpoints\",",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_checkpoint_path",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "def get_checkpoint_path(cluster_type: Optional[ClusterType] = None) -> Optional[Path]:\n    cluster_type = get_cluster_type(cluster_type)\n    if cluster_type is None:\n        return None\n    CHECKPOINT_DIRNAMES = {\n        ClusterType.AWS: \"checkpoints\",\n        ClusterType.FAIR: \"checkpoint\",\n        ClusterType.RSC: \"checkpoint/dino\",\n    }\n    return Path(\"/\") / CHECKPOINT_DIRNAMES[cluster_type]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_user_checkpoint_path",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "def get_user_checkpoint_path(cluster_type: Optional[ClusterType] = None) -> Optional[Path]:\n    checkpoint_path = get_checkpoint_path(cluster_type)\n    if checkpoint_path is None:\n        return None\n    username = os.environ.get(\"USER\")\n    assert username is not None\n    return checkpoint_path / username\ndef get_slurm_partition(cluster_type: Optional[ClusterType] = None) -> Optional[str]:\n    cluster_type = get_cluster_type(cluster_type)\n    if cluster_type is None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_slurm_partition",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "def get_slurm_partition(cluster_type: Optional[ClusterType] = None) -> Optional[str]:\n    cluster_type = get_cluster_type(cluster_type)\n    if cluster_type is None:\n        return None\n    SLURM_PARTITIONS = {\n        ClusterType.AWS: \"learnlab\",\n        ClusterType.FAIR: \"learnlab\",\n        ClusterType.RSC: \"learn\",\n    }\n    return SLURM_PARTITIONS[cluster_type]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_slurm_executor_parameters",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "def get_slurm_executor_parameters(\n    nodes: int, num_gpus_per_node: int, cluster_type: Optional[ClusterType] = None, **kwargs\n) -> Dict[str, Any]:\n    # create default parameters\n    params = {\n        \"mem_gb\": 0,  # Requests all memory on a node, see https://slurm.schedmd.com/sbatch.html\n        \"gpus_per_node\": num_gpus_per_node,\n        \"tasks_per_node\": num_gpus_per_node,  # one task per GPU\n        \"cpus_per_task\": 10,\n        \"nodes\": nodes,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "apply_scaling_rules_to_cfg",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "def apply_scaling_rules_to_cfg(cfg):  # to fix\n    if cfg.optim.scaling_rule == \"sqrt_wrt_1024\":\n        base_lr = cfg.optim.base_lr\n        cfg.optim.lr = base_lr\n        cfg.optim.lr *= math.sqrt(cfg.train.batch_size_per_gpu * distributed.get_global_size() / 1024.0)\n        logger.info(f\"sqrt scaling learning rate; base: {base_lr}, new: {cfg.optim.lr}\")\n    else:\n        raise NotImplementedError\n    return cfg\ndef write_config(cfg, output_dir, name=\"config.yaml\"):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "write_config",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "def write_config(cfg, output_dir, name=\"config.yaml\"):\n    logger.info(OmegaConf.to_yaml(cfg))\n    saved_cfg_path = os.path.join(output_dir, name)\n    with open(saved_cfg_path, \"w\") as f:\n        OmegaConf.save(config=cfg, f=f)\n    return saved_cfg_path\ndef get_cfg_from_args(args):\n    args.output_dir = os.path.abspath(args.output_dir)\n    args.opts += [f\"train.output_dir={args.output_dir}\"]\n    default_cfg = OmegaConf.create(dinov2_default_config)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "get_cfg_from_args",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "def get_cfg_from_args(args):\n    args.output_dir = os.path.abspath(args.output_dir)\n    args.opts += [f\"train.output_dir={args.output_dir}\"]\n    default_cfg = OmegaConf.create(dinov2_default_config)\n    cfg = OmegaConf.load(args.config_file)\n    cfg = OmegaConf.merge(default_cfg, cfg, OmegaConf.from_cli(args.opts))\n    return cfg\ndef default_setup(args):\n    distributed.enable(overwrite=True)\n    seed = getattr(args, \"seed\", 0)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "default_setup",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "def default_setup(args):\n    distributed.enable(overwrite=True)\n    seed = getattr(args, \"seed\", 0)\n    rank = distributed.get_global_rank()\n    global logger\n    setup_logging(output=args.output_dir, level=logging.INFO)\n    logger = logging.getLogger(\"dinov2\")\n    utils.fix_random_seeds(seed + rank)\n    logger.info(\"git:\\n  {}\\n\".format(utils.get_sha()))\n    logger.info(\"\\n\".join(\"%s: %s\" % (k, str(v)) for k, v in sorted(dict(vars(args)).items())))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "setup",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "def setup(args):\n    \"\"\"\n    Create configs and perform basic setups.\n    \"\"\"\n    cfg = get_cfg_from_args(args)\n    os.makedirs(args.output_dir, exist_ok=True)\n    default_setup(args)\n    apply_scaling_rules_to_cfg(cfg)\n    write_config(cfg, args.output_dir)\n    return cfg",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef apply_scaling_rules_to_cfg(cfg):  # to fix\n    if cfg.optim.scaling_rule == \"sqrt_wrt_1024\":\n        base_lr = cfg.optim.base_lr\n        cfg.optim.lr = base_lr\n        cfg.optim.lr *= math.sqrt(cfg.train.batch_size_per_gpu * distributed.get_global_size() / 1024.0)\n        logger.info(f\"sqrt scaling learning rate; base: {base_lr}, new: {cfg.optim.lr}\")\n    else:\n        raise NotImplementedError\n    return cfg",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "as_torch_dtype",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "peekOfCode": "def as_torch_dtype(dtype: TypeSpec) -> torch.dtype:\n    if isinstance(dtype, torch.dtype):\n        return dtype\n    if isinstance(dtype, str):\n        dtype = np.dtype(dtype)\n    assert isinstance(dtype, np.dtype), f\"Expected an instance of nunpy dtype, got {type(dtype)}\"\n    return _NUMPY_TO_TORCH_DTYPE[dtype]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "documentation": {}
    },
    {
        "label": "TypeSpec",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "peekOfCode": "TypeSpec = Union[str, np.dtype, torch.dtype]\n_NUMPY_TO_TORCH_DTYPE: Dict[np.dtype, torch.dtype] = {\n    np.dtype(\"bool\"): torch.bool,\n    np.dtype(\"uint8\"): torch.uint8,\n    np.dtype(\"int8\"): torch.int8,\n    np.dtype(\"int16\"): torch.int16,\n    np.dtype(\"int32\"): torch.int32,\n    np.dtype(\"int64\"): torch.int64,\n    np.dtype(\"float16\"): torch.float16,\n    np.dtype(\"float32\"): torch.float32,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "documentation": {}
    },
    {
        "label": "get_vit_lr_decay_rate",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "peekOfCode": "def get_vit_lr_decay_rate(name, lr_decay_rate=1.0, num_layers=12, force_is_backbone=False, chunked_blocks=False):\n    \"\"\"\n    Calculate lr decay rate for different ViT blocks.\n    Args:\n        name (string): parameter name.\n        lr_decay_rate (float): base lr decay rate.\n        num_layers (int): number of ViT blocks.\n    Returns:\n        lr decay rate for the given parameter.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "get_params_groups_with_decay",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "peekOfCode": "def get_params_groups_with_decay(model, lr_decay_rate=1.0, patch_embed_lr_mult=1.0):\n    chunked_blocks = False\n    if hasattr(model, \"n_blocks\"):\n        logger.info(\"chunked fsdp\")\n        n_blocks = model.n_blocks\n        chunked_blocks = model.chunked_blocks\n    elif hasattr(model, \"blocks\"):\n        logger.info(\"first code branch\")\n        n_blocks = len(model.blocks)\n    elif hasattr(model, \"backbone\"):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "fuse_params_groups",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "peekOfCode": "def fuse_params_groups(all_params_groups, keys=(\"lr_multiplier\", \"wd_multiplier\", \"is_last_layer\")):\n    fused_params_groups = defaultdict(lambda: {\"params\": []})\n    for d in all_params_groups:\n        identifier = \"\"\n        for k in keys:\n            identifier += k + str(d[k]) + \"_\"\n        for k in keys:\n            fused_params_groups[identifier][k] = d[k]\n        fused_params_groups[identifier][\"params\"].append(d[\"params\"])\n    return fused_params_groups.values()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef get_vit_lr_decay_rate(name, lr_decay_rate=1.0, num_layers=12, force_is_backbone=False, chunked_blocks=False):\n    \"\"\"\n    Calculate lr decay rate for different ViT blocks.\n    Args:\n        name (string): parameter name.\n        lr_decay_rate (float): base lr decay rate.\n        num_layers (int): number of ViT blocks.\n    Returns:\n        lr decay rate for the given parameter.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "CosineScheduler",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "class CosineScheduler(object):\n    def __init__(self, base_value, final_value, total_iters, warmup_iters=0, start_warmup_value=0, freeze_iters=0):\n        super().__init__()\n        self.final_value = final_value\n        self.total_iters = total_iters\n        freeze_schedule = np.zeros((freeze_iters))\n        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n        iters = np.arange(total_iters - warmup_iters - freeze_iters)\n        schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n        self.schedule = np.concatenate((freeze_schedule, warmup_schedule, schedule))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "load_pretrained_weights",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "def load_pretrained_weights(model, pretrained_weights, checkpoint_key):\n    if urlparse(pretrained_weights).scheme:  # If it looks like an URL\n        state_dict = torch.hub.load_state_dict_from_url(pretrained_weights, map_location=\"cpu\")\n    else:\n        state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n    if checkpoint_key is not None and checkpoint_key in state_dict:\n        logger.info(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n        state_dict = state_dict[checkpoint_key]\n    # remove `module.` prefix\n    state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "fix_random_seeds",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "def fix_random_seeds(seed=31):\n    \"\"\"\n    Fix random seeds.\n    \"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\ndef get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "get_sha",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "def get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))\n    def _run(command):\n        return subprocess.check_output(command, cwd=cwd).decode(\"ascii\").strip()\n    sha = \"N/A\"\n    diff = \"clean\"\n    branch = \"N/A\"\n    try:\n        sha = _run([\"git\", \"rev-parse\", \"HEAD\"])\n        subprocess.check_output([\"git\", \"diff\"], cwd=cwd)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "has_batchnorms",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "def has_batchnorms(model):\n    bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm)\n    for name, module in model.named_modules():\n        if isinstance(module, bn_types):\n            return True\n    return False",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef load_pretrained_weights(model, pretrained_weights, checkpoint_key):\n    if urlparse(pretrained_weights).scheme:  # If it looks like an URL\n        state_dict = torch.hub.load_state_dict_from_url(pretrained_weights, map_location=\"cpu\")\n    else:\n        state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n    if checkpoint_key is not None and checkpoint_key in state_dict:\n        logger.info(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n        state_dict = state_dict[checkpoint_key]\n    # remove `module.` prefix",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "set_logging_format",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def set_logging_format(level=logging.INFO):\n  importlib.reload(logging)\n  FORMAT = '%(message)s'\n  logging.basicConfig(level=level, format=FORMAT, datefmt='%m-%d|%H:%M:%S')\nset_logging_format()\ndef set_seed(random_seed):\n  import torch,random\n  np.random.seed(random_seed)\n  random.seed(random_seed)\n  torch.manual_seed(random_seed)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def set_seed(random_seed):\n  import torch,random\n  np.random.seed(random_seed)\n  random.seed(random_seed)\n  torch.manual_seed(random_seed)\n  torch.cuda.manual_seed_all(random_seed)\n  torch.backends.cudnn.deterministic = True\n  torch.backends.cudnn.benchmark = False\ndef toOpen3dCloud(points,colors=None,normals=None):\n  cloud = o3d.geometry.PointCloud()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "toOpen3dCloud",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def toOpen3dCloud(points,colors=None,normals=None):\n  cloud = o3d.geometry.PointCloud()\n  cloud.points = o3d.utility.Vector3dVector(points.astype(np.float64))\n  if colors is not None:\n    if colors.max()>1:\n      colors = colors/255.0\n    cloud.colors = o3d.utility.Vector3dVector(colors.astype(np.float64))\n  if normals is not None:\n    cloud.normals = o3d.utility.Vector3dVector(normals.astype(np.float64))\n  return cloud",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "depth2xyzmap",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def depth2xyzmap(depth:np.ndarray, K, uvs:np.ndarray=None, zmin=0.1):\n  invalid_mask = (depth<zmin)\n  H,W = depth.shape[:2]\n  if uvs is None:\n    vs,us = np.meshgrid(np.arange(0,H),np.arange(0,W), sparse=False, indexing='ij')\n    vs = vs.reshape(-1)\n    us = us.reshape(-1)\n  else:\n    uvs = uvs.round().astype(int)\n    us = uvs[:,0]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "freeze_model",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def freeze_model(model):\n  model = model.eval()\n  for p in model.parameters():\n    p.requires_grad = False\n  for p in model.buffers():\n    p.requires_grad = False\n  return model\ndef get_resize_keep_aspect_ratio(H, W, divider=16, max_H=1232, max_W=1232):\n  assert max_H%divider==0\n  assert max_W%divider==0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "get_resize_keep_aspect_ratio",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def get_resize_keep_aspect_ratio(H, W, divider=16, max_H=1232, max_W=1232):\n  assert max_H%divider==0\n  assert max_W%divider==0\n  def round_by_divider(x):\n    return int(np.ceil(x/divider)*divider)\n  H_resize = round_by_divider(H)   #!NOTE KITTI width=1242\n  W_resize = round_by_divider(W)\n  if H_resize>max_H or W_resize>max_W:\n    if H_resize>W_resize:\n      W_resize = round_by_divider(W_resize*max_H/H_resize)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "vis_disparity",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def vis_disparity(disp, min_val=None, max_val=None, invalid_thres=np.inf, color_map=cv2.COLORMAP_TURBO, cmap=None, other_output={}):\n  \"\"\"\n  @disp: np array (H,W)\n  @invalid_thres: > thres is invalid\n  \"\"\"\n  disp = disp.copy()\n  H,W = disp.shape[:2]\n  invalid_mask = disp>=invalid_thres\n  if (invalid_mask==0).sum()==0:\n    other_output['min_val'] = None",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "depth_uint8_decoding",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def depth_uint8_decoding(depth_uint8, scale=1000):\n  depth_uint8 = depth_uint8.astype(float)\n  out = depth_uint8[...,0]*255*255 + depth_uint8[...,1]*255 + depth_uint8[...,2]\n  return out/float(scale)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(code_dir)\ndef set_logging_format(level=logging.INFO):\n  importlib.reload(logging)\n  FORMAT = '%(message)s'\n  logging.basicConfig(level=level, format=FORMAT, datefmt='%m-%d|%H:%M:%S')\nset_logging_format()\ndef set_seed(random_seed):\n  import torch,random\n  np.random.seed(random_seed)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.trainer",
        "peekOfCode": "__all__ = {\n    'FoundationStereo': FoundationStereo,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.foundationstereo.trainer",
        "documentation": {}
    },
    {
        "label": "GwcNet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet",
        "peekOfCode": "class GwcNet(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        use_concat_volume = cfgs.USE_CONCAT_VOLUME\n        concat_channels = cfgs.CONCAT_CHANNELS\n        downsample = cfgs.DOWNSAMPLE\n        num_groups = cfgs.NUM_GROUPS\n        self.Backbone = GwcNetBackbone(use_concat_volume=use_concat_volume, concat_channels=concat_channels)\n        self.CostProcessor = GwcVolumeCostProcessor(maxdisp=self.maxdisp, downsample=downsample, num_groups=num_groups,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self, concat_feature=False, concat_feature_channel=12):\n        super(feature_extraction, self).__init__()\n        self.concat_feature = concat_feature\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn(3, 32, 3, 2, 1, 1),\n                                       nn.ReLU(inplace=True),\n                                       convbn(32, 32, 3, 1, 1, 1),\n                                       nn.ReLU(inplace=True),\n                                       convbn(32, 32, 3, 1, 1, 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "documentation": {}
    },
    {
        "label": "GwcNet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "peekOfCode": "class GwcNet(nn.Module):\n    def __init__(self, use_concat_volume=True, concat_channels=12):\n        super().__init__()\n        self.use_concat_volume = use_concat_volume\n        self.concat_channels = concat_channels if use_concat_volume else 0\n        self.feature_extraction = feature_extraction(self.use_concat_volume, self.concat_channels)\n    def forward(self, inputs):\n        ref_img = inputs[\"left\"]\n        tgt_img = inputs[\"right\"]\n        ref_feature = self.feature_extraction(ref_img)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "peekOfCode": "def convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_channels))\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_backbone",
        "documentation": {}
    },
    {
        "label": "GwcVolumeCostProcessor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_cost_processor",
        "peekOfCode": "class GwcVolumeCostProcessor(nn.Module):\n    def __init__(self, maxdisp=192, downsample=4, num_groups=40, use_concat_volume=True, *args, **kwargs):\n        super().__init__()\n        self.maxdisp = maxdisp\n        self.downsample = downsample\n        self.num_groups = num_groups\n        self.use_concat_volume = use_concat_volume\n    def groupwise_correlation(self, fea1, fea2):\n        B, C, H, W = fea1.shape\n        num_groups = self.num_groups",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "GwcDispProcessor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "peekOfCode": "class GwcDispProcessor(nn.Module):\n    def __init__(self, maxdisp=192, downsample=4, num_groups=40, use_concat_volume=True, concat_channels=12, *args,\n                 **kwargs):\n        super().__init__()\n        self.maxdisp = maxdisp\n        self.downsample = downsample\n        self.num_groups = num_groups\n        self.use_concat_volume = use_concat_volume\n        self.concat_channels = concat_channels if use_concat_volume else 0\n        self.dres0 = nn.Sequential(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "peekOfCode": "def convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(\n        nn.Conv3d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=pad,\n            bias=False\n        ),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=False)\nclass GwcDispProcessor(nn.Module):\n    def __init__(self, maxdisp=192, downsample=4, num_groups=40, use_concat_volume=True, concat_channels=12, *args,\n                 **kwargs):\n        super().__init__()\n        self.maxdisp = maxdisp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "documentation": {}
    },
    {
        "label": "Hourglass",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.hourglass",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.hourglass",
        "peekOfCode": "class Hourglass(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.conv1 = nn.Sequential(convbn_3d(in_channels, in_channels * 2, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 2, 3, 1, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv3 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 4, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv4 = nn.Sequential(convbn_3d(in_channels * 4, in_channels * 4, 3, 1, 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.hourglass",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.hourglass",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.hourglass",
        "peekOfCode": "def convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(\n        nn.Conv3d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=pad,\n            bias=False\n        ),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.hourglass",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.trainer",
        "peekOfCode": "__all__ = {\n    'GwcNet': GwcNet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.gwcnet.trainer",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "BottleneckBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "peekOfCode": "class BottleneckBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(BottleneckBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes // 4, kernel_size=1, padding=0)\n        self.conv2 = nn.Conv2d(planes // 4, planes // 4, kernel_size=3, padding=1, stride=stride)\n        self.conv3 = nn.Conv2d(planes // 4, planes, kernel_size=1, padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes // 4)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "BasicEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "peekOfCode": "class BasicEncoder(nn.Module):\n    def __init__(self, output_dim=128, norm_fn='batch', dropout=0.0, downsample=3):\n        super(BasicEncoder, self).__init__()\n        self.norm_fn = norm_fn\n        self.downsample = downsample\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n        elif self.norm_fn == 'batch':\n            self.norm1 = nn.BatchNorm2d(64)\n        elif self.norm_fn == 'instance':",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "MultiBasicEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "peekOfCode": "class MultiBasicEncoder(nn.Module):\n    def __init__(self, output_dim=[128], norm_fn='batch', dropout=0.0, downsample=3):\n        super(MultiBasicEncoder, self).__init__()\n        self.norm_fn = norm_fn\n        self.downsample = downsample\n        # self.norm_111 = nn.BatchNorm2d(128, affine=False, track_running_stats=False)\n        # self.norm_222 = nn.BatchNorm2d(128, affine=False, track_running_stats=False)\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n        elif self.norm_fn == 'batch':",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "SubModule",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "peekOfCode": "class SubModule(nn.Module):\n    def __init__(self):\n        super(SubModule, self).__init__()\n    def weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "peekOfCode": "class Feature(SubModule):\n    def __init__(self):\n        super(Feature, self).__init__()\n        pretrained = True\n        model = timm.create_model('mobilenetv2_100', pretrained=pretrained, features_only=True)\n        layers = [1, 2, 3, 5, 6]\n        chans = [16, 24, 32, 96, 160]\n        self.conv_stem = model.conv_stem\n        self.bn1 = model.bn1\n        self.act1 = model.act1",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "Combined_Geo_Encoding_Volume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.geometry",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.geometry",
        "peekOfCode": "class Combined_Geo_Encoding_Volume:\n    def __init__(self, init_fmap1, init_fmap2, geo_volume, num_levels=2, radius=4):\n        self.num_levels = num_levels\n        self.radius = radius\n        self.geo_volume_pyramid = []\n        self.init_corr_pyramid = []\n        # all pairs correlation\n        init_corr = Combined_Geo_Encoding_Volume.corr(init_fmap1, init_fmap2)\n        b, h, w, _, w2 = init_corr.shape\n        b, c, d, h, w = geo_volume.shape",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.geometry",
        "documentation": {}
    },
    {
        "label": "hourglass",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.igev_stereo",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.igev_stereo",
        "peekOfCode": "class hourglass(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass, self).__init__()\n        self.conv1 = nn.Sequential(\n            BasicConv(in_channels, in_channels * 2, is_3d=True, bn=True, relu=True, kernel_size=3,\n                      padding=1, stride=2, dilation=1),\n            BasicConv(in_channels * 2, in_channels * 2, is_3d=True, bn=True, relu=True, kernel_size=3,\n                      padding=1, stride=1, dilation=1))\n        self.conv2 = nn.Sequential(\n            BasicConv(in_channels * 2, in_channels * 4, is_3d=True, bn=True, relu=True, kernel_size=3,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.igev_stereo",
        "documentation": {}
    },
    {
        "label": "IGEVStereo",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.igev_stereo",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.igev_stereo",
        "peekOfCode": "class IGEVStereo(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.max_disp = args.MAX_DISP\n        context_dims = args.HIDDEN_DIMS\n        self.cnet = MultiBasicEncoder(output_dim=[args.HIDDEN_DIMS, context_dims], norm_fn=\"batch\",\n                                      downsample=args.N_DOWNSAMPLE)\n        self.update_block = BasicMultiUpdateBlock(self.args, hidden_dims=args.HIDDEN_DIMS)\n        self.context_zqr_convs = nn.ModuleList(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.igev_stereo",
        "documentation": {}
    },
    {
        "label": "BasicConv",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "class BasicConv(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, bn=True, relu=True, **kwargs):\n        super(BasicConv, self).__init__()\n        self.relu = relu\n        self.use_bn = bn\n        if is_3d:\n            if deconv:\n                self.conv = nn.ConvTranspose3d(in_channels, out_channels, bias=False, **kwargs)\n            else:\n                self.conv = nn.Conv3d(in_channels, out_channels, bias=False, **kwargs)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "Conv2x",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "class Conv2x(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, concat=True, keep_concat=True, bn=True,\n                 relu=True, keep_dispc=False):\n        super(Conv2x, self).__init__()\n        self.concat = concat\n        self.is_3d = is_3d\n        if deconv and is_3d:\n            kernel = (4, 4, 4)\n        elif deconv:\n            kernel = 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv_IN",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "class BasicConv_IN(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, IN=True, relu=True, **kwargs):\n        super(BasicConv_IN, self).__init__()\n        self.relu = relu\n        self.use_in = IN\n        if is_3d:\n            if deconv:\n                self.conv = nn.ConvTranspose3d(in_channels, out_channels, bias=False, **kwargs)\n            else:\n                self.conv = nn.Conv3d(in_channels, out_channels, bias=False, **kwargs)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "Conv2x_IN",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "class Conv2x_IN(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, concat=True, keep_concat=True, IN=True,\n                 relu=True, keep_dispc=False):\n        super(Conv2x_IN, self).__init__()\n        self.concat = concat\n        self.is_3d = is_3d\n        if deconv and is_3d:\n            kernel = (4, 4, 4)\n        elif deconv:\n            kernel = 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "FeatureAtt",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "class FeatureAtt(nn.Module):\n    def __init__(self, cv_chan, feat_chan):\n        super(FeatureAtt, self).__init__()\n        self.feat_att = nn.Sequential(\n            BasicConv(feat_chan, feat_chan // 2, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(feat_chan // 2, cv_chan, 1))\n    def forward(self, cv, feat):\n        '''\n        '''\n        feat_att = self.feat_att(feat).unsqueeze(2)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "norm_correlation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "def norm_correlation(fea1, fea2):\n    cost = torch.mean(\n        ((fea1 / (torch.norm(fea1, 2, 1, True) + 1e-05)) * (fea2 / (torch.norm(fea2, 2, 1, True) + 1e-05))), dim=1,\n        keepdim=True)\n    return cost\ndef build_norm_correlation_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 1, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "build_norm_correlation_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "def build_norm_correlation_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 1, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = norm_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i])\n        else:\n            volume[:, :, i, :, :] = norm_correlation(refimg_fea, targetimg_fea)\n    volume = volume.contiguous()\n    return volume",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "correlation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "def correlation(fea1, fea2):\n    cost = torch.sum((fea1 * fea2), dim=1, keepdim=True)\n    return cost\ndef build_correlation_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 1, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i])\n        else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "build_correlation_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "def build_correlation_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 1, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i])\n        else:\n            volume[:, :, i, :, :] = correlation(refimg_fea, targetimg_fea)\n    volume = volume.contiguous()\n    return volume",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, :] = refimg_fea[:, :, :, :]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=True)\nclass FeatureAtt(nn.Module):\n    def __init__(self, cv_chan, feat_chan):\n        super(FeatureAtt, self).__init__()\n        self.feat_att = nn.Sequential(\n            BasicConv(feat_chan, feat_chan // 2, kernel_size=1, stride=1, padding=0),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "context_upsample",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "peekOfCode": "def context_upsample(disp_low, up_weights):\n    ###\n    # cv (b,1,h,w)\n    # sp (b,9,4*h,4*w)\n    ###\n    b, c, h, w = disp_low.shape\n    disp_unfold = F.unfold(disp_low.reshape(b, c, h, w), 3, 1, 1).reshape(b, -1, h, w)\n    disp_unfold = F.interpolate(disp_unfold, (h * 4, w * 4), mode='nearest').reshape(b, 9, h * 4, w * 4)\n    disp = (disp_unfold * up_weights).sum(1)\n    return disp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.trainer",
        "peekOfCode": "__all__ = {\n    'IGEV': IGEVStereo,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.trainer",
        "documentation": {}
    },
    {
        "label": "FlowHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "peekOfCode": "class FlowHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=2):\n        super(FlowHead, self).__init__()\n        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 3, padding=1)\n        self.conv2 = nn.Conv2d(hidden_dim, output_dim, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\nclass DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "DispHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "peekOfCode": "class DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):\n        super(DispHead, self).__init__()\n        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 3, padding=1)\n        self.conv2 = nn.Conv2d(hidden_dim, output_dim, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\nclass ConvGRU(nn.Module):\n    def __init__(self, hidden_dim, input_dim, kernel_size=3):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "ConvGRU",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "peekOfCode": "class ConvGRU(nn.Module):\n    def __init__(self, hidden_dim, input_dim, kernel_size=3):\n        super(ConvGRU, self).__init__()\n        self.convz = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convq = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n    def forward(self, h, cz, cr, cq, *x_list):\n        x = torch.cat(x_list, dim=1)\n        hx = torch.cat([h, x], dim=1)\n        z = torch.sigmoid(self.convz(hx) + cz)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "SepConvGRU",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "peekOfCode": "class SepConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=192 + 128):\n        super(SepConvGRU, self).__init__()\n        self.convz1 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (1, 5), padding=(0, 2))\n        self.convr1 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (1, 5), padding=(0, 2))\n        self.convq1 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (1, 5), padding=(0, 2))\n        self.convz2 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (5, 1), padding=(2, 0))\n        self.convr2 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (5, 1), padding=(2, 0))\n        self.convq2 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (5, 1), padding=(2, 0))\n    def forward(self, h, *x):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "BasicMotionEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "peekOfCode": "class BasicMotionEncoder(nn.Module):\n    def __init__(self, args):\n        super(BasicMotionEncoder, self).__init__()\n        self.args = args\n        cor_planes = args.CORR_LEVELS * (2 * args.CORR_RADIUS + 1) * (8 + 1)\n        self.convc1 = nn.Conv2d(cor_planes, 64, 1, padding=0)\n        self.convc2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.convd1 = nn.Conv2d(1, 64, 7, padding=3)\n        self.convd2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.conv = nn.Conv2d(64 + 64, 128 - 1, 3, padding=1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "BasicMultiUpdateBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "peekOfCode": "class BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, args, hidden_dims=[]):\n        super().__init__()\n        self.args = args\n        self.encoder = BasicMotionEncoder(args)\n        encoder_output_dim = 128\n        self.gru04 = ConvGRU(hidden_dims[2], encoder_output_dim + hidden_dims[1] * (args.N_GRU_LAYERS > 1))\n        self.gru08 = ConvGRU(hidden_dims[1], hidden_dims[0] * (args.N_GRU_LAYERS == 3) + hidden_dims[2])\n        self.gru16 = ConvGRU(hidden_dims[0], hidden_dims[1])\n        self.disp_head = DispHead(hidden_dims[2], hidden_dim=256, output_dim=1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "pool2x",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "peekOfCode": "def pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)\ndef pool4x(x):\n    return F.avg_pool2d(x, 5, stride=4, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, args, hidden_dims=[]):\n        super().__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "pool4x",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "peekOfCode": "def pool4x(x):\n    return F.avg_pool2d(x, 5, stride=4, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, args, hidden_dims=[]):\n        super().__init__()\n        self.args = args\n        self.encoder = BasicMotionEncoder(args)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "interp",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "peekOfCode": "def interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, args, hidden_dims=[]):\n        super().__init__()\n        self.args = args\n        self.encoder = BasicMotionEncoder(args)\n        encoder_output_dim = 128\n        self.gru04 = ConvGRU(hidden_dims[2], encoder_output_dim + hidden_dims[1] * (args.N_GRU_LAYERS > 1))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "InputPadder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "peekOfCode": "class InputPadder:\n    \"\"\" Pads images such that dimensions are divisible by 8 \"\"\"\n    def __init__(self, dims, mode='sintel', divis_by=8):\n        self.ht, self.wd = dims[-2:]\n        pad_ht = (((self.ht // divis_by) + 1) * divis_by - self.ht) % divis_by\n        pad_wd = (((self.wd // divis_by) + 1) * divis_by - self.wd) % divis_by\n        if mode == 'sintel':\n            self._pad = [pad_wd // 2, pad_wd - pad_wd // 2, pad_ht // 2, pad_ht - pad_ht // 2]\n        else:\n            self._pad = [pad_wd // 2, pad_wd - pad_wd // 2, 0, pad_ht]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "Map",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "peekOfCode": "class Map(dict):\n    \"\"\"\n    Example:\n    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(Map, self).__init__(*args, **kwargs)\n        for arg in args:\n            if isinstance(arg, dict):\n                for k, v in arg.items():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "forward_interpolate",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "peekOfCode": "def forward_interpolate(flow):\n    flow = flow.detach().cpu().numpy()\n    dx, dy = flow[0], flow[1]\n    ht, wd = dx.shape\n    x0, y0 = np.meshgrid(np.arange(wd), np.arange(ht))\n    x1 = x0 + dx\n    y1 = y0 + dy\n    x1 = x1.reshape(-1)\n    y1 = y1.reshape(-1)\n    dx = dx.reshape(-1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "peekOfCode": "def bilinear_sampler(img, coords, mode='bilinear', mask=False):\n    \"\"\" Wrapper for grid_sample, uses pixel coordinates \"\"\"\n    H, W = img.shape[-2:]\n    # print(\"$$$55555\", img.shape, coords.shape)\n    xgrid, ygrid = coords.split([1, 1], dim=-1)\n    xgrid = 2 * xgrid / (W - 1) - 1\n    # print(\"######88888\", xgrid)\n    assert torch.unique(ygrid).numel() == 1 and H == 1  # This is a stereo problem\n    grid = torch.cat([xgrid, ygrid], dim=-1)\n    # print(\"###37777\", grid.shape)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "coords_grid",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "peekOfCode": "def coords_grid(batch, ht, wd):\n    coords = torch.meshgrid(torch.arange(ht), torch.arange(wd))\n    coords = torch.stack(coords[::-1], dim=0).float()\n    return coords[None].repeat(batch, 1, 1, 1)\ndef upflow8(flow, mode='bilinear'):\n    new_size = (8 * flow.shape[2], 8 * flow.shape[3])\n    return 8 * F.interpolate(flow, size=new_size, mode=mode, align_corners=True)\ndef gauss_blur(input, N=5, std=1):\n    B, D, H, W = input.shape\n    x, y = torch.meshgrid(torch.arange(N).float() - N // 2, torch.arange(N).float() - N // 2)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "upflow8",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "peekOfCode": "def upflow8(flow, mode='bilinear'):\n    new_size = (8 * flow.shape[2], 8 * flow.shape[3])\n    return 8 * F.interpolate(flow, size=new_size, mode=mode, align_corners=True)\ndef gauss_blur(input, N=5, std=1):\n    B, D, H, W = input.shape\n    x, y = torch.meshgrid(torch.arange(N).float() - N // 2, torch.arange(N).float() - N // 2)\n    unnormalized_gaussian = torch.exp(-(x.pow(2) + y.pow(2)) / (2 * std ** 2))\n    weights = unnormalized_gaussian / unnormalized_gaussian.sum().clamp(min=1e-4)\n    weights = weights.view(1, 1, N, N).to(input)\n    output = F.conv2d(input.reshape(B * D, 1, H, W), weights, padding=N // 2)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "gauss_blur",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "peekOfCode": "def gauss_blur(input, N=5, std=1):\n    B, D, H, W = input.shape\n    x, y = torch.meshgrid(torch.arange(N).float() - N // 2, torch.arange(N).float() - N // 2)\n    unnormalized_gaussian = torch.exp(-(x.pow(2) + y.pow(2)) / (2 * std ** 2))\n    weights = unnormalized_gaussian / unnormalized_gaussian.sum().clamp(min=1e-4)\n    weights = weights.view(1, 1, N, N).to(input)\n    output = F.conv2d(input.reshape(B * D, 1, H, W), weights, padding=N // 2)\n    return output.view(B, D, H, W)\nclass Map(dict):\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "MsCostVolumeManager",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.cost_volume",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.cost_volume",
        "peekOfCode": "class MsCostVolumeManager(nn.Module):\n    def __init__(self,\n                 num_depth_bins=64,\n                 multiscale=1,\n                 disp_scale=2,\n                 matching_dim_size=16,\n                 dot_dim=1):\n        super().__init__()\n        self.num_depth_bins = [4, 6, num_depth_bins]\n        self.disp_scale = disp_scale",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.cost_volume",
        "documentation": {}
    },
    {
        "label": "IINet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.iinet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.iinet",
        "peekOfCode": "class IINet(nn.Module):\n    def __init__(self, opts):\n        super().__init__()\n        self.run_opts = opts\n        self.num_ch_enc = [16, 24, 40, 112, 160]\n        # iniitalize the first half of the U-Net, encoding the cost volume\n        # and image prior image feautres\n        if self.run_opts.CV_ENCODER_TYPE == \"multi_scale_encoder\":\n            self.cost_volume_net = CVEncoder(\n                num_ch_cv=self.run_opts.MAX_DISP // 2 ** (self.run_opts.MATCHING_SCALE + 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.iinet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "documentation": {}
    },
    {
        "label": "TensorFormatter",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "peekOfCode": "class TensorFormatter(nn.Module):\n    \"\"\"Helper to format, apply operation, format back tensor.\n    Class to format tensors of shape B x D x C_i x H x W into B*D x C_i x H x W,\n    apply an operation, and reshape back into B x D x C_o x H x W.\n    Used for multidepth - batching feature extraction on source images\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.batch_size = None\n        self.depth_chns = None\n    def _expand_batch_with_channels(self, x):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "peekOfCode": "def conv3x3(\n            in_planes: int, \n            out_planes: int, \n            stride: int = 1, \n            groups: int = 1, \n            dilation: int = 1, \n            bias: bool = False\n        ) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "peekOfCode": "def conv1x1(in_planes: int, out_planes: int, stride: int = 1, bias: bool = False) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias)\nclass BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "documentation": {}
    },
    {
        "label": "multiple_basic_block",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "peekOfCode": "def multiple_basic_block(num_ch_in, num_ch_out, num_repeats=2):\n    layers = nn.Sequential(BasicBlock(num_ch_in, num_ch_out))\n    for i in range(num_repeats - 1):\n        layers.add_module(f\"conv_{i}\", BasicBlock(num_ch_out, num_ch_out))\n    return layers\nclass TensorFormatter(nn.Module):\n    \"\"\"Helper to format, apply operation, format back tensor.\n    Class to format tensors of shape B x D x C_i x H x W into B*D x C_i x H x W,\n    apply an operation, and reshape back into B x D x C_o x H x W.\n    Used for multidepth - batching feature extraction on source images\"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.layers",
        "documentation": {}
    },
    {
        "label": "VolumeBiFocalLoss",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.loss",
        "peekOfCode": "class VolumeBiFocalLoss(nn.Module):\n    def __init__(self, alpha, gamma):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    def forward(self, pt, target, weight):\n        loss = - weight * self.alpha * ((1 - pt) ** self.gamma) * (target * torch.log(pt)) - (1 - self.alpha) * (\n                        pt ** self.gamma) * ((1 - target) * torch.log(1 - pt))\n        return loss\nclass Criterion(nn.Module):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.loss",
        "documentation": {}
    },
    {
        "label": "Criterion",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.loss",
        "peekOfCode": "class Criterion(nn.Module):\n    \"\"\"\n    Compute loss and evaluation metrics\n    \"\"\"\n    def __init__(self, max_disp: int = -1, loss_weight: dict = None, out_scales=4, disp_scale=16):\n        super(Criterion, self).__init__()\n        if loss_weight is None:\n            loss_weight = {}\n        self.num_scales = out_scales\n        self.max_disp = max_disp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.loss",
        "documentation": {}
    },
    {
        "label": "build_criterion",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.loss",
        "peekOfCode": "def build_criterion(opts, loss_weight):\n    return Criterion(opts.MAX_DISP, loss_weight, opts.OUT_SCALE, opts.DISP_SCALE)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.loss",
        "documentation": {}
    },
    {
        "label": "make_nograd_func",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def make_nograd_func(func):\n    def wrapper(*f_args, **f_kwargs):\n        with torch.no_grad():\n            ret = func(*f_args, **f_kwargs)\n        return ret\n    return wrapper\ndef check_shape_for_metric_computation(*vars):\n    assert isinstance(vars, tuple)\n    for var in vars:\n        assert var.size() == vars[0].size()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "check_shape_for_metric_computation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def check_shape_for_metric_computation(*vars):\n    assert isinstance(vars, tuple)\n    for var in vars:\n        assert var.size() == vars[0].size()\n# a wrapper to compute metrics for each image individually\ndef compute_metric_for_each_image(metric_func):\n    def wrapper(D_ests, D_gts, masks, *nargs):\n        check_shape_for_metric_computation(D_ests, D_gts, masks)\n        bn = D_gts.shape[0]  # batch size\n        results = []  # a list to store results for each image",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "compute_metric_for_each_image",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def compute_metric_for_each_image(metric_func):\n    def wrapper(D_ests, D_gts, masks, *nargs):\n        check_shape_for_metric_computation(D_ests, D_gts, masks)\n        bn = D_gts.shape[0]  # batch size\n        results = []  # a list to store results for each image\n        # compute result one by one\n        for idx in range(bn):\n            # if tensor, then pick idx, else pass the same value\n            cur_nargs = [x[idx] if isinstance(x, (Tensor, Variable)) else x for x in nargs]\n            #if masks[idx].float().mean() / (D_gts[idx] > 0).float().mean() < 0.1:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "D1_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def D1_metric(D_est, D_gt, mask):\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    E = torch.abs(D_gt - D_est)\n    err_mask = (E > 3) & (E / D_gt.abs() > 0.05)\n    return torch.mean(err_mask.float())\n@make_nograd_func\n@compute_metric_for_each_image\ndef Thres_metric(D_est, D_gt, mask, thres):\n    assert isinstance(thres, (int, float))\n    D_est, D_gt = D_est[mask], D_gt[mask]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "Thres_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def Thres_metric(D_est, D_gt, mask, thres):\n    assert isinstance(thres, (int, float))\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    E = torch.abs(D_gt - D_est)\n    err_mask = E > thres\n    return torch.mean(err_mask.float())\n# NOTE: please do not use this to build up training loss\n@make_nograd_func\n@compute_metric_for_each_image\ndef EPE_metric(D_est, D_gt, mask):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "EPE_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def EPE_metric(D_est, D_gt, mask):\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    return F.l1_loss(D_est, D_gt, size_average=True)\n@make_nograd_func\n@compute_metric_for_each_image\ndef D1_metric_mask(D_est, D_gt, mask, mask_img):\n    # D_est, D_gt = D_est[(mask&mask_img)], D_gt[(mask&mask_img)]\n    D_est, D_gt = D_est[mask_img], D_gt[mask_img]\n    E = torch.abs(D_gt - D_est)\n    err_mask = (E > 3) & (E / D_gt.abs() > 0.05)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "D1_metric_mask",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def D1_metric_mask(D_est, D_gt, mask, mask_img):\n    # D_est, D_gt = D_est[(mask&mask_img)], D_gt[(mask&mask_img)]\n    D_est, D_gt = D_est[mask_img], D_gt[mask_img]\n    E = torch.abs(D_gt - D_est)\n    err_mask = (E > 3) & (E / D_gt.abs() > 0.05)\n    return torch.mean(err_mask.float())\n@make_nograd_func\n@compute_metric_for_each_image\ndef Thres_metric_mask(D_est, D_gt, mask, thres, mask_img):\n    assert isinstance(thres, (int, float))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "Thres_metric_mask",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def Thres_metric_mask(D_est, D_gt, mask, thres, mask_img):\n    assert isinstance(thres, (int, float))\n    # D_est, D_gt = D_est[(mask&mask_img)], D_gt[(mask&mask_img)]\n    D_est, D_gt = D_est[mask_img], D_gt[mask_img]\n    E = torch.abs(D_gt - D_est)\n    err_mask = E > thres\n    return torch.mean(err_mask.float())\n# NOTE: please do not use this to build up training loss\n@make_nograd_func\n@compute_metric_for_each_image",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "EPE_metric_mask",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def EPE_metric_mask(D_est, D_gt, mask, mask_img):\n    # print((mask&mask_img).size(), D_est.size(), mask, mask_img)\n    # D_est, D_gt = D_est[(mask&mask_img)], D_gt[(mask&mask_img)]\n    D_est, D_gt = D_est[mask_img], D_gt[mask_img]\n    return F.l1_loss(D_est, D_gt, size_average=True)\n@make_nograd_func\ndef cal_metric(disp_est: Tensor, disp_gt: Tensor, metric_dict: dict, valid_mask: Tensor):\n    with torch.no_grad():\n        metric_dict[\"epe\"] = EPE_metric(disp_est, disp_gt, valid_mask)\n        metric_dict[\"d1\"] = D1_metric(disp_est, disp_gt, valid_mask) * 100",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "cal_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def cal_metric(disp_est: Tensor, disp_gt: Tensor, metric_dict: dict, valid_mask: Tensor):\n    with torch.no_grad():\n        metric_dict[\"epe\"] = EPE_metric(disp_est, disp_gt, valid_mask)\n        metric_dict[\"d1\"] = D1_metric(disp_est, disp_gt, valid_mask) * 100\n        metric_dict[\"bad1\"] = Thres_metric(disp_est, disp_gt, valid_mask, 1.0) * 100\n        metric_dict[\"bad2\"] = Thres_metric(disp_est, disp_gt, valid_mask, 2.0) * 100\n        metric_dict[\"bad3\"] = Thres_metric(disp_est, disp_gt, valid_mask, 3.0) * 100\n    return",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "RaftUpSampler",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "peekOfCode": "class RaftUpSampler(nn.Module):\n    def __init__(\n                self,\n                fdim=128,\n                hdim=128,\n                priordim=1,\n            ):\n        super().__init__()\n        self.fdim = fdim\n        self.hdim = hdim",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "DepthDecoderMSR",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "peekOfCode": "class DepthDecoderMSR(nn.Module):\n    def __init__(\n                self,\n                num_ch_enc,\n                lrcv_scale=3,\n                multiscale=1,\n                scales=4,\n                num_output_channels=1,\n                use_skips=True\n            ):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "CVEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "peekOfCode": "class CVEncoder(nn.Module):\n    def __init__(self, num_ch_cv, num_ch_encs, num_ch_outs, lrcv_level, multi_scale=1):\n        super().__init__()\n        self.convs = nn.ModuleDict()\n        self.num_ch_enc = []\n        self.multi_scale = multi_scale\n        start_level = lrcv_level - multi_scale\n        num_ch_subencs = num_ch_encs[start_level:] #lrcv_level=0 represent stat with (1/2) resolution\n        num_ch_subouts = num_ch_outs[start_level:]\n        self.num_blocks = len(num_ch_subouts)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "peekOfCode": "class MLP(nn.Module):\n    def __init__(self, channel_list, disable_final_activation=True):\n        super(MLP, self).__init__()\n        layer_list = []\n        for layer_index in list(range(len(channel_list)))[:-1]:\n            layer_list.append(\n                            nn.Linear(channel_list[layer_index], \n                                channel_list[layer_index+1])\n                            )\n            layer_list.append(nn.LeakyReLU(inplace=True))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "ResnetMatchingEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "peekOfCode": "class ResnetMatchingEncoder(nn.Module):\n    \"\"\"Pytorch module for a resnet encoder\n    \"\"\"\n    def __init__(\n                self,\n                num_layers,\n                num_ch_out,\n                matching_scale=1,\n                multiscale=1,\n                pretrainedfp=None,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "UnetMatchingEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "peekOfCode": "class UnetMatchingEncoder(nn.Module):\n    \"\"\"Pytorch module for a resnet encoder\n        \"\"\"\n    def __init__(\n            self,\n            num_ch_out,\n            matching_scale=1,\n            multiscale=1,\n            pretrainedfp=None,\n    ):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)\n    def _get_pos_fullres(self, fx, w, h):\n        x_range = (np.linspace(0, w - 1, w) + 0.5 - w // 2) / fx\n        y_range = (np.linspace(0, h - 1, h) + 0.5 - h // 2) / fx\n        x, y = np.meshgrid(x_range, y_range)\n        z = np.ones_like(x)\n        pos_grid = np.stack([x, y, z], axis=0).astype(np.float32)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.trainer",
        "peekOfCode": "__all__ = {\n    'IINet': IINet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)\n    def _get_pos_fullres(self, fx, w, h):\n        x_range = (np.linspace(0, w - 1, w) + 0.5 - w // 2) / fx\n        y_range = (np.linspace(0, h - 1, h) + 0.5 - h // 2) / fx",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.trainer",
        "documentation": {}
    },
    {
        "label": "DictAverageMeter",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "class DictAverageMeter(object):\n    def __init__(self, wsize=100):\n        self.wsize = wsize\n        self.sum_data = {}\n        self.avg_data = {}\n        self.count = {}\n        return\n    def update(self, new_input):\n        if len(self.sum_data) == 0:\n            for k, v in new_input.items():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "make_nograd_func",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def make_nograd_func(func):\n    def wrapper(*f_args, **f_kwargs):\n        with torch.no_grad():\n            ret = func(*f_args, **f_kwargs)\n        return ret\n    return wrapper\n# convert a function into recursive style to handle nested dict/list/tuple variables\ndef make_recursive_func(func):\n    def wrapper(vars):\n        if isinstance(vars, list):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "make_recursive_func",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def make_recursive_func(func):\n    def wrapper(vars):\n        if isinstance(vars, list):\n            return [wrapper(x) for x in vars]\n        elif isinstance(vars, tuple):\n            return tuple([wrapper(x) for x in vars])\n        elif isinstance(vars, dict):\n            return {k: wrapper(v) for k, v in vars.items()}\n        else:\n            return func(vars)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "tensor2float",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def tensor2float(vars):\n    if isinstance(vars, float):\n        return vars\n    elif isinstance(vars, torch.Tensor):\n        return vars.data.item()\n    else:\n        raise NotImplementedError(\"invalid input type {} for tensor2float\".format(type(vars)))\n@make_recursive_func\ndef tensor2numpy(vars):\n    if isinstance(vars, np.ndarray):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "tensor2numpy",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def tensor2numpy(vars):\n    if isinstance(vars, np.ndarray):\n        return vars\n    elif isinstance(vars, torch.Tensor):\n        return vars.detach().cpu().numpy().copy()\n    else:\n        raise NotImplementedError(\"invalid input type {} for tensor2numpy\".format(type(vars)))\n@make_recursive_func\ndef tocuda(vars):\n    if isinstance(vars, torch.Tensor):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "tocuda",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def tocuda(vars):\n    if isinstance(vars, torch.Tensor):\n        return vars.to(torch.device(\"cuda\"))\n    elif isinstance(vars, str):\n        return vars\n    else:\n        raise NotImplementedError(\"invalid input type {} for tensor2numpy\".format(type(vars)))\ndef upsample(x):\n    \"\"\"\n    Upsample input tensor by a factor of 2",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "upsample",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def upsample(x):\n    \"\"\"\n    Upsample input tensor by a factor of 2\n    \"\"\"\n    return nn.functional.interpolate(\n                                x,\n                                scale_factor=2,\n                                mode=\"bilinear\",\n                                align_corners=False,\n                            )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "save_scalars",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def save_scalars(logger, mode, scalar_dict, global_step):\n    scalar_dict = tensor2float(scalar_dict)\n    for key, value in scalar_dict.items():\n        if not isinstance(value, (list, tuple)):\n            name = '{}/{}'.format(mode, key)\n            logger.add_scalar(name, value, global_step)\n        else:\n            for idx in range(len(value)):\n                name = '{}/{}_{}'.format(mode, key, idx)\n                logger.add_scalar(name, value[idx], global_step)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "save_images",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def save_images(logger, mode, images_dict, global_step):\n    images_dict = tensor2numpy(images_dict)\n    def preprocess(name, img):\n        if not (len(img.shape) == 3 or len(img.shape) == 4):\n            raise NotImplementedError(\"invalid img shape {}:{} in save_images\".format(name, img.shape))\n        if len(img.shape) == 3:\n            normalizer = mpl.colors.Normalize(vmin=img[0].min(), vmax=img[0].max())\n            mapper = cm.ScalarMappable(norm=normalizer, cmap='jet')\n            cimg = (mapper.to_rgba(img[0])[:,:,:3] *255).astype(np.uint8).transpose(2,0,1)\n            cimg = torch.from_numpy(cimg)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "savepreprocess",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def savepreprocess(img, ifRgb=False):\n    if not ifRgb:\n        normalizer = mpl.colors.Normalize(vmin=img[0].min(), vmax=img[0].max())\n        mapper = cm.ScalarMappable(norm=normalizer, cmap='jet')\n        cimg = (mapper.to_rgba(img[0])[:, :, :3] * 255).astype(np.uint8)\n        return cimg\n    else:\n        channel_max = np.max(img[0], axis=(1, 2), keepdims=True)\n        channel_min = np.min(img[0], axis=(1, 2), keepdims=True)\n        cimg = (img[0] - channel_min) / (channel_max - channel_min)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "compute_metrics_for_each_image",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def compute_metrics_for_each_image(metric_func):\n    def wrapper(depth_est, depth_gt, mask, *args):\n        batch_size = depth_gt.shape[0]\n        results = []\n        # compute result one by one\n        for idx in range(batch_size):\n            ret = metric_func(depth_est[idx], depth_gt[idx], mask[idx], *args)\n            results.append(ret)\n        return torch.stack(results).mean()\n    return wrapper",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "D1_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def D1_metric(D_est, D_gt, mask):\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    E = torch.abs(D_gt - D_est)\n    err_mask = (E > 3) & (E / D_gt.abs() > 0.05)\n    return torch.mean(err_mask.float())\n@make_nograd_func\n@compute_metrics_for_each_image\ndef Thres_metric(D_est, D_gt, mask, thres):\n    assert isinstance(thres, (int, float))\n    D_est, D_gt = D_est[mask], D_gt[mask]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "Thres_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def Thres_metric(D_est, D_gt, mask, thres):\n    assert isinstance(thres, (int, float))\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    E = torch.abs(D_gt - D_est)\n    err_mask = E > thres\n    return torch.mean(err_mask.float())\n# NOTE: please do not use this to build up training loss\n@make_nograd_func\n@compute_metrics_for_each_image\ndef EPE_metric(D_est, D_gt, mask):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "EPE_metric",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def EPE_metric(D_est, D_gt, mask):\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    return F.l1_loss(D_est, D_gt, size_average=True)\nimport torch.distributed as dist\ndef synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "synchronize",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def get_world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\ndef reduce_scalar_outputs(scalar_outputs):\n    world_size = get_world_size()\n    if world_size < 2:\n        return scalar_outputs",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "reduce_scalar_outputs",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def reduce_scalar_outputs(scalar_outputs):\n    world_size = get_world_size()\n    if world_size < 2:\n        return scalar_outputs\n    with torch.no_grad():\n        names = []\n        scalars = []\n        for k in sorted(scalar_outputs.keys()):\n            names.append(k)\n            scalars.append(scalar_outputs[k])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "adjust_learning_rate",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def adjust_learning_rate(optimizer, epoch, base_lr, lrepochs):\n    splits = lrepochs.split(':')\n    assert len(splits) == 2\n    # parse the epochs to downscale the learning rate (before :)\n    downscale_epochs = [int(eid_str) for eid_str in splits[0].split(',')]\n    # parse downscale rate (after :)\n    downscale_rates = [float(eid_str) for eid_str in splits[1].split(',')]\n    print(\"downscale epochs: {}, downscale rate: {}\".format(downscale_epochs, downscale_rates))\n    lr = base_lr\n    for eid, downscale_rate in zip(downscale_epochs, downscale_rates):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "set_random_seed",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "peekOfCode": "def set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "Aggregation",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.aggregation",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.aggregation",
        "peekOfCode": "class Aggregation(nn.Module):\n    def __init__(self, in_channels, left_att, blocks, expanse_ratio, backbone_channels):\n        super(Aggregation, self).__init__()\n        self.left_att = left_att\n        self.expanse_ratio = expanse_ratio\n        conv0 = [MobileV2Residual(in_channels, in_channels, stride=1, expanse_ratio=self.expanse_ratio)\n                 for i in range(blocks[0])]\n        self.conv0 = nn.Sequential(*conv0)\n        self.conv1 = MobileV2Residual(in_channels, in_channels * 2, stride=2, expanse_ratio=self.expanse_ratio)\n        conv2_add = [MobileV2Residual(in_channels * 2, in_channels * 2, stride=1, expanse_ratio=self.expanse_ratio)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.aggregation",
        "documentation": {}
    },
    {
        "label": "MobileV2Residual",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.aggregation",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.aggregation",
        "peekOfCode": "class MobileV2Residual(nn.Module):\n    def __init__(self, inp, oup, stride, expanse_ratio, dilation=1):\n        super(MobileV2Residual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n        hidden_dim = int(inp * expanse_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n        pad = dilation\n        # v2\n        self.pwconv = nn.Sequential(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.aggregation",
        "documentation": {}
    },
    {
        "label": "AttentionModule",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.aggregation",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.aggregation",
        "peekOfCode": "class AttentionModule(nn.Module):\n    def __init__(self, dim, img_feat_dim):\n        super().__init__()\n        self.conv0 = nn.Conv2d(img_feat_dim, dim, 1)\n        self.conv0_1 = nn.Conv2d(dim, dim, (1, 7), padding=(0, 3), groups=dim)\n        self.conv0_2 = nn.Conv2d(dim, dim, (7, 1), padding=(3, 0), groups=dim)\n        self.conv1_1 = nn.Conv2d(dim, dim, (1, 11), padding=(0, 5), groups=dim)\n        self.conv1_2 = nn.Conv2d(dim, dim, (11, 1), padding=(5, 0), groups=dim)\n        self.conv2_1 = nn.Conv2d(dim, dim, (1, 21), padding=(0, 10), groups=dim)\n        self.conv2_2 = nn.Conv2d(dim, dim, (21, 1), padding=(10, 0), groups=dim)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.aggregation",
        "documentation": {}
    },
    {
        "label": "FPNLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.backbone",
        "peekOfCode": "class FPNLayer(nn.Module):\n    def __init__(self, chan_low, chan_high):\n        super().__init__()\n        self.deconv = BasicDeconv2d(chan_low, chan_high, kernel_size=4, stride=2, padding=1,\n                                    norm_layer=nn.BatchNorm2d,\n                                    act_layer=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True))\n        self.conv = BasicConv2d(chan_high * 2, chan_high, kernel_size=3, padding=1,\n                                norm_layer=nn.BatchNorm2d,\n                                act_layer=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True))\n    def forward(self, low, high):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.backbone",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.backbone",
        "peekOfCode": "class Backbone(nn.Module):\n    def __init__(self, backbone='MobileNetv2'):\n        super().__init__()\n        if backbone == 'MobileNetv2':\n            model = timm.create_model('mobilenetv2_100', pretrained=True, features_only=True)\n            channels = [160, 96, 32, 24]\n        elif backbone == 'EfficientNetv2':\n            model = timm.create_model('efficientnetv2_rw_s', pretrained=True, features_only=True)\n            channels = [272, 160, 64, 48]\n        else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.backbone",
        "documentation": {}
    },
    {
        "label": "LightStereo",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.lightstereo",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.lightstereo",
        "peekOfCode": "class LightStereo(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.max_disp = cfgs.MAX_DISP\n        self.left_att = cfgs.LEFT_ATT\n        # backbobe\n        self.backbone = Backbone(cfgs.get('BACKCONE', 'MobileNetv2'))\n        # aggregation\n        self.cost_agg = Aggregation(in_channels=48,\n                                    left_att=self.left_att,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.lightstereo",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.trainer",
        "peekOfCode": "__all__ = {\n    'LightStereo': LightStereo,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.lightstereo.trainer",
        "documentation": {}
    },
    {
        "label": "MobileV2Residual",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "peekOfCode": "class MobileV2Residual(nn.Module):\n    def __init__(self, inp, oup, stride, expanse_ratio, dilation=1):\n        super(MobileV2Residual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n        hidden_dim = int(inp * expanse_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n        pad = dilation\n        if expanse_ratio == 1:\n            self.conv = nn.Sequential(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "documentation": {}
    },
    {
        "label": "Hourglass2D",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "peekOfCode": "class Hourglass2D(nn.Module):\n    def __init__(self, in_channels):\n        super(Hourglass2D, self).__init__()\n        self.expanse_ratio = 2\n        self.conv1 = MobileV2Residual(in_channels, in_channels * 2, stride=2, expanse_ratio=self.expanse_ratio)\n        self.conv2 = MobileV2Residual(in_channels * 2, in_channels * 2, stride=1, expanse_ratio=self.expanse_ratio)\n        self.conv3 = MobileV2Residual(in_channels * 2, in_channels * 4, stride=2, expanse_ratio=self.expanse_ratio)\n        self.conv4 = MobileV2Residual(in_channels * 4, in_channels * 4, stride=1, expanse_ratio=self.expanse_ratio)\n        self.conv5 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels * 4, in_channels * 2, 3, padding=1, output_padding=1, stride=2, bias=False),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "documentation": {}
    },
    {
        "label": "hourglass2D",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet2D",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet2D",
        "peekOfCode": "class hourglass2D(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass2D, self).__init__()\n        self.expanse_ratio = 2\n        self.conv1 = MobileV2_Residual(in_channels, in_channels * 2, stride=2, expanse_ratio=self.expanse_ratio)\n        self.conv2 = MobileV2_Residual(in_channels * 2, in_channels * 2, stride=1, expanse_ratio=self.expanse_ratio)\n        self.conv3 = MobileV2_Residual(in_channels * 2, in_channels * 4, stride=2, expanse_ratio=self.expanse_ratio)\n        self.conv4 = MobileV2_Residual(in_channels * 4, in_channels * 4, stride=1, expanse_ratio=self.expanse_ratio)\n        self.conv5 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels * 4, in_channels * 2, 3, padding=1, output_padding=1, stride=2, bias=False),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet2D",
        "documentation": {}
    },
    {
        "label": "MSNet2D",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet2D",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet2D",
        "peekOfCode": "class MSNet2D(nn.Module):\n    def __init__(self, cfgs):\n        super(MSNet2D, self).__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.num_groups = 1\n        self.volume_size = 48\n        self.hg_size = 48\n        self.dres_expanse_ratio = 3\n        self.feature_extraction = feature_extraction(add_relus=True)\n        self.preconv11 = nn.Sequential(convbn(320, 256, 1, 1, 0, 1),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet2D",
        "documentation": {}
    },
    {
        "label": "hourglass3D",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet3D",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet3D",
        "peekOfCode": "class hourglass3D(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass3D, self).__init__()\n        self.expanse_ratio = 2\n        self.conv1 = MobileV2_Residual_3D(in_channels, in_channels * 2, 2, self.expanse_ratio)\n        self.conv2 = MobileV2_Residual_3D(in_channels * 2, in_channels * 2, 1, self.expanse_ratio)\n        self.conv3 = MobileV2_Residual_3D(in_channels * 2, in_channels * 4, 2, self.expanse_ratio)\n        self.conv4 = MobileV2_Residual_3D(in_channels * 4, in_channels * 4, 1, self.expanse_ratio)\n        self.conv5 = nn.Sequential(\n            nn.ConvTranspose3d(in_channels * 4, in_channels * 2, 3, padding=1, output_padding=1, stride=2, bias=False),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet3D",
        "documentation": {}
    },
    {
        "label": "MSNet3D",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet3D",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet3D",
        "peekOfCode": "class MSNet3D(nn.Module):\n    def __init__(self, cfgs):\n        super(MSNet3D, self).__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.hourglass_size = 32\n        self.dres_expanse_ratio = 3\n        self.num_groups = 40\n        self.feature_extraction = feature_extraction()\n        self.dres0 = nn.Sequential(\n            MobileV2_Residual_3D(self.num_groups, self.hourglass_size, 1, self.dres_expanse_ratio),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.MSNet3D",
        "documentation": {}
    },
    {
        "label": "MobileV1_Residual",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "class MobileV1_Residual(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(MobileV1_Residual, self).__init__()\n        self.stride = stride\n        self.downsample = downsample\n        self.conv1 = convbn_dws(inplanes, planes, 3, stride, pad, dilation)\n        self.conv2 = convbn_dws(planes, planes, 3, 1, pad, dilation, second_relu=False)\n    def forward(self, x):\n        out = self.conv1(x)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "MobileV2_Residual",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "class MobileV2_Residual(nn.Module):\n    def __init__(self, inp, oup, stride, expanse_ratio, dilation=1):\n        super(MobileV2_Residual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n        hidden_dim = int(inp * expanse_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n        pad = dilation\n        if expanse_ratio == 1:\n            self.conv = nn.Sequential(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "MobileV2_Residual_3D",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "class MobileV2_Residual_3D(nn.Module):\n    def __init__(self, inp, oup, stride, expanse_ratio):\n        super(MobileV2_Residual_3D, self).__init__()\n        self.stride = stride\n        hidden_dim = round(inp * expanse_ratio)\n        self.use_res_connect = self.stride == (1, 1, 1) and inp == oup\n        if expanse_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv3d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self, add_relus=False):\n        super(feature_extraction, self).__init__()\n        self.expanse_ratio = 3\n        self.inplanes = 32\n        if add_relus:\n            self.firstconv = nn.Sequential(MobileV2_Residual(3, 32, 2, self.expanse_ratio),\n                                           nn.ReLU(inplace=True),\n                                           MobileV2_Residual(32, 32, 1, self.expanse_ratio),\n                                           nn.ReLU(inplace=True),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                  padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n        nn.BatchNorm2d(out_channels)\n    )\ndef convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(\n        nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                  padding=pad, bias=False),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(\n        nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                  padding=pad, bias=False),\n        nn.BatchNorm3d(out_channels)\n    )\ndef convbn_dws(inp, oup, kernel_size, stride, pad, dilation, second_relu=True):\n    if second_relu:\n        return nn.Sequential(\n            # dw",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_dws",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def convbn_dws(inp, oup, kernel_size, stride, pad, dilation, second_relu=True):\n    if second_relu:\n        return nn.Sequential(\n            # dw\n            nn.Conv2d(inp, inp, kernel_size=kernel_size, stride=stride, padding=dilation if dilation > 1 else pad,\n                      dilation=dilation, groups=inp, bias=False),\n            nn.BatchNorm2d(inp),\n            nn.ReLU6(inplace=True),\n            # pw\n            nn.Conv2d(inp, oup, 1, 1, 0, bias=False),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "interweave_tensors",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def interweave_tensors(refimg_fea, targetimg_fea):\n    B, C, H, W = refimg_fea.shape\n    interwoven_features = refimg_fea.new_zeros([B, 2 * C, H, W])\n    interwoven_features[:, ::2, :, :] = refimg_fea\n    interwoven_features[:, 1::2, :, :] = targetimg_fea\n    interwoven_features = interwoven_features.contiguous()\n    return interwoven_features\ndef groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=False)\n###############################################################################\n\"\"\" Loss Function \"\"\"\n###############################################################################\ndef model_loss(disp_ests, disp_gt, mask):\n    weights = [0.5, 0.5, 0.7, 1.0]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "model_loss",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def model_loss(disp_ests, disp_gt, mask):\n    weights = [0.5, 0.5, 0.7, 1.0]\n    all_losses = []\n    for disp_est, weight in zip(disp_ests, weights):\n        all_losses.append(weight * F.smooth_l1_loss(disp_est[mask], disp_gt[mask], reduction='mean'))\n    return sum(all_losses)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.trainer",
        "peekOfCode": "__all__ = {\n    'MSNet2D': MSNet2D,\n    'MSNet3D': MSNet3D\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.msnet.trainer",
        "documentation": {}
    },
    {
        "label": "CfgNode",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "class CfgNode(_CfgNode):\n    \"\"\"\n    Our extended version of :class:`yacs.config.CfgNode`.\n    It contains the following extra features:\n    1. The :meth:`merge_from_file` method supports the \"__BASE__\" key,\n       which allows the new CfgNode to inherit all the attributes from the\n       base configuration file(s).\n    2. Keys that start with \"COMPUTED_\" are treated as insertion-only\n       \"computed\" attributes. They can be inserted regardless of whether\n       the CfgNode is frozen or not.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "get_cfg",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "def get_cfg() -> CfgNode:\n    \"\"\"\n    Get a copy of the default config.\n    Returns:\n        a CfgNode instance.\n    \"\"\"\n    from .default import _CN\n    return _CN.clone()\ndef set_global_cfg(cfg: CfgNode) -> None:\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "set_global_cfg",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "def set_global_cfg(cfg: CfgNode) -> None:\n    \"\"\"\n    Let the global config point to the given cfg.\n    Assume that the given \"cfg\" has the key \"KEY\", after calling\n    `set_global_cfg(cfg)`, the key can be accessed by:\n    ::\n        from nmrf.config import global_cfg\n        print(global_cfg.KEY)\n    By using a hacky global config, you can access these config anywhere,\n    without having to pass the config object or the values deep into the code.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "configurable",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "def configurable(init_func=None, *, from_config=None):\n    \"\"\"\n    Decorate a function or a class's __init__ method so that it can be called\n    with a :class:`CfgNode` object using a :func:`from_config` function that translates\n    :class:`CfgNode` to arguments.\n    Examples:\n    ::\n        # Usage 1: Decorate on __init__:\n        class A:\n            @configurable",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "BASE_KEY",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "BASE_KEY = \"__BASE__\"\nclass CfgNode(_CfgNode):\n    \"\"\"\n    Our extended version of :class:`yacs.config.CfgNode`.\n    It contains the following extra features:\n    1. The :meth:`merge_from_file` method supports the \"__BASE__\" key,\n       which allows the new CfgNode to inherit all the attributes from the\n       base configuration file(s).\n    2. Keys that start with \"COMPUTED_\" are treated as insertion-only\n       \"computed\" attributes. They can be inserted regardless of whether",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "global_cfg",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "global_cfg = CfgNode()\ndef get_cfg() -> CfgNode:\n    \"\"\"\n    Get a copy of the default config.\n    Returns:\n        a CfgNode instance.\n    \"\"\"\n    from .default import _CN\n    return _CN.clone()\ndef set_global_cfg(cfg: CfgNode) -> None:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "_CN",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN = CN()\n# The version number, to upgrade from old config to new ones if any\n# changes happen. It's recommended to keep a VERSION in your config file.\n_CN.VERSION = 2\n# ---------------------------------------------------------------------------- #\n# Model\n# ---------------------------------------------------------------------------- #\n_CN.BACKBONE = CN()\n_CN.BACKBONE.MODEL_TYPE = \"resnet\"\n_CN.BACKBONE.NORM_FN = \"instance\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.VERSION",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.VERSION = 2\n# ---------------------------------------------------------------------------- #\n# Model\n# ---------------------------------------------------------------------------- #\n_CN.BACKBONE = CN()\n_CN.BACKBONE.MODEL_TYPE = \"resnet\"\n_CN.BACKBONE.NORM_FN = \"instance\"\n_CN.BACKBONE.OUT_CHANNELS = 256\n_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE = CN()\n_CN.BACKBONE.MODEL_TYPE = \"resnet\"\n_CN.BACKBONE.NORM_FN = \"instance\"\n_CN.BACKBONE.OUT_CHANNELS = 256\n_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.MODEL_TYPE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.MODEL_TYPE = \"resnet\"\n_CN.BACKBONE.NORM_FN = \"instance\"\n_CN.BACKBONE.OUT_CHANNELS = 256\n_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.NORM_FN",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.NORM_FN = \"instance\"\n_CN.BACKBONE.OUT_CHANNELS = 256\n_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.OUT_CHANNELS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.OUT_CHANNELS = 256\n_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.WEIGHT_URL",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.DROP_PATH",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.COMPAT",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DPN",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DPN.MAX_DISP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DPN.COST_GROUP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DPN.NUM_PROPOSALS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DPN.CONTEXT_DIM",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.PROP_EMBED_DIM",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.INFER_EMBED_DIM",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.MLP_RATIO",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.SPLIT_SIZE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.WINDOW_SIZE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.REFINE_WINDOW_SIZE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.PROP_N_HEADS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.INFER_N_HEADS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.NUM_PROP_LAYERS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.NUM_INFER_LAYERS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.NUM_REFINE_LAYERS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.RETURN_INTERMEDIATE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.ATTN_DROP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.PROJ_DROP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()\n# List of dataset name for training",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.DROP_PATH",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()\n# List of dataset name for training\n_CN.DATASETS.TRAIN = [\"sceneflow\"]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.DROPOUT",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()\n# List of dataset name for training\n_CN.DATASETS.TRAIN = [\"sceneflow\"]\n# List of dataset name for testing",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.NORMALIZE_BEFORE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()\n# List of dataset name for training\n_CN.DATASETS.TRAIN = [\"sceneflow\"]\n# List of dataset name for testing\n_CN.DATASETS.TEST = [\"things\"]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.WITH_REFINEMENT",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()\n# List of dataset name for training\n_CN.DATASETS.TRAIN = [\"sceneflow\"]\n# List of dataset name for testing\n_CN.DATASETS.TEST = [\"things\"]\n# Image gamma",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS = CN()\n# List of dataset name for training\n_CN.DATASETS.TRAIN = [\"sceneflow\"]\n# List of dataset name for testing\n_CN.DATASETS.TEST = [\"things\"]\n# Image gamma\n_CN.DATASETS.IMG_GAMMA = None\n# Color saturation\n_CN.DATASETS.SATURATION_RANGE = [0, 1.4]\n# Flip the images horizontally or vertically, valid choice [False, 'h', 'v']",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.TRAIN",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.TRAIN = [\"sceneflow\"]\n# List of dataset name for testing\n_CN.DATASETS.TEST = [\"things\"]\n# Image gamma\n_CN.DATASETS.IMG_GAMMA = None\n# Color saturation\n_CN.DATASETS.SATURATION_RANGE = [0, 1.4]\n# Flip the images horizontally or vertically, valid choice [False, 'h', 'v']\n_CN.DATASETS.DO_FLIP = False\n# Re-scale the image randomly",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.TEST",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.TEST = [\"things\"]\n# Image gamma\n_CN.DATASETS.IMG_GAMMA = None\n# Color saturation\n_CN.DATASETS.SATURATION_RANGE = [0, 1.4]\n# Flip the images horizontally or vertically, valid choice [False, 'h', 'v']\n_CN.DATASETS.DO_FLIP = False\n# Re-scale the image randomly\n_CN.DATASETS.SPATIAL_SCALE = [-0.2, 0.4]\n# Simulate imperfect rectification",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.IMG_GAMMA",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.IMG_GAMMA = None\n# Color saturation\n_CN.DATASETS.SATURATION_RANGE = [0, 1.4]\n# Flip the images horizontally or vertically, valid choice [False, 'h', 'v']\n_CN.DATASETS.DO_FLIP = False\n# Re-scale the image randomly\n_CN.DATASETS.SPATIAL_SCALE = [-0.2, 0.4]\n# Simulate imperfect rectification\n_CN.DATASETS.YJITTER = False\n# Image size for training",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.SATURATION_RANGE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.SATURATION_RANGE = [0, 1.4]\n# Flip the images horizontally or vertically, valid choice [False, 'h', 'v']\n_CN.DATASETS.DO_FLIP = False\n# Re-scale the image randomly\n_CN.DATASETS.SPATIAL_SCALE = [-0.2, 0.4]\n# Simulate imperfect rectification\n_CN.DATASETS.YJITTER = False\n# Image size for training\n_CN.DATASETS.CROP_SIZE = [384, 768]\n_CN.DATASETS.DIVIS_BY = 8",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.DO_FLIP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.DO_FLIP = False\n# Re-scale the image randomly\n_CN.DATASETS.SPATIAL_SCALE = [-0.2, 0.4]\n# Simulate imperfect rectification\n_CN.DATASETS.YJITTER = False\n# Image size for training\n_CN.DATASETS.CROP_SIZE = [384, 768]\n_CN.DATASETS.DIVIS_BY = 8\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.SPATIAL_SCALE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.SPATIAL_SCALE = [-0.2, 0.4]\n# Simulate imperfect rectification\n_CN.DATASETS.YJITTER = False\n# Image size for training\n_CN.DATASETS.CROP_SIZE = [384, 768]\n_CN.DATASETS.DIVIS_BY = 8\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATALOADER = CN()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.YJITTER",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.YJITTER = False\n# Image size for training\n_CN.DATASETS.CROP_SIZE = [384, 768]\n_CN.DATASETS.DIVIS_BY = 8\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATALOADER = CN()\n# Number of data loading threads\n_CN.DATALOADER.NUM_WORKERS = 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.CROP_SIZE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.CROP_SIZE = [384, 768]\n_CN.DATASETS.DIVIS_BY = 8\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATALOADER = CN()\n# Number of data loading threads\n_CN.DATALOADER.NUM_WORKERS = 0\n# ---------------------------------------------------------------------------- #\n# Solver",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.DIVIS_BY",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.DIVIS_BY = 8\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATALOADER = CN()\n# Number of data loading threads\n_CN.DATALOADER.NUM_WORKERS = 0\n# ---------------------------------------------------------------------------- #\n# Solver\n# ---------------------------------------------------------------------------- #",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATALOADER",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATALOADER = CN()\n# Number of data loading threads\n_CN.DATALOADER.NUM_WORKERS = 0\n# ---------------------------------------------------------------------------- #\n# Solver\n# ---------------------------------------------------------------------------- #\n_CN.SOLVER = CN()\n_CN.SOLVER.MAX_ITER = 300000\n_CN.SOLVER.BASE_LR = 0.0005\n_CN.SOLVER.BASE_LR_END = 0.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATALOADER.NUM_WORKERS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATALOADER.NUM_WORKERS = 0\n# ---------------------------------------------------------------------------- #\n# Solver\n# ---------------------------------------------------------------------------- #\n_CN.SOLVER = CN()\n_CN.SOLVER.MAX_ITER = 300000\n_CN.SOLVER.BASE_LR = 0.0005\n_CN.SOLVER.BASE_LR_END = 0.0\n_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER = CN()\n_CN.SOLVER.MAX_ITER = 300000\n_CN.SOLVER.BASE_LR = 0.0005\n_CN.SOLVER.BASE_LR_END = 0.0\n_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.MAX_ITER",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.MAX_ITER = 300000\n_CN.SOLVER.BASE_LR = 0.0005\n_CN.SOLVER.BASE_LR_END = 0.0\n_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.BASE_LR",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.BASE_LR = 0.0005\n_CN.SOLVER.BASE_LR_END = 0.0\n_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.BASE_LR_END",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.BASE_LR_END = 0.0\n_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.BACKBONE_LR_DECAY",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.WEIGHT_DECAY",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number\n# of training images per step (i.e. per iteration). If we use 16 GPUs",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.WEIGHT_DECAY_NORM",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number\n# of training images per step (i.e. per iteration). If we use 16 GPUs\n# and IMS_PER_BATCH = 32, each GPU will see 2 images per batch.\n_CN.SOLVER.IMS_PER_BATCH = 8\n# Gradient clipping",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.BACKBONE_WEIGHT_DECAY",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number\n# of training images per step (i.e. per iteration). If we use 16 GPUs\n# and IMS_PER_BATCH = 32, each GPU will see 2 images per batch.\n_CN.SOLVER.IMS_PER_BATCH = 8\n# Gradient clipping\n_CN.SOLVER.GRAD_CLIP = 1.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.CHECKPOINT_PERIOD",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number\n# of training images per step (i.e. per iteration). If we use 16 GPUs\n# and IMS_PER_BATCH = 32, each GPU will see 2 images per batch.\n_CN.SOLVER.IMS_PER_BATCH = 8\n# Gradient clipping\n_CN.SOLVER.GRAD_CLIP = 1.0\n_CN.SOLVER.LOSS_WEIGHTS = [1.0, 1.0, 1.0, 1.4, 1.4, 1.4, 1.4, 1.6, 2.0, 2.0]\n# resume from pretrain model for finetuning or resuming from terminated training",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.LATEST_CHECKPOINT_PERIOD",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number\n# of training images per step (i.e. per iteration). If we use 16 GPUs\n# and IMS_PER_BATCH = 32, each GPU will see 2 images per batch.\n_CN.SOLVER.IMS_PER_BATCH = 8\n# Gradient clipping\n_CN.SOLVER.GRAD_CLIP = 1.0\n_CN.SOLVER.LOSS_WEIGHTS = [1.0, 1.0, 1.0, 1.4, 1.4, 1.4, 1.4, 1.6, 2.0, 2.0]\n# resume from pretrain model for finetuning or resuming from terminated training\n_CN.SOLVER.RESUME = None",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.IMS_PER_BATCH",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.IMS_PER_BATCH = 8\n# Gradient clipping\n_CN.SOLVER.GRAD_CLIP = 1.0\n_CN.SOLVER.LOSS_WEIGHTS = [1.0, 1.0, 1.0, 1.4, 1.4, 1.4, 1.4, 1.6, 2.0, 2.0]\n# resume from pretrain model for finetuning or resuming from terminated training\n_CN.SOLVER.RESUME = None\n_CN.SOLVER.STRICT_RESUME = True\n_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.GRAD_CLIP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.GRAD_CLIP = 1.0\n_CN.SOLVER.LOSS_WEIGHTS = [1.0, 1.0, 1.0, 1.4, 1.4, 1.4, 1.4, 1.6, 2.0, 2.0]\n# resume from pretrain model for finetuning or resuming from terminated training\n_CN.SOLVER.RESUME = None\n_CN.SOLVER.STRICT_RESUME = True\n_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.LOSS_WEIGHTS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.LOSS_WEIGHTS = [1.0, 1.0, 1.0, 1.4, 1.4, 1.4, 1.4, 1.6, 2.0, 2.0]\n# resume from pretrain model for finetuning or resuming from terminated training\n_CN.SOLVER.RESUME = None\n_CN.SOLVER.STRICT_RESUME = True\n_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.RESUME",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.RESUME = None\n_CN.SOLVER.STRICT_RESUME = True\n_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement\n_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.STRICT_RESUME",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.STRICT_RESUME = True\n_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement\n_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #\n# Specific test options",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.NO_RESUME_OPTIMIZER",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement\n_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #\n# Specific test options\n# ---------------------------------------------------------------------------- #",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.AUX_LOSS",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement\n_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #\n# Specific test options\n# ---------------------------------------------------------------------------- #\n_CN.TEST = CN()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.MAX_DISP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement\n_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #\n# Specific test options\n# ---------------------------------------------------------------------------- #\n_CN.TEST = CN()\n# The period (in terms of steps) to evaluate the model during training.\n# Set to 0 to disable.\n_CN.TEST.EVAL_PERIOD = 20000",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.LOSS_TYPE",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #\n# Specific test options\n# ---------------------------------------------------------------------------- #\n_CN.TEST = CN()\n# The period (in terms of steps) to evaluate the model during training.\n# Set to 0 to disable.\n_CN.TEST.EVAL_PERIOD = 20000\n# Threshold for metric computation for testing\n_CN.TEST.EVAL_THRESH = [['1.0', '3.0']]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST = CN()\n# The period (in terms of steps) to evaluate the model during training.\n# Set to 0 to disable.\n_CN.TEST.EVAL_PERIOD = 20000\n# Threshold for metric computation for testing\n_CN.TEST.EVAL_THRESH = [['1.0', '3.0']]\n# Maximum disparity for metric computation mask\n_CN.TEST.EVAL_MAX_DISP = [192]\n# Whether use only valid pixels in evaluation\n_CN.TEST.EVAL_ONLY_VALID = [True]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST.EVAL_PERIOD",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST.EVAL_PERIOD = 20000\n# Threshold for metric computation for testing\n_CN.TEST.EVAL_THRESH = [['1.0', '3.0']]\n# Maximum disparity for metric computation mask\n_CN.TEST.EVAL_MAX_DISP = [192]\n# Whether use only valid pixels in evaluation\n_CN.TEST.EVAL_ONLY_VALID = [True]\n# Whether evaluate disparity proposal\n_CN.TEST.EVAL_PROP = [True]\n# ---------------------------------------------------------------------------- #",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST.EVAL_THRESH",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST.EVAL_THRESH = [['1.0', '3.0']]\n# Maximum disparity for metric computation mask\n_CN.TEST.EVAL_MAX_DISP = [192]\n# Whether use only valid pixels in evaluation\n_CN.TEST.EVAL_ONLY_VALID = [True]\n# Whether evaluate disparity proposal\n_CN.TEST.EVAL_PROP = [True]\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST.EVAL_MAX_DISP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST.EVAL_MAX_DISP = [192]\n# Whether use only valid pixels in evaluation\n_CN.TEST.EVAL_ONLY_VALID = [True]\n# Whether evaluate disparity proposal\n_CN.TEST.EVAL_PROP = [True]\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Set seed to negative to fully randomize everything.\n# Set seed to positive to use a fixed seed. Note that a fixed seed increases",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST.EVAL_ONLY_VALID",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST.EVAL_ONLY_VALID = [True]\n# Whether evaluate disparity proposal\n_CN.TEST.EVAL_PROP = [True]\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Set seed to negative to fully randomize everything.\n# Set seed to positive to use a fixed seed. Note that a fixed seed increases\n# reproducibility but does not guarantee fully deterministic behavior.\n# Disabling all parallelism further increases reproducibility.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST.EVAL_PROP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST.EVAL_PROP = [True]\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Set seed to negative to fully randomize everything.\n# Set seed to positive to use a fixed seed. Note that a fixed seed increases\n# reproducibility but does not guarantee fully deterministic behavior.\n# Disabling all parallelism further increases reproducibility.\n_CN.SEED = 326\n# Benchmark different cudnn algorithms.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SEED",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SEED = 326\n# Benchmark different cudnn algorithms.\n# If input images have very different sizes, this option will have large overhead\n# for about 10k iterations. It usually hurts total time, but can benefit for certain models.\n# If input images have the same of similar sizes, benchmark is often helpful.\n_CN.CUDNN_BENCHMARK = True\n# global config is for quick hack purposes.\n# You can set them in command line or config files,\n# and access it with:\n#",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.CUDNN_BENCHMARK",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.CUDNN_BENCHMARK = True\n# global config is for quick hack purposes.\n# You can set them in command line or config files,\n# and access it with:\n#\n# from nmrf.config import global_cfg\n# print(global_cfg.HACK)\n#\n# Do not commit any configs into it.\n_CN.GLOBAL = CN()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.GLOBAL",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.GLOBAL = CN()\n_CN.GLOBAL.HACK = 1.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.GLOBAL.HACK",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.GLOBAL.HACK = 1.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_DownSample",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "peekOfCode": "class _DownSample(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, label, nms_thresh):\n        return MSDA.downsample_forward(input, label, nms_thresh)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return None, None, None\ndownsample = _DownSample.apply",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "documentation": {}
    },
    {
        "label": "downsample",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "peekOfCode": "downsample = _DownSample.apply",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "documentation": {}
    },
    {
        "label": "MSDeformAttnFunction",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "peekOfCode": "class MSDeformAttnFunction(Function):\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float32)\n    def forward(ctx, value, value_spatial_shapes, value_level_start_index,\n                sampling_locations, attention_weights, im2col_step):\n        ctx.im2col_step = im2col_step\n        output = MSDA.ms_deform_attn_forward(value, value_spatial_shapes,\n                                             value_level_start_index,\n                                             sampling_locations,\n                                             attention_weights,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "documentation": {}
    },
    {
        "label": "ms_deform_attn_core_pytorch",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "peekOfCode": "def ms_deform_attn_core_pytorch(value, value_spatial_shapes,\n                                sampling_locations, attention_weights):\n    # for debug and test only,\n    # need to use cuda version instead\n    N_, S_, M_, D_ = value.shape\n    _, Lq_, M_, L_, P_, _ = sampling_locations.shape\n    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)\n    sampling_grids = 2 * sampling_locations - 1\n    sampling_value_list = []\n    for lid_, (H_, W_) in enumerate(value_spatial_shapes):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "documentation": {}
    },
    {
        "label": "MSDeformAttn",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.modules.ms_deform_attn",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.modules.ms_deform_attn",
        "peekOfCode": "class MSDeformAttn(nn.Module):\n    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4, ratio=1.0):\n        \"\"\"Multi-Scale Deformable Attention Module.\n        :param d_model      hidden dimension\n        :param n_levels     number of feature levels\n        :param n_heads      number of attention heads\n        :param n_points     number of sampling points per attention head per feature level\n        \"\"\"\n        super().__init__()\n        if d_model % n_heads != 0:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.modules.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "get_extensions",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.setup",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.setup",
        "peekOfCode": "def get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, 'src')\n    main_file = glob.glob(os.path.join(extensions_dir, '*.cpp'))\n    source_cpu = glob.glob(os.path.join(extensions_dir, 'cpu', '*.cpp'))\n    source_cuda = glob.glob(os.path.join(extensions_dir, 'cuda', '*.cu'))\n    sources = main_file + source_cpu\n    extension = CppExtension\n    extra_compile_args = {'cxx': []}\n    define_macros = []",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.setup",
        "documentation": {}
    },
    {
        "label": "requirements",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.setup",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.setup",
        "peekOfCode": "requirements = ['torch', 'torchvision']\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, 'src')\n    main_file = glob.glob(os.path.join(extensions_dir, '*.cpp'))\n    source_cpu = glob.glob(os.path.join(extensions_dir, 'cpu', '*.cpp'))\n    source_cuda = glob.glob(os.path.join(extensions_dir, 'cuda', '*.cu'))\n    sources = main_file + source_cpu\n    extension = CppExtension\n    extra_compile_args = {'cxx': []}",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.setup",
        "documentation": {}
    },
    {
        "label": "check_forward_equal_with_pytorch_double",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "def check_forward_equal_with_pytorch_double():\n    value = torch.rand(N, S, M, D).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5\n    attention_weights /= attention_weights.sum(-1,\n                                               keepdim=True).sum(-2,\n                                                                 keepdim=True)\n    im2col_step = 2\n    output_pytorch = ms_deform_attn_core_pytorch(\n        value.double(), shapes, sampling_locations.double(),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "check_forward_equal_with_pytorch_float",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "def check_forward_equal_with_pytorch_float():\n    value = torch.rand(N, S, M, D).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5\n    attention_weights /= attention_weights.sum(-1,\n                                               keepdim=True).sum(-2,\n                                                                 keepdim=True)\n    im2col_step = 2\n    output_pytorch = ms_deform_attn_core_pytorch(\n        value, shapes, sampling_locations, attention_weights).detach().cpu()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "check_gradient_numerical",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "def check_gradient_numerical(channels=4,\n                             grad_value=True,\n                             grad_sampling_loc=True,\n                             grad_attn_weight=True):\n    value = torch.rand(N, S, M, channels).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5\n    attention_weights /= attention_weights.sum(-1,\n                                               keepdim=True).sum(-2,\n                                                                 keepdim=True)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "shapes",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "shapes = torch.as_tensor([(6, 4), (3, 2)], dtype=torch.long).cuda()\nlevel_start_index = torch.cat((shapes.new_zeros(\n    (1, )), shapes.prod(1).cumsum(0)[:-1]))\nS = sum([(H * W).item() for H, W in shapes])\ntorch.manual_seed(3)\n@torch.no_grad()\ndef check_forward_equal_with_pytorch_double():\n    value = torch.rand(N, S, M, D).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "level_start_index",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "level_start_index = torch.cat((shapes.new_zeros(\n    (1, )), shapes.prod(1).cumsum(0)[:-1]))\nS = sum([(H * W).item() for H, W in shapes])\ntorch.manual_seed(3)\n@torch.no_grad()\ndef check_forward_equal_with_pytorch_double():\n    value = torch.rand(N, S, M, D).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5\n    attention_weights /= attention_weights.sum(-1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "S",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "S = sum([(H * W).item() for H, W in shapes])\ntorch.manual_seed(3)\n@torch.no_grad()\ndef check_forward_equal_with_pytorch_double():\n    value = torch.rand(N, S, M, D).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5\n    attention_weights /= attention_weights.sum(-1,\n                                               keepdim=True).sum(-2,\n                                                                 keepdim=True)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "get_color_map",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.colorize",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.colorize",
        "peekOfCode": "def get_color_map():\n    return   np.array([[0.18995,0.07176,0.23217],\n                       [0.19483,0.08339,0.26149],\n                       [0.19956,0.09498,0.29024],\n                       [0.20415,0.10652,0.31844],\n                       [0.20860,0.11802,0.34607],\n                       [0.21291,0.12947,0.37314],\n                       [0.21708,0.14087,0.39964],\n                       [0.22111,0.15223,0.42558],\n                       [0.22500,0.16354,0.45096],",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.colorize",
        "documentation": {}
    },
    {
        "label": "init_dist",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def init_dist(launcher, backend='nccl', **kwargs):\n    if mp.get_start_method(allow_none=True) is None:\n        mp.set_start_method('spawn')\n    if launcher == 'pytorch':\n        _init_dist_pytorch(backend, **kwargs)\n    elif launcher == 'mpi':\n        _init_dist_mpi(backend, **kwargs)\n    elif launcher == 'slurm':\n        _init_dist_slurm(backend, **kwargs)\n    else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "get_dist_info",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def get_dist_info():\n    if dist.is_available():\n        initialized = dist.is_initialized()\n    else:\n        initialized = False\n    if initialized:\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n    else:\n        rank = 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "setup_for_distributed",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\ndef is_main_process() -> bool:\n    return get_dist_info()[0] == 0\ndef get_world_size() -> int:\n    return get_dist_info()[1]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def is_main_process() -> bool:\n    return get_dist_info()[0] == 0\ndef get_world_size() -> int:\n    return get_dist_info()[1]\ndef get_rank() -> int:\n    return get_dist_info()[0]\ndef synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def get_world_size() -> int:\n    return get_dist_info()[1]\ndef get_rank() -> int:\n    return get_dist_info()[0]\ndef synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def get_rank() -> int:\n    return get_dist_info()[0]\ndef synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "synchronize",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "gather",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def gather(data, dst=0, group=None):\n    \"\"\"\n    Run gather on arbitrary picklable data (not necessarily tensors).\n    Args:\n        data: any picklable object\n        dst (int): destination rank\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n    Returns:\n        list[data]: on dst, a list of data gathered from each rank. Otherwise,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "create_local_process_group",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def create_local_process_group(num_workers_per_machine: int) -> None:\n    \"\"\"\n    Create a process group that contains ranks within the same machine.\n    launch() will call this function. If you start\n    workers without launch(), you will have to also call this. Otherwise utilities\n    like `get_local_rank()` will not work.\n    This function contains a barrier. All processes must call it together.\n    Args:\n        num_workers_per_machine: the number of worker processes per machine. Typically\n            the number of GPUs.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "get_local_rank",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def get_local_rank() -> int:\n    \"\"\"\n    Returns:\n        the rank of the current process within the local (per-machine) process group.\n    \"\"\"\n    if not is_dist_avail_and_initialized():\n        return 0\n    assert _LOCAL_PROCESS_GROUP is not None, _MISSING_LOCAL_PG_ERROR\n    return dist.get_rank(group=_LOCAL_PROCESS_GROUP)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "_LOCAL_PROCESS_GROUP",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "_LOCAL_PROCESS_GROUP = None\n_MISSING_LOCAL_PG_ERROR = (\n    \"Local process group is not yet created! Please use `launch()` \"\n    \"to start processes and initialize pytorch process group. If you need to start \"\n    \"processes in other ways, please call create_local_process_group(\"\n    \"num_workers_per_machine) after calling torch.distributed.init_process_group().\"\n)\n@functools.lru_cache()\ndef create_local_process_group(num_workers_per_machine: int) -> None:\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "_MISSING_LOCAL_PG_ERROR",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "_MISSING_LOCAL_PG_ERROR = (\n    \"Local process group is not yet created! Please use `launch()` \"\n    \"to start processes and initialize pytorch process group. If you need to start \"\n    \"processes in other ways, please call create_local_process_group(\"\n    \"num_workers_per_machine) after calling torch.distributed.init_process_group().\"\n)\n@functools.lru_cache()\ndef create_local_process_group(num_workers_per_machine: int) -> None:\n    \"\"\"\n    Create a process group that contains ranks within the same machine.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "InferenceSampler",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "class InferenceSampler(Sampler):\n    \"\"\"\n    Produce indices for inference across all workers.\n    Inference needs to run on the __exact__ set of samples,\n    therefore when the total number of samples is not divisible by the number of workers,\n    this sampler produces different number of samples on different workers.\n    \"\"\"\n    def __init__(self, size):\n        \"\"\"\n        rgs:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "DatasetEvaluator",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "class DatasetEvaluator:\n    \"\"\"\n    Base class for a dataset evaluator.\n    The function :func:`inference_on_dataset` runs the model over\n    all samples in the dataset, and have a DatasetEvaluator to process the inputs/outputs.\n    This class will accumulate information of the inputs/outputs (by:meth:`process`),\n    add produce evaluation results in the end (by :meth:`evaluate`).\n    \"\"\"\n    def reset(self):\n        \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "DatasetEvaluators",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "class DatasetEvaluators(DatasetEvaluator):\n    \"\"\"\n    Wrapper class to combine multiple :class:`DatasetEvaluator` instances.\n    This class dispatches every evaluation call to\n    all of its :class:`DatasetEvaluator`.\n    \"\"\"\n    def __init__(self, evaluators):\n        \"\"\"\n        Args:\n            evaluators (list): the evaluators to combine.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "DispEvaluator",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "class DispEvaluator(DatasetEvaluator):\n    \"\"\"Evaluate disparity accuracy using metrics.\"\"\"\n    def __init__(self, thres, only_valid, max_disp=None, eval_prop=True, divis_by=8):\n        \"\"\"\n        Args:\n            thres (list[str] or None): threshold for outlier\n            only_valid (bool): whether invalid pixels are excluded from evaluation\n            max_disp (int or None): If None, maximum disparity will be regarded as infinity\n            eval_prop (bool): whether evaluate the proposal quality.\n        \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "print_csv_format",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "def print_csv_format(results):\n    \"\"\"\n    Print main metrics in a format similar to Detectron2,\n    so that they are easy to copypaste into a spreadsheet.\n    Args:\n        results (OrderedDict[dict]): task_name -> {metric -> score}\n            unordered dict can also be printed, but in arbitrary order\n    \"\"\"\n    assert isinstance(results, abc.Mapping) or not len(results), results\n    logger = logging.getLogger(__name__)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "inference_on_dataset",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "def inference_on_dataset(\n        model, data_loader, evaluator: Union[DatasetEvaluator, List[DatasetEvaluator], None], **kwargs\n):\n    \"\"\"\n    Run model on the data_loader and evaluate the metrics with evaluator.\n    Also benchmark the inference speed of `model.__call__` accurately.\n    The model will be used in eval mode.\n    Args:\n        model (callable): a callable which takes an object from\n            `data_loader` and returns some outputs.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "inference_context",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "def inference_context(model):\n    \"\"\"\n    A context where the model is temporarily changed to eval mode,\n    and restored to previous mode afterwards,\n    Args:\n        model: a torch Module\n    \"\"\"\n    training_mode = model.training\n    model.eval()\n    yield",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "InputPadder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "class InputPadder:\n    \"\"\" Pads images such that dimensions are divisible by 8 \"\"\"\n    def __init__(self, dims, mode='sintel', divis_by=8):\n        self.ht, self.wd = dims[-2:]\n        pad_ht = (((self.ht // divis_by) + 1) * divis_by - self.ht) % divis_by\n        pad_wd = (((self.wd // divis_by) + 1) * divis_by - self.wd) % divis_by\n        if mode == 'sintel':\n            self._pad = [pad_wd//2, pad_wd - pad_wd//2, pad_ht//2, pad_ht - pad_ht//2]\n        elif mode == 'proposal':\n            self._pad = [0, pad_wd, 0, pad_ht]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readFlow",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readFlow(fn):\n    \"\"\" Read .flo file in Middlebury format\"\"\"\n    # Code adapted from:\n    # http://stackoverflow.com/questions/28013200/reading-middlebury-flow-files-with-python-bytes-array-numpy\n    # WARNING: this will work on little-endian architectures (eg Intel x86) only!\n    # print 'fn = %s'%(fn)\n    with open(fn, 'rb') as f:\n        magic = np.fromfile(f, np.float32, count=1)\n        if 202021.25 != magic:\n            print('Magic number incorrect. Invalid .flo file')",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readPFM",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readPFM(file):\n    file = open(file, 'rb')\n    color = None\n    width = None\n    height = None\n    scale = None\n    endian = None\n    header = file.readline().rstrip()\n    if header == b'PF':\n        color = True",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "writePFM",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def writePFM(file, array):\n    import os\n    assert type(file) is str and type(array) is np.ndarray and \\\n           os.path.splitext(file)[1] == \".pfm\"\n    with open(file, 'wb') as f:\n        H, W = array.shape\n        headers = [\"Pf\\n\", f\"{W} {H}\\n\", \"-1\\n\"]\n        for header in headers:\n            f.write(str.encode(header))\n        array = np.flip(array, axis=0).astype(np.float32)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "writeFlow",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def writeFlow(filename, uv, v=None):\n    \"\"\" Write optical flow to file.\n    If v is None, uv is assumed to contain both u and v channels,\n    stacked in depth.\n    Original code by Deqing Sun, adapted from Daniel Scharstein.\n    \"\"\"\n    nBands = 2\n    if v is None:\n        assert (uv.ndim == 3)\n        assert (uv.shape[2] == 2)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readFlowKITTI",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readFlowKITTI(filename):\n    flow = cv2.imread(filename, cv2.IMREAD_ANYDEPTH | cv2.IMREAD_COLOR)\n    flow = flow[:, :, ::-1].astype(np.float32)\n    flow, valid = flow[:, :, :2], flow[:, :, 2]\n    flow = (flow - 2 ** 15) / 64.0\n    return flow, valid\ndef readDispKITTI(filename):\n    disp = cv2.imread(filename, cv2.IMREAD_ANYDEPTH) / 256.0\n    valid = disp > 0.0\n    return disp, valid",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispKITTI",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispKITTI(filename):\n    disp = cv2.imread(filename, cv2.IMREAD_ANYDEPTH) / 256.0\n    valid = disp > 0.0\n    return disp, valid\ndef readDispVKITTI(filename):\n    depth = cv2.imread(filename, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH)\n    f = 725.0087\n    baseline = 0.532725\n    disp = f * baseline * 100 / depth\n    # magic value = f * baseline * 100 / 65535 = 0.589",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispVKITTI",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispVKITTI(filename):\n    depth = cv2.imread(filename, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH)\n    f = 725.0087\n    baseline = 0.532725\n    disp = f * baseline * 100 / depth\n    # magic value = f * baseline * 100 / 65535 = 0.589\n    valid = disp > 0.59\n    return disp, valid\ndef readDispCarla(filename, max_disp=0.9):\n    import math",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispCarla",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispCarla(filename, max_disp=0.9):\n    import math\n    bgr = cv2.imread(filename)\n    bgr = bgr.astype(np.float32)\n    # Apply (R + G * 256 + B * 256 * 256) / (256 * 256 * 256 - 1).\n    normalized_depth = np.dot(bgr, [65536.0, 256.0, 1.0])\n    normalized_depth /= 16777215.0  # (256.0 * 256.0 * 256.0 - 1.0)\n    far = 1000.0  # max depth in meters.\n    depth = normalized_depth * far\n    valid = normalized_depth < max_disp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispArgoverse",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispArgoverse(filename):\n    \"\"\"\n    The disparity maps are saved as uint16 PNG images. A zero-value (\"0\") indicates that no ground truth exists\n    for that pixel. The true disparity for a pixel can be recovered by first converting the uint16 value to float\n    and then dividing it by 256.\n    Args:\n        filename: Path to the disparity map file;\n    Returns:\n        a tuple (disparity_map, valid),\n        disparity map is an array of shape (H, W) representing a float32 single-channel disparity map.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispSintelStereo",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispSintelStereo(file_name):\n    a = np.array(Image.open(file_name))\n    d_r, d_g, d_b = np.split(a, axis=2, indices_or_sections=3)\n    disp = (d_r * 4 + d_g / (2 ** 6) + d_b / (2 ** 14))[..., 0]\n    mask = np.array(Image.open(file_name.replace('disparities', 'occlusions')))\n    valid = ((mask == 0) & (disp > 0))\n    return disp, valid\n# Method taken from https://research.nvidia.com/sites/default/files/pubs/2018-06_Falling-Things/readme_0.txt\ndef readDispFallingThings(file_name):\n    a = np.array(Image.open(file_name))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispFallingThings",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispFallingThings(file_name):\n    a = np.array(Image.open(file_name))\n    with open('/'.join(file_name.split('/')[:-1] + ['_camera_settings.json']), 'r') as f:\n        intrinsics = json.load(f)\n    fx = intrinsics['camera_settings'][0]['intrinsic_settings']['fx']\n    disp = (fx * 6.0 * 100) / a.astype(np.float32)\n    valid = disp > 0\n    return disp, valid\n# Method taken from https://github.com/castacks/tartanair_tools/blob/master/data_type.md\ndef readDispTartanAir(file_name):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispTartanAir",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispTartanAir(file_name):\n    depth = np.load(file_name)\n    disp = 80.0 / depth\n    valid = disp > 0\n    return disp, valid\ndef readDispMiddlebury(file_name):\n    if basename(file_name) == 'disp0GT.pfm':\n        disp = readPFM(file_name).astype(np.float32)\n        assert len(disp.shape) == 2\n        nocc_pix = file_name.replace('disp0GT.pfm', 'mask0nocc.png')",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispMiddlebury",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispMiddlebury(file_name):\n    if basename(file_name) == 'disp0GT.pfm':\n        disp = readPFM(file_name).astype(np.float32)\n        assert len(disp.shape) == 2\n        nocc_pix = file_name.replace('disp0GT.pfm', 'mask0nocc.png')\n        assert exists(nocc_pix)\n        nocc_pix = imageio.imread(nocc_pix) == 255\n        assert np.any(nocc_pix)\n        return disp, nocc_pix\n    elif basename(file_name) == 'disp0.pfm':",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "writeFlowKITTI",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def writeFlowKITTI(filename, uv):\n    uv = 64.0 * uv + 2 ** 15\n    valid = np.ones([uv.shape[0], uv.shape[1], 1])\n    uv = np.concatenate([uv, valid], axis=-1).astype(np.uint16)\n    cv2.imwrite(filename, uv[..., ::-1])\ndef writeDispKITTI(filename, disp):\n    disp = np.round(disp * 256).astype(np.uint16)\n    cv2.imwrite(filename, disp)\ndef readOcclusionMap(filename):\n    return cv2.imread(filename)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "writeDispKITTI",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def writeDispKITTI(filename, disp):\n    disp = np.round(disp * 256).astype(np.uint16)\n    cv2.imwrite(filename, disp)\ndef readOcclusionMap(filename):\n    return cv2.imread(filename)\ndef read_super_pixel_label(filename):\n    label = cv2.imread(filename, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH).astype(np.int32)\n    return label\ndef read_gen(file_name, pil=False):\n    ext = splitext(file_name)[-1]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readOcclusionMap",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readOcclusionMap(filename):\n    return cv2.imread(filename)\ndef read_super_pixel_label(filename):\n    label = cv2.imread(filename, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH).astype(np.int32)\n    return label\ndef read_gen(file_name, pil=False):\n    ext = splitext(file_name)[-1]\n    if ext == '.png' or ext == '.jpeg' or ext == '.ppm' or ext == '.jpg':\n        return Image.open(file_name)\n    elif ext == '.bin' or ext == '.raw':",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "read_super_pixel_label",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def read_super_pixel_label(filename):\n    label = cv2.imread(filename, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH).astype(np.int32)\n    return label\ndef read_gen(file_name, pil=False):\n    ext = splitext(file_name)[-1]\n    if ext == '.png' or ext == '.jpeg' or ext == '.ppm' or ext == '.jpg':\n        return Image.open(file_name)\n    elif ext == '.bin' or ext == '.raw':\n        return np.load(file_name)\n    elif ext == '.flo':",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "read_gen",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def read_gen(file_name, pil=False):\n    ext = splitext(file_name)[-1]\n    if ext == '.png' or ext == '.jpeg' or ext == '.ppm' or ext == '.jpg':\n        return Image.open(file_name)\n    elif ext == '.bin' or ext == '.raw':\n        return np.load(file_name)\n    elif ext == '.flo':\n        return readFlow(file_name).astype(np.float32)\n    elif ext == '.pfm':\n        flow = readPFM(file_name).astype(np.float32)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "downsample_disp",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def downsample_disp(disp, label, stride=8):\n    ht, wd = disp.shape[-2:]\n    right_pad = (stride - wd % stride) % stride\n    down_pad = (stride - ht % stride) % stride\n    disp = F.pad(disp, pad=(0, right_pad, 0, down_pad), mode='constant', value=0)\n    label = F.pad(label, pad=(0, right_pad, 0, down_pad), mode='constant', value=-1)\n    bs, ht, wd = disp.shape\n    disp = rearrange(disp, 'b (h hs) (w ws) -> (b h w) (hs ws)', hs=stride, ws=stride)\n    disp = disp.contiguous()\n    label = rearrange(label, 'b (h hs) (w ws) -> (b h w) (hs ws)', hs=stride, ws=stride)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "TAG_CHAR",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "TAG_CHAR = np.array([202021.25], np.float32)\ndef readFlow(fn):\n    \"\"\" Read .flo file in Middlebury format\"\"\"\n    # Code adapted from:\n    # http://stackoverflow.com/questions/28013200/reading-middlebury-flow-files-with-python-bytes-array-numpy\n    # WARNING: this will work on little-endian architectures (eg Intel x86) only!\n    # print 'fn = %s'%(fn)\n    with open(fn, 'rb') as f:\n        magic = np.fromfile(f, np.float32, count=1)\n        if 202021.25 != magic:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "_ColorfulFormatter",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "peekOfCode": "class _ColorfulFormatter(logging.Formatter):\n    def __init__(self, *args, **kwargs):\n        self._root_name = kwargs.pop(\"root_name\") + \".\"\n        self._abbrev_name = kwargs.pop(\"abbrev_name\", \"\")\n        if len(self._abbrev_name):\n            self._abbrev_name = self._abbrev_name + \".\"\n        super(_ColorfulFormatter, self).__init__(*args, **kwargs)\n    def formatMessage(self, record):\n        record.name = record.name.replace(self._root_name, self._abbrev_name)\n        log = super(_ColorfulFormatter, self).formatMessage(record)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "setup_logger",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "peekOfCode": "def setup_logger(\n    output=None, distributed_rank=0, *, color=True, name=\"imagenet\", abbrev_name=None\n):\n    \"\"\"\n    Initialize the detectron2 logger and set its verbosity level to \"INFO\".\n    Args:\n        output (str): a file name or a directory to save log. If None, will not save log file.\n            If ends with \".txt\" or \".log\", assumed to be a file name.\n            Otherwise, logs will be saved to `output/log.txt`.\n        name (str): the root module name of this logger",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "log_every_n_seconds",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "peekOfCode": "def log_every_n_seconds(lvl, msg, n=1, *, name=None):\n    \"\"\"\n    Log no more than once per n seconds.\n    Args:\n        lvl (int): the logging level\n        msg (str):\n        n (int):\n        name (str): name of the logger to use. Will use the caller's module by default.\n    \"\"\"\n    caller_module, key = _find_caller()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "_LOG_COUNTER",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "peekOfCode": "_LOG_COUNTER = Counter()\n_LOG_TIMER = {}\ndef log_every_n_seconds(lvl, msg, n=1, *, name=None):\n    \"\"\"\n    Log no more than once per n seconds.\n    Args:\n        lvl (int): the logging level\n        msg (str):\n        n (int):\n        name (str): name of the logger to use. Will use the caller's module by default.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "_LOG_TIMER",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "peekOfCode": "_LOG_TIMER = {}\ndef log_every_n_seconds(lvl, msg, n=1, *, name=None):\n    \"\"\"\n    Log no more than once per n seconds.\n    Args:\n        lvl (int): the logging level\n        msg (str):\n        n (int):\n        name (str): name of the logger to use. Will use the caller's module by default.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "SmoothedValue",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "class SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "class MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "check_path",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def check_path(path):\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)  # explicitly set exist_ok when multi-processing\ndef save_command(save_path, filename='command_train.txt'):\n    check_path(save_path)\n    command = sys.argv\n    save_file = os.path.join(save_path, filename)\n    # Save all training commands when resuming training\n    with open(save_file, 'a') as f:\n        f.write(' '.join(command))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "save_command",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def save_command(save_path, filename='command_train.txt'):\n    check_path(save_path)\n    command = sys.argv\n    save_file = os.path.join(save_path, filename)\n    # Save all training commands when resuming training\n    with open(save_file, 'a') as f:\n        f.write(' '.join(command))\n        f.write('\\n\\n')\ndef save_config(config, filename='config.txt'):\n    check_path(config.checkpoint_dir)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "save_config",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def save_config(config, filename='config.txt'):\n    check_path(config.checkpoint_dir)\n    save_path = os.path.join(config.checkpoint_dir, filename)\n    # save all training config when resuming training\n    with open(save_path, 'a') as f:\n        f.write(config.dump())\n        f.write('\\n\\n')\n@torch.no_grad()\ndef accuracy(output, target):\n    \"\"\"Computes the precision\"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def accuracy(output, target):\n    \"\"\"Computes the precision\"\"\"\n    if target.numel() == 0:\n        return [torch.zeros([], device=output.device)]\n    pred = output > 0.5\n    correct = pred.eq(target > 0.5)\n    batch_size = target.size(0)\n    res = correct.float().sum().mul_(100.0 / batch_size)\n    return res\ndef get_sha():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_sha",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))\n    def _run(command):\n        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n    sha = 'N/A'\n    diff = 'clean'\n    branch = 'N/A'\n    try:\n        sha = _run(['git', 'rev-parse', 'HEAD'])\n        subprocess.check_output(['git', 'diff'], cwd=cwd)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "detect_compute_compatibility",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def detect_compute_compatibility(CUDA_HOME, so_file):\n    try:\n        cuobjdump = os.path.join(CUDA_HOME, \"bin\", \"cuobjdump\")\n        if os.path.isfile(cuobjdump):\n            output = subprocess.check_output(\n                \"'{}' --list-elf '{}'\".format(cuobjdump, so_file), shell=True\n            )\n            output = output.decode(\"utf-8\").strip().split(\"\\n\")\n            arch = []\n            for line in output:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "collect_torch_env",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def collect_torch_env():\n    try:\n        import torch.__config__\n        return torch.__config__.show()\n    except ImportError:\n        # compatible with older versions of pytorch\n        from torch.utils.collect_env import get_pretty_env_info\n        return get_pretty_env_info()\ndef collect_env_info():\n    has_gpu = torch.cuda.is_available()  # true for both CUDA & ROCM",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "collect_env_info",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def collect_env_info():\n    has_gpu = torch.cuda.is_available()  # true for both CUDA & ROCM\n    torch_version = torch.__version__\n    # NOTE that CUDA_HOME/ROCM_HOME could be None even when CUDA runtime libs are functional\n    from torch.utils.cpp_extension import CUDA_HOME, ROCM_HOME\n    has_rocm = False\n    if (getattr(torch.version, \"hip\", None) is not None) and (ROCM_HOME is not None):\n        has_rocm = True\n    has_cuda = has_gpu and (not has_rocm)\n    data = []",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "seed_all_rng",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def seed_all_rng(seed=None):\n    \"\"\"\n    Set the random seed for the RNG in torch, numpy and python.\n    Args:\n        seed (int): if None, will use a strong random seed.\n    \"\"\"\n    if seed is None:\n        seed = (\n            os.getpid()\n            + int(datetime.now().strftime(\"%S%f\"))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "__torchvision_need_compat_flag",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "__torchvision_need_compat_flag = float(torchvision.__version__.split('.')[1]) < 7\nif __torchvision_need_compat_flag:\n    from torchvision.ops import _new_empty_tensor\n    from torchvision.ops.misc import _output_size\ndef check_path(path):\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)  # explicitly set exist_ok when multi-processing\ndef save_command(save_path, filename='command_train.txt'):\n    check_path(save_path)\n    command = sys.argv",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "VisImage",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "class VisImage:\n    def __init__(self, img, scale=1.0):\n        \"\"\"\n        Args:\n            img (ndarray): an RGB image of shape (H, W, 3) in range[0, 255].\n            scale (float): scale the input image\n        \"\"\"\n        self.img = img\n        self.scale = scale\n        self.width, self.height = img.shape[1], img.shape[0]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "Visualizer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "class Visualizer:\n    \"\"\"\n    Visualizer that draws data about disparity on images.\n    It contains methods like `draw_{uncertainty,disp,normal,error}`\n    that draws primitive objects to images in some pre-defined style.\n    This visualizer focuses on high rendering quality rather than performance. It is not\n    designed to be used for real-time applications.\n    \"\"\"\n    def __init__(self, img_rgb, scale=1.0):\n        \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "plot_disparity",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "def plot_disparity(savename, data, max_disp):\n    plt.imsave(savename, data, vmin=0, vmax=max_disp, cmap='turbo')\ndef plot_gradient_map(savename, data):\n    data = (data + 1.0) / 2\n    data = 255 * data\n    data = data.astype(np.uint8)\n    plt.imsave(savename, data)\ndef gen_error_colormap():\n    cols = np.array(\n        [[0 / 3.0, 0.1875 / 3.0, 49, 54, 149],",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "plot_gradient_map",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "def plot_gradient_map(savename, data):\n    data = (data + 1.0) / 2\n    data = 255 * data\n    data = data.astype(np.uint8)\n    plt.imsave(savename, data)\ndef gen_error_colormap():\n    cols = np.array(\n        [[0 / 3.0, 0.1875 / 3.0, 49, 54, 149],\n         [0.1875 / 3.0, 0.375 / 3.0, 69, 117, 180],\n         [0.375 / 3.0, 0.75 / 3.0, 116, 173, 209],",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "gen_error_colormap",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "def gen_error_colormap():\n    cols = np.array(\n        [[0 / 3.0, 0.1875 / 3.0, 49, 54, 149],\n         [0.1875 / 3.0, 0.375 / 3.0, 69, 117, 180],\n         [0.375 / 3.0, 0.75 / 3.0, 116, 173, 209],\n         [0.75 / 3.0, 1.5 / 3.0, 171, 217, 233],\n         [1.5 / 3.0, 3 / 3.0, 224, 243, 248],\n         [3 / 3.0, 6 / 3.0, 254, 224, 144],\n         [6 / 3.0, 12 / 3.0, 253, 174, 97],\n         [12 / 3.0, 24 / 3.0, 244, 109, 67],",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "disp_error_img",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "def disp_error_img(save_name, pred, gt, abs_thres=3., rel_thres=0.05):\n    pred_np = pred.detach().cpu().numpy()\n    gt_np = gt.detach().cpu().numpy()\n    H, W = pred_np.shape\n    # valid mask\n    mask = gt_np > 0\n    # error in percentage. When error <= 1, the pixel is valid since <= 3px & 5%\n    error = np.abs(gt_np - pred_np)\n    error[np.logical_not(mask)] = 0\n    error[mask] = np.minimum(error[mask] / abs_thres, (error[mask] / gt_np[mask]) / rel_thres)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "gen_kitti_cmap",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "def gen_kitti_cmap():\n    map = np.array([[0, 0, 0, 114],\n                    [0, 0, 1, 185],\n                    [1, 0, 0, 114],\n                    [1, 0, 1, 174],\n                    [0, 1, 0, 114],\n                    [0, 1, 1, 185],\n                    [1, 1, 0, 114],\n                    [1, 1, 1, 0]])\n    bins = map[:-1, 3]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "DPN",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.DPN",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.DPN",
        "peekOfCode": "class DPN(nn.Module):\n    \"\"\"Disparity proposal seed extraction network.\n    Args:\n        cost_group (int): group number of groupwise cost volume\n        num_proposals (int): number of proposals for each pixel\n        feat_dim (int): dimension of backbone feature map\n        context_dim (int): dimension of visual context\n        prop_embed_dim (int): dimension of label seed embedding\n        split_size (int): width of stripe\n        prop_n_heads: head of attention",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.DPN",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class MLP(nn.Module):\n    \"\"\" Very simple multi-layer perception (also called FFN)\"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "BasicAttention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class BasicAttention(nn.Module):\n    \"\"\"\n    label representation:  [B, N, C]\n    \"\"\"\n    def __init__(self, dim, qk_dim, num_heads=8, attn_drop=0., proj_drop=0., drop_path=0., dropout=0., normalize_before=False):\n        super().__init__()\n        assert dim % num_heads == 0, f'dim {dim} should be multiple times of heads {num_heads}'\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class WindowAttention(nn.Module):\n    \"\"\" Window based multi-head positional sensitive self attention (W-MSA).\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        shift_size (int): Shift size for SW-MSA.\n        num_heads (int): Number of attention heads.\n        qk_scale (float | None, optional): Override a default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "SwinNMP",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class SwinNMP(nn.Module):\n    r\"\"\"Swin Message Passing Block.\n    Args:\n        dim (int): Number of input channels.\n        qkv_dim (int): Number of input token channels\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "CSWinAttention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class CSWinAttention(nn.Module):\n    def __init__(self, dim, resolution, idx, split_size=7, num_heads=8, qk_scale=None, attn_drop=0.):\n        \"\"\"Attention within cross-shaped windows.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.resolution = resolution\n        self.split_size = split_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "CSWinNMP",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class CSWinNMP(nn.Module):\n    def __init__(self, dim, qk_dim, v_dim, patches_resolution, num_heads,\n                 split_size=7, mlp_ratio=4., qk_scale=None,\n                 attn_drop=0., proj_drop=0., drop_path=0., dropout=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, normalize_before=False):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.patches_resolution = patches_resolution\n        self.split_size = split_size",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "Propagation",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class Propagation(nn.Module):\n    \"\"\"Label seed propagation\"\"\"\n    def __init__(self, embed_dim, cost_group, layers, norm=None, return_intermediate=False):\n        super().__init__()\n        self.cost_encoder = nn.Sequential(\n            nn.Linear(cost_group*9, embed_dim),\n            nn.GELU(),\n            nn.Linear(embed_dim, embed_dim),\n        )\n        self.proj = nn.Linear(embed_dim+31, embed_dim, bias=False)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "Inference",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class Inference(nn.Module):\n    \"\"\"Neural MRF Inference\"\"\"\n    def __init__(self, cost_group, dim, layers, norm, return_intermediate=False):\n        super().__init__()\n        self.ffn = Mlp(dim+cost_group, dim, dim)\n        self.dim = dim\n        self.layers = layers\n        self.norm = norm\n        self.cost_group = cost_group\n        self.return_intermediate = return_intermediate",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "Refinement",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class Refinement(Inference):\n    @staticmethod\n    def gen_shift_window_attn_mask(input_resolution, window_size, shift_size, device=torch.device('cuda')):\n        \"\"\"\n        input_resolution (tuple[int]): The height and width of input\n        window_size (tuple[int]): The height, width and depth of window\n        shift_size (int): Shift size for SW-MSA.\n        \"\"\"\n        H, W = input_resolution\n        img_mask = torch.zeros((1, H, W, 1), device=device)  # 1 H W 1",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "PropagationLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class PropagationLayer(nn.Module):\n    def __init__(self, embed_dim, mlp_ratio, context_dim, split_size, n_heads,\n                 activation=\"gelu\", attn_drop=0., proj_drop=0., drop_path=0., dropout=0., normalize_before=False):\n        super().__init__()\n        # self attention\n        act_layer = _get_activation_fn(activation)\n        # concat seed embedding with visual context when linearly projecting to\n        # query and key since visually similar pixel tends to have coherent disparities\n        qk_dim = embed_dim + context_dim\n        v_dim = embed_dim",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "InferenceLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class InferenceLayer(nn.Module):\n    def __init__(self, embed_dim, mlp_ratio, window_size, shift_size, n_heads,\n                 activation=\"gelu\", attn_drop=0., proj_drop=0., drop_path=0., dropout=0., normalize_before=False):\n        super().__init__()\n        # self attention\n        act_layer = _get_activation_fn(activation)\n        qk_dim = embed_dim + 31\n        self.window_size = window_size\n        self.shift_size = shift_size\n        # attend to proposals of the same pixel to suppress non-accurate proposals",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "RefinementLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class RefinementLayer(nn.Module):\n    def __init__(self, dim, mlp_ratio, window_size, shift_size, n_heads,\n                 activation=\"gelu\", attn_drop=0., proj_drop=0., drop_path=0., dropout=0., normalize_before=False):\n        super().__init__()\n        act_layer = _get_activation_fn(activation)\n        qk_dim = dim + 31\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.nmp = SwinNMP(dim, qk_dim, num_heads=n_heads, window_size=window_size,\n                           shift_size=shift_size, mlp_ratio=mlp_ratio, attn_drop=attn_drop,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "fourier_grid_embed",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "def fourier_grid_embed(data, embed_dim):\n    \"\"\"data format: B[spatial dims]C\n    Returns pos_embedding: same format with data\n    \"\"\"\n    b, *axis, _, device, dtype = *data.shape, data.device, data.dtype\n    assert embed_dim % (2 * len(axis)) == 0\n    # calculate fourier encoded positions in the range of [-1, 1], for all axis\n    axis_pos = list(map(lambda size: torch.linspace(-1., 1., steps=size, device=device, dtype=dtype), axis))\n    pos = torch.stack(torch.meshgrid(*axis_pos, indexing='ij'), dim=-1)\n    num_bands = embed_dim // (2 * len(axis))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "fourier_coord_embed",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "def fourier_coord_embed(coord, N_freqs, normalizer=3.14/512, logscale=True):\n    \"\"\"\n    coord: [...]D\n    returns:\n        [...]dim, where dim=(2*N_freqs+1)*D\n    \"\"\"\n    if logscale:\n        freq_bands = 2**torch.linspace(0, N_freqs-1, N_freqs, device=coord.device)\n    else:\n        freq_bands = torch.linspace(1, 2**(N_freqs-1), N_freqs)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "NMRF",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMRF",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMRF",
        "peekOfCode": "class NMRF(nn.Module):\n    @configurable\n    def __init__(self,\n                 cfgs,\n                 num_proposals=4,\n                 max_disp=320,\n                 num_infer_layers=5,\n                 num_refine_layers=5,\n                 infer_embed_dim=128,\n                 infer_n_heads=4,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMRF",
        "documentation": {}
    },
    {
        "label": "Criterion",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMRF",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMRF",
        "peekOfCode": "class Criterion(nn.Module):\n    \"\"\" This class computes the loss for disparity proposal extraction.\n    The process happens in two steps:\n        1) we compute a one-to-one matching between ground truth disparities and the outputs of the model\n        2) we supervise each output to be closer to the ground truth disparity it was matched to\n    Note: to avoid trivial solution, we add a prior term in the loss computation that we favor positive output.\n    \"\"\"\n    def __init__(self, weight_dict, max_disp, loss_type):\n        \"\"\" Create the criterion.\n        Parameters:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMRF",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMRF",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMRF",
        "peekOfCode": "def build(cfg):\n    model = NMRF(cfg)\n    weight_dict = {'proposal_disp': 1, 'init': 1}\n    assert len(cfg.SOLVER.LOSS_WEIGHTS) == cfg.NMP.NUM_INFER_LAYERS + cfg.NMP.NUM_REFINE_LAYERS\n    if cfg.SOLVER.AUX_LOSS:\n        aux_weight_dict = {}\n        for i in range(cfg.NMP.NUM_INFER_LAYERS + cfg.NMP.NUM_REFINE_LAYERS-1):\n            if i < cfg.NMP.NUM_INFER_LAYERS:\n                aux_weight_dict.update({f'loss_coarse_disp_{i}': cfg.SOLVER.LOSS_WEIGHTS[i]})\n            else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.NMRF",
        "documentation": {}
    },
    {
        "label": "ConvFFN",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "class ConvFFN(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None,\n                 act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "DWConv",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "class DWConv(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, H, W).contiguous()\n        x = self.dwconv(x).flatten(2).transpose(1, 2)\n        return x\nclass Extractor(nn.Module):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "Extractor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "class Extractor(nn.Module):\n    def __init__(self, dim, num_heads=8, n_points=4, n_levels=1, deform_ratio=1.0,\n                 with_cffn=True, cffn_ratio=0.25, drop=0., drop_path=0.,\n                 norm_layer=partial(nn.LayerNorm, eps=1e-6), with_cp=False):\n        super().__init__()\n        self.query_norm = norm_layer(dim)\n        self.feat_norm = norm_layer(dim)\n        self.attn = MSDeformAttn(d_model=dim, n_levels=n_levels, n_heads=num_heads,\n                                 n_points=n_points, ratio=deform_ratio)\n        self.with_cffn = with_cffn",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "ConvStem",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "class ConvStem(nn.Module):\n    def __init__(self, inplanes=64, out_channels=256, norm_layer=nn.InstanceNorm2d, with_cp=False):\n        super().__init__()\n        self.with_cp = with_cp\n        self.stem = nn.Sequential(*[\n            nn.Conv2d(3, inplanes, kernel_size=3, stride=2, padding=1, bias=False),\n            norm_layer(inplanes),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(inplanes, inplanes, kernel_size=3, stride=1, padding=1, bias=False),\n            norm_layer(inplanes),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "DeformNeck",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "class DeformNeck(nn.Module):\n    def __init__(self, dim, in_channel_list, num_heads=8, n_points=4,\n                 norm_layer=partial(nn.LayerNorm, eps=1e-6), drop=0., drop_path=0.,\n                 with_cffn=True, cffn_ratio=0.25, deform_ratio=1.0, with_cp=False):\n        super().__init__()\n        self.stem = ConvStem(inplanes=64, out_channels=dim)\n        self.dim = dim\n        self.extractors = nn.ModuleList([\n            Extractor(dim=dim, n_levels=1, num_heads=num_heads, n_points=n_points,\n                      norm_layer=norm_layer, deform_ratio=deform_ratio, with_cffn=with_cffn,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "get_reference_points",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "def get_reference_points(spatial_shapes, device):\n    reference_points_list = []\n    for lvl, (H_, W_) in enumerate(spatial_shapes):\n        ref_y, ref_x = torch.meshgrid(\n            torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n            torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n        ref_y = ref_y.reshape(-1)[None] / H_\n        ref_x = ref_x.reshape(-1)[None] / W_\n        ref = torch.stack((ref_x, ref_y), -1)\n        reference_points_list.append(ref)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "deform_inputs_dn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "def deform_inputs_dn(x):\n    bs, c, h, w = x.shape\n    spatial_shapes = torch.as_tensor([(h // 4, w // 4),\n                                      (h // 8, w // 8),\n                                      (h // 16, w // 16),\n                                      (h // 32, w // 32)],\n                                     dtype=torch.long, device=x.device)\n    level_start_index = spatial_shapes.new_zeros((1,))\n    reference_points = get_reference_points([(h // 4, w // 4)], x.device)\n    return reference_points, spatial_shapes, level_start_index",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_layer=nn.InstanceNorm2d, stride=1, dilation=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n                               dilation=dilation, padding=dilation, stride=stride, bias=False)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               dilation=dilation, padding=dilation, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.norm1 = norm_layer(planes)\n        self.norm2 = norm_layer(planes)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "peekOfCode": "class Backbone(nn.Module):\n    def __init__(self, output_dim=128, norm_layer=nn.InstanceNorm2d):\n        super(Backbone, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) # 1/2\n        self.norm1 = norm_layer(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.in_planes = 64\n        self.layer1 = self._make_layer(64, stride=1, norm_layer=norm_layer) # 1/2\n        self.layer2 = self._make_layer(96, stride=2, norm_layer=norm_layer) # 1/4\n        self.layer3 = self._make_layer(128, stride=1, norm_layer=norm_layer) # 1/4",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "documentation": {}
    },
    {
        "label": "SwinAdaptor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "peekOfCode": "class SwinAdaptor(nn.Module):\n    def __init__(self, out_channels, drop_path_rate=0.):\n        super().__init__()\n        self.backbone = SwinTransformer(\n            depths=(2, 2, 6, 2),\n            drop_path_rate=drop_path_rate,\n            embed_dim=96,\n            num_heads=(3, 6, 12, 24),\n        )\n        self.neck = DeformNeck(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "documentation": {}
    },
    {
        "label": "checkpoint_filter_fn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "peekOfCode": "def checkpoint_filter_fn(state_dict):\n    out_dict = {}\n    state_dict = state_dict.get('model', state_dict)\n    state_dict = state_dict.get('state_dict', state_dict)\n    for k, v in state_dict.items():\n        if \"attn_mask\" in k:\n            continue  # skip buffers that should not be persistent\n        if any([k.startswith(n) for n in ('norm', 'head')]):\n            continue\n        out_dict[k] = v",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "documentation": {}
    },
    {
        "label": "create_backbone",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "peekOfCode": "def create_backbone(model_type, norm_fn, out_channels, drop_path):\n    model_type = model_type\n    if model_type == \"resnet\":\n        if norm_fn == \"instance\":\n            norm_layer = nn.InstanceNorm2d\n        elif norm_fn == 'batch':\n            norm_layer = nn.BatchNorm2d\n        else:\n            raise ValueError(f'Invalid backbone normalization type: {norm_fn}')\n        backbone = Backbone(out_channels, norm_layer)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.backbone",
        "documentation": {}
    },
    {
        "label": "for_compatibility",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.build_optimizer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.build_optimizer",
        "peekOfCode": "def for_compatibility(model):\n    return model\ndef build_optimizer(params, base_lr):\n    model = params.module\n    base_lr = base_lr\n    backbone_lr_decay = 0.1\n    backbone_weight_decay = 1e-05\n    weight_decay_norm = 1e-05\n    norm_module_types = (\n        torch.nn.BatchNorm2d,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.build_optimizer",
        "documentation": {}
    },
    {
        "label": "build_optimizer",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.build_optimizer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.build_optimizer",
        "peekOfCode": "def build_optimizer(params, base_lr):\n    model = params.module\n    base_lr = base_lr\n    backbone_lr_decay = 0.1\n    backbone_weight_decay = 1e-05\n    weight_decay_norm = 1e-05\n    norm_module_types = (\n        torch.nn.BatchNorm2d,\n        torch.nn.InstanceNorm2d,\n        torch.nn.LayerNorm,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.build_optimizer",
        "documentation": {}
    },
    {
        "label": "MemoryEfficientCrossAttention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "peekOfCode": "class MemoryEfficientCrossAttention(nn.Module):\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0, qkv_bias=False):\n        super().__init__()\n        inner_dim = dim_head * heads\n        self.heads = heads\n        self.dim_head = dim_head\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=qkv_bias)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=qkv_bias)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=qkv_bias)\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "documentation": {}
    },
    {
        "label": "FPNLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "peekOfCode": "class FPNLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, high_dim):\n        super(FPNLayer, self).__init__()\n        self.deconv = nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n                                    nn.BatchNorm2d(out_channels),\n                                    nn.ReLU())\n        self.conv = nn.Sequential(nn.Conv2d(out_channels + high_dim, out_channels, kernel_size=3, padding=1, bias=False),\n                                  nn.BatchNorm2d(out_channels),\n                                  nn.ReLU())\n    def forward(self, low, high):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "documentation": {}
    },
    {
        "label": "ResidualConvUnit",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "peekOfCode": "class ResidualConvUnit(nn.Module):\n    def __init__(self, features, activation, bn):\n        super().__init__()\n        self.bn = bn\n        self.groups = 1\n        self.conv1 = nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1, bias=True, groups=self.groups)\n        self.conv2 = nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1, bias=True, groups=self.groups)\n        if self.bn:\n            self.bn1 = nn.BatchNorm2d(features)\n            self.bn2 = nn.BatchNorm2d(features)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "peekOfCode": "class FeatureFusionBlock(nn.Module):\n    def __init__(\n            self,\n            features,\n            activation,\n            deconv=False,\n            bn=False,\n            expand=False,\n            align_corners=True,\n            size=None",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "documentation": {}
    },
    {
        "label": "Hybrid",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "peekOfCode": "class Hybrid(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.vit_backbone = DINOv2(model_name='vits')\n        state_dict = torch.hub.load_state_dict_from_url('dinov2_vits14_pretrain.pth', map_location=\"cpu\")\n        self.vit_backbone.load_state_dict(state_dict, strict=True)\n        self.projects = nn.ModuleList([\n            nn.Conv2d(in_channels=384, out_channels=128, kernel_size=1) for _ in range(4)\n        ])\n        self.resize_layers = nn.ModuleList([",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.hybrid_backbone",
        "documentation": {}
    },
    {
        "label": "NearestMatcher",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.matcher",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.matcher",
        "peekOfCode": "class NearestMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n    The targets include null (=0) disparity. We do a many-to-one matching and identify the best predictions\n    among the ones assigned to a same target.\n    \"\"\"\n    def __init__(self, cost_class: float = 1, cost_disp: float = 1):\n        \"\"\"Create the matcher\n        Params:\n            cost_class: This is the relative weight of the classification error in the matching cost\n            cost_disp: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.matcher",
        "documentation": {}
    },
    {
        "label": "bf_match",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.matcher",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.matcher",
        "peekOfCode": "def bf_match(outputs, targets, num=4):\n    \"\"\" This function computes an assignment between the targets and the predictions of the network.\n    The targets include null (=0) disparity. We do a 1-to-1 matching between non-null targets and best predictions,\n    while the others are un-matched (and thus not propagate gradients).\n    outputs: This is a tensor of dim [batch_size*H*W, num_queries]\n    targets: This is a tensor of dim [batch_size*H*W, 4] containing the target disparities\n    Returns:\n        index tensor of dim [batch_size*H*W, 4], where ith element of each row denotes the index of matched proposal\n            with ith ground truth.\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.matcher",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super().__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(inplanes, planes, 3, stride, padding=dilation if dilation > 1 else pad, dilation=dilation), nn.ReLU(inplace=True))\n        self.conv2 = nn.Conv2d(planes, planes, 3, 1, padding=dilation if dilation > 1 else pad, dilation=dilation)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):\n        out = self.conv1(x)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_correlation_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.submodule",
        "documentation": {}
    },
    {
        "label": "build_correlation_volume",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.submodule",
        "peekOfCode": "def build_correlation_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.submodule",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n    def __init__(\n        self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class WindowAttention(nn.Module):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value.\n            Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "SwinTransformerBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class SwinTransformerBlock(nn.Module):\n    \"\"\"Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class PatchMerging(nn.Module):\n    \"\"\"Patch Merging Layer\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "BasicLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (int): Local window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\n    Args:\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "SwinTransformer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class SwinTransformer(nn.Module):\n    \"\"\"Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted\n            Windows`  - https://arxiv.org/pdf/2103.14030\n    Args:\n        pretrain_img_size (int): Input image size for training the pretrained model,\n            used in absolute postion embedding. Default 224.\n        patch_size (int | tuple(int)): Patch size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "peekOfCode": "def window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "_to_2tuple",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "peekOfCode": "_to_2tuple = nn.modules.utils._ntuple(2)\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n    def __init__(\n        self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)\n        if self.args.run_mode == 'train':\n            self.optimizer = self.build_optimizer(self.model, cfgs.OPTIMIZATION.OPTIMIZER.LR)\n            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=self.optimizer, max_lr=cfgs.OPTIMIZATION.OPTIMIZER.LR,\n                                                                 total_steps=self.max_iter, pct_start=0.05,\n                                                                 cycle_momentum=False, anneal_strategy='cos')\n    def build_optimizer(self, params, base_lr):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.trainer",
        "peekOfCode": "__all__ = {\n    'NMRF': NMRF,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)\n        if self.args.run_mode == 'train':\n            self.optimizer = self.build_optimizer(self.model, cfgs.OPTIMIZATION.OPTIMIZER.LR)\n            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=self.optimizer, max_lr=cfgs.OPTIMIZATION.OPTIMIZER.LR,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.nmrf.trainer",
        "documentation": {}
    },
    {
        "label": "PSMNet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet",
        "peekOfCode": "class PSMNet(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.Backbone = PSMNetBackbone()\n        self.CostProcessor = PSMCostProcessor(max_disp=self.maxdisp)\n        self.DispProcessor = PSMDispProcessor(max_disp=self.maxdisp)\n    def forward(self, inputs):\n        \"\"\"Forward the network.\"\"\"\n        backbone_out = self.Backbone(inputs)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet",
        "documentation": {}
    },
    {
        "label": "PSMNet",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_backbone",
        "peekOfCode": "class PSMNet(nn.Module):\n    \"\"\"\n    Backbone proposed in PSMNet.\n    Args:\n        in_planes (int): the channels of input\n        batch_norm (bool): whether use batch normalization layer, default True\n    Inputs:\n        l_img (Tensor): left image, in [BatchSize, 3, Height, Width] layout\n        r_img (Tensor): right image, in [BatchSize, 3, Height, Width] layout\n    Outputs:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_backbone",
        "documentation": {}
    },
    {
        "label": "Hourglass",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "peekOfCode": "class Hourglass(nn.Module):\n    \"\"\"\n    An implementation of hourglass module proposed in PSMNet.\n    Args:\n        in_planes (int): the channels of raw cost volume\n        batch_norm (bool): whether use batch normalization layer,\n            default True\n    Inputs:\n        x, (Tensor): cost volume\n            in [BatchSize, in_planes, MaxDisparity, Height, Width] layout",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "PSMAggregator",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "peekOfCode": "class PSMAggregator(nn.Module):\n    \"\"\"\n    Args:\n        max_disp (int): max disparity\n        in_planes (int): the channels of raw cost volume\n        batch_norm (bool): whether use batch normalization layer, default True\n    Inputs:\n        raw_cost (Tensor): concatenation-based cost volume without further processing,\n            in [BatchSize, in_planes, MaxDisparity//4, Height//4, Width//4] layout\n    Outputs:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "PSMCostProcessor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "peekOfCode": "class PSMCostProcessor(nn.Module):\n    def __init__(self, max_disp=192, in_planes=64):\n        super().__init__()\n        self.cat_func = partial(\n            cat_fms,\n            max_disp=int(max_disp // 4),\n            start_disp=0,\n            dilation=1,\n        )\n        self.aggregator = PSMAggregator(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "cat_fms",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "peekOfCode": "def cat_fms(reference_fm, target_fm, max_disp=192, start_disp=0, dilation=1):\n    \"\"\"\n    Concat left and right in Channel dimension to form the raw cost volume.\n    Args:\n        max_disp, (int): under the scale of feature used,\n            often equals to (end disp - start disp + 1), the maximum searching range of disparity\n        start_disp (int): the start searching disparity index, usually be 0\n            dilation (int): the step between near disparity index\n        dilation (int): the step between near disparity index\n    Inputs:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "FasterSoftArgmin",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_disp_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_disp_processor",
        "peekOfCode": "class FasterSoftArgmin(nn.Module):\n    \"\"\"\n    A faster implementation of soft argmin.\n    details can refer to dmb.modeling.stereo.disp_predictors.soft_argmin\n    Args:\n        max_disp, (int): under the scale of feature used,\n            often equals to (end disp - start disp + 1), the maximum searching range of disparity\n        start_disp (int): the start searching disparity index, usually be 0\n        dilation (optional, int): the step between near disparity index\n        alpha (float or int): a factor will times with cost_volume",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_disp_processor",
        "documentation": {}
    },
    {
        "label": "PSMDispProcessor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_disp_processor",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_disp_processor",
        "peekOfCode": "class PSMDispProcessor(nn.Module):\n    def __init__(self, max_disp=192):\n        super().__init__()\n        self.disp_processor = FasterSoftArgmin(\n            # the maximum disparity of disparity search range\n            max_disp=max_disp,\n            # the start disparity of disparity search range\n            start_disp=0,\n            # the step between near disparity sample\n            dilation=1,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.psmnet_disp_processor",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, batchNorm, in_planes, out_planes, stride, downsample, padding, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv_bn_relu(\n            batchNorm=batchNorm, in_planes=in_planes, out_planes=out_planes,\n            kernel_size=3, stride=stride, padding=padding, dilation=dilation, bias=False\n        )\n        self.conv2 = conv_bn(\n            batchNorm=batchNorm, in_planes=out_planes, out_planes=out_planes,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "consistent_padding_with_dilation",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def consistent_padding_with_dilation(padding, dilation, dim=2):\n    assert dim == 2 or dim == 3, 'Convolution layer only support 2D and 3D'\n    if dim == 2:\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n    else:  # dim == 3\n        padding = _triple(padding)\n        dilation = _triple(dilation)\n    padding = list(padding)\n    for d in range(dim):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv_bn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def conv_bn(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=2)\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv2d(\n                in_planes, out_planes, kernel_size=kernel_size,\n                stride=stride, padding=padding, dilation=dilation, bias=bias\n            ),\n            nn.BatchNorm2d(out_planes),\n        )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "deconv_bn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def deconv_bn(batchNorm, in_planes, out_planes, kernel_size=4, stride=2, padding=1, output_padding=0, bias=True):\n    if batchNorm:\n        return nn.Sequential(\n            nn.ConvTranspose2d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, output_padding=output_padding, bias=bias\n            ),\n            nn.BatchNorm2d(out_planes),\n        )\n    else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv3d_bn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def conv3d_bn(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=3)\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv3d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, dilation=dilation, bias=bias),\n            nn.BatchNorm3d(out_planes),\n        )\n    else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "deconv3d_bn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def deconv3d_bn(batchNorm, in_planes, out_planes, kernel_size=4, stride=2, padding=1, output_padding=0, bias=True):\n    if batchNorm:\n        return nn.Sequential(\n            nn.ConvTranspose3d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, output_padding=output_padding, bias=bias),\n            nn.BatchNorm3d(out_planes),\n        )\n    else:\n        return nn.Sequential(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv_bn_relu",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def conv_bn_relu(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=2)\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv2d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, dilation=dilation, bias=bias),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU(inplace=True),\n        )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "bn_relu_conv",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def bn_relu_conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=2)\n    if batchNorm:\n        return nn.Sequential(\n            nn.BatchNorm2d(in_planes),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, dilation=dilation, bias=bias),\n        )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "deconv_bn_relu",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def deconv_bn_relu(batchNorm, in_planes, out_planes, kernel_size=4, stride=2, padding=1, output_padding=0, bias=True):\n    if batchNorm:\n        return nn.Sequential(\n            nn.ConvTranspose2d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, output_padding=output_padding, bias=bias),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU(inplace=True),\n        )\n    else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv3d_bn_relu",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def conv3d_bn_relu(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=3)\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv3d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, dilation=dilation, bias=bias),\n            nn.BatchNorm3d(out_planes),\n            nn.ReLU(inplace=True),\n        )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "bn_relu_conv3d",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def bn_relu_conv3d(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=3)\n    if batchNorm:\n        return nn.Sequential(\n            nn.BatchNorm3d(in_planes),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, dilation=dilation, bias=bias),\n        )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "deconv3d_bn_relu",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def deconv3d_bn_relu(batchNorm, in_planes, out_planes, kernel_size=4, stride=2, padding=1, output_padding=0, bias=True):\n    if batchNorm:\n        return nn.Sequential(\n            nn.ConvTranspose3d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, output_padding=output_padding, bias=bias),\n            nn.BatchNorm3d(out_planes),\n            nn.ReLU(inplace=True),\n        )\n    else:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "__all__ = ['conv_bn', 'deconv_bn',\n           'conv_bn_relu', 'bn_relu_conv', 'deconv_bn_relu',\n           'conv3d_bn', 'deconv3d_bn',\n           'conv3d_bn_relu', 'bn_relu_conv3d', 'deconv3d_bn_relu',\n           'BasicBlock',\n           ]\ndef consistent_padding_with_dilation(padding, dilation, dim=2):\n    assert dim == 2 or dim == 3, 'Convolution layer only support 2D and 3D'\n    if dim == 2:\n        padding = _pair(padding)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.trainer",
        "peekOfCode": "__all__ = {\n    'PSMNet': PSMNet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.psmnet.trainer",
        "documentation": {}
    },
    {
        "label": "SubModule",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.backbone",
        "peekOfCode": "class SubModule(nn.Module):\n    def __init__(self):\n        super(SubModule, self).__init__()\n    def weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.backbone",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.backbone",
        "peekOfCode": "class Feature(SubModule):\n    def __init__(self):\n        super(Feature, self).__init__()\n        model = timm.create_model('mobilenetv2_100', pretrained=True, features_only=True)\n        layers = [1, 2, 3, 5, 6]\n        chans = [16, 24, 32, 96, 160]\n        self.conv_stem = model.conv_stem\n        self.bn1 = model.bn1\n        self.act1 = model.act1\n        self.block0 = torch.nn.Sequential(*model.blocks[0:layers[0]])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.backbone",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "MultiBasicEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class MultiBasicEncoder(nn.Module):\n    def __init__(self, output_dim=None, norm_fn='batch', downsample=3):\n        super(MultiBasicEncoder, self).__init__()\n        if output_dim is None:\n            output_dim = [128]\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1 + (downsample > 2), padding=3)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.norm_fn = norm_fn\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "CombinedGeoEncodingVolume",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class CombinedGeoEncodingVolume:\n    def __init__(self, init_fmap1, init_fmap2, geo_volume, num_levels=2, radius=4):\n        self.num_levels = num_levels\n        self.radius = radius\n        self.geo_volume_pyramid = []\n        self.init_corr_pyramid = []\n        # all pairs correlation\n        init_corr = self.corr(init_fmap1, init_fmap2)  # [bz, H/4, W/4, 1, W/4]\n        b, h, w, _, w2 = init_corr.shape\n        b, c, d, h, w = geo_volume.shape  # [bz, channel, maxdisp/4, H/4, W/4]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "BasicMotionEncoder",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class BasicMotionEncoder(nn.Module):\n    def __init__(self, corr_levels, corr_radius, volume_channel):\n        super(BasicMotionEncoder, self).__init__()\n        cor_planes = corr_levels * (2 * corr_radius + 1) * (volume_channel + 1)\n        self.convc1 = nn.Conv2d(cor_planes, 64, 1, padding=0)\n        self.convc2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.convd1 = nn.Conv2d(1, 64, 7, padding=3)\n        self.convd2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.conv = nn.Conv2d(64 + 64, 128 - 1, 3, padding=1)\n    def forward(self, disp, corr):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "ConvGRU",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class ConvGRU(nn.Module):\n    def __init__(self, hidden_dim, input_dim, kernel_size=3):\n        super(ConvGRU, self).__init__()\n        self.convz = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convq = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n    def forward(self, h, cz, cr, cq, *x_list):\n        x = torch.cat(x_list, dim=1)\n        hx = torch.cat([h, x], dim=1)\n        z = torch.sigmoid(self.convz(hx) + cz)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "DispHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):\n        super(DispHead, self).__init__()\n        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 3, padding=1)\n        self.conv2 = nn.Conv2d(hidden_dim, output_dim, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\ndef pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "BasicMultiUpdateBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []\n        self.n_gru_layers = n_gru_layers\n        self.encoder = BasicMotionEncoder(corr_levels, corr_radius, volume_channel)\n        encoder_output_dim = 128\n        self.gru04 = ConvGRU(hidden_dims[2], encoder_output_dim + hidden_dims[1] * (self.n_gru_layers > 1))\n        self.gru08 = ConvGRU(hidden_dims[1], hidden_dims[0] * (self.n_gru_layers == 3) + hidden_dims[2])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "def bilinear_sampler(img, coords, mask=False):\n    \"\"\" Wrapper for grid_sample, uses pixel coordinates \"\"\"\n    H, W = img.shape[-2:]\n    xgrid, ygrid = coords.split([1, 1], dim=-1)\n    xgrid = 2 * xgrid / (W - 1) - 1\n    assert torch.unique(ygrid).numel() == 1 and H == 1  # This is a stereo problem\n    grid = torch.cat([xgrid, ygrid], dim=-1)\n    img = F.grid_sample(img, grid, align_corners=True)\n    if mask:\n        mask = (xgrid > -1) & (ygrid > -1) & (xgrid < 1) & (ygrid < 1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "pool2x",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "def pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "interp",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "def interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []\n        self.n_gru_layers = n_gru_layers\n        self.encoder = BasicMotionEncoder(corr_levels, corr_radius, volume_channel)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "Hourglass",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.hourglass",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.hourglass",
        "peekOfCode": "class Hourglass(nn.Module):\n    def __init__(self, in_channels, backbone_channels=None):\n        super(Hourglass, self).__init__()\n        if backbone_channels is None:\n            backbone_channels = [48, 64, 192, 120]\n        self.conv1 = nn.Sequential(\n            BasicConv3d(in_channels, in_channels * 2,\n                        norm_layer=nn.BatchNorm3d, act_layer=nn.LeakyReLU,\n                        kernel_size=3, padding=1, stride=2, dilation=1),\n            BasicConv3d(in_channels * 2, in_channels * 2,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.hourglass",
        "documentation": {}
    },
    {
        "label": "Conv2xUp",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.igev_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.igev_blocks",
        "peekOfCode": "class Conv2xUp(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer, concat=True):\n        super(Conv2xUp, self).__init__()\n        self.concat = concat\n        self.conv1 = BasicDeconv2d(in_channels, out_channels,\n                                   norm_layer=norm_layer, act_layer=nn.LeakyReLU,\n                                   kernel_size=4, stride=2, padding=1)\n        self.conv2 = BasicConv2d(out_channels * 2, out_channels * 2,\n                                 norm_layer=norm_layer, act_layer=nn.LeakyReLU,\n                                 kernel_size=3, stride=1, padding=1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.igev_blocks",
        "documentation": {}
    },
    {
        "label": "FeatureAtt",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.igev_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.igev_blocks",
        "peekOfCode": "class FeatureAtt(nn.Module):\n    def __init__(self, cv_chan, feat_chan):\n        super(FeatureAtt, self).__init__()\n        self.feat_att = nn.Sequential(\n            BasicConv2d(feat_chan, feat_chan // 2,\n                        norm_layer=nn.BatchNorm2d, act_layer=nn.LeakyReLU,\n                        kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(feat_chan // 2, cv_chan, 1))\n    def forward(self, cv, feat):\n        feat_att = self.feat_att(feat).unsqueeze(2)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.igev_blocks",
        "documentation": {}
    },
    {
        "label": "context_upsample",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.igev_blocks",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.igev_blocks",
        "peekOfCode": "def context_upsample(disp_low, up_weights, scale_factor=4):\n    ###\n    # disp_low (b,1,h,w)\n    # up_weights (b,9,4*h,4*w)\n    ###\n    b, c, h, w = disp_low.shape\n    disp_unfold = F.unfold(disp_low.reshape(b, c, h, w), 3, 1, 1).reshape(b, -1, h, w)\n    disp_unfold = F.interpolate(disp_unfold, (h * scale_factor, w * scale_factor), mode='nearest').reshape(b, 9, h * scale_factor, w * scale_factor)\n    disp = (disp_unfold * up_weights).sum(1)\n    return disp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.igev_blocks",
        "documentation": {}
    },
    {
        "label": "StereoBase",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.stereobase_gru",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.stereobase_gru",
        "peekOfCode": "class StereoBase(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.cfgs = cfgs\n        self.max_disp = cfgs.MAX_DISP\n        self.num_groups = cfgs.get('NUM_GROUPS', 8)\n        self.use_concat_volume = cfgs.get('USE_CONCAT_VOLUME', False)\n        self.use_gwc_volume = cfgs.get('USE_GWC_VOLUME', True)\n        self.use_sub_volume = cfgs.get('USE_SUB_VOLUME', False)\n        self.use_interlaced_volume = cfgs.get('USE_INTERLACED_VOLUME', False)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.stereobase_gru",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.trainer",
        "peekOfCode": "__all__ = {\n    'StereoBaseGRU': StereoBaseGRU,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.stereobase.trainer",
        "documentation": {}
    },
    {
        "label": "MultiheadAttentionRelative",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.attention",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.attention",
        "peekOfCode": "class MultiheadAttentionRelative(nn.MultiheadAttention):\n    \"\"\"\n    Multihead attention with relative positional encoding\n    \"\"\"\n    def __init__(self, embed_dim, num_heads):\n        super(MultiheadAttentionRelative, self).__init__(embed_dim, num_heads, dropout=0.0, bias=True,\n                                                         add_bias_kv=False, add_zero_attn=False,\n                                                         kdim=None, vdim=None)\n    def forward(self, query, key, value, attn_mask=None, pos_enc=None, pos_indexes=None):\n        \"\"\"",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.attention",
        "documentation": {}
    },
    {
        "label": "ContextAdjustmentLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "peekOfCode": "class ContextAdjustmentLayer(nn.Module):\n    \"\"\"\n    Adjust the disp and occ based on image context, design loosely follows https://github.com/JiahuiYu/wdsr_ntire2018\n    \"\"\"\n    def __init__(self, num_blocks=8, feature_dim=16, expansion=3):\n        super().__init__()\n        self.num_blocks = num_blocks\n        # disp head\n        self.in_conv = nn.Conv2d(4, feature_dim, kernel_size=3, padding=1)\n        self.layers = nn.ModuleList([ResBlock(feature_dim, expansion) for _ in range(num_blocks)])",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "peekOfCode": "class ResBlock(nn.Module):\n    def __init__(self, n_feats: int, expansion_ratio: int, res_scale: int = 1.0):\n        super(ResBlock, self).__init__()\n        self.res_scale = res_scale\n        self.module = nn.Sequential(\n            weight_norm(nn.Conv2d(n_feats + 1, n_feats * expansion_ratio, kernel_size=3, padding=1)),\n            nn.ReLU(inplace=True),\n            weight_norm(nn.Conv2d(n_feats * expansion_ratio, n_feats, kernel_size=3, padding=1))\n        )\n    def forward(self, x: torch.Tensor, disp: torch.Tensor):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "documentation": {}
    },
    {
        "label": "build_context_adjustment_layer",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "peekOfCode": "def build_context_adjustment_layer(args):\n    if args.CONTEXT_ADJUSTMENT_LAYER == 'cal':\n        return ContextAdjustmentLayer(args.CAL_NUM_BLOCKS, args.CAL_FEAT_DIM,\n                                      args.CAL_EXPANSION_RATIO)\n    elif args.CONTEXT_ADJUSTMENT_LAYER == 'none':\n        return None\n    else:\n        raise ValueError(f'Context adjustment layer option not recognized: {args.context_adjustment_layer}')",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "documentation": {}
    },
    {
        "label": "_DenseLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "peekOfCode": "class _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm1', nn.InstanceNorm2d(num_input_features)),\n        self.add_module('relu1', nn.ReLU(inplace=True)),\n        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n                                           growth_rate, kernel_size=1, stride=1,\n                                           bias=False)),\n        self.add_module('norm2', nn.InstanceNorm2d(bn_size * growth_rate)),\n        self.add_module('relu2', nn.ReLU(inplace=True)),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "documentation": {}
    },
    {
        "label": "_DenseBlock",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "peekOfCode": "class _DenseBlock(nn.ModuleDict):\n    _version = 2\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(\n                num_input_features + i * growth_rate,\n                growth_rate=growth_rate,\n                bn_size=bn_size,\n                drop_rate=drop_rate,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "documentation": {}
    },
    {
        "label": "_Transition",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "peekOfCode": "class _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module('norm', nn.InstanceNorm2d(num_input_features))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "peekOfCode": "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\nmodel_urls = {\n    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n}\nclass _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n        super(_DenseLayer, self).__init__()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "documentation": {}
    },
    {
        "label": "model_urls",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "peekOfCode": "model_urls = {\n    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n}\nclass _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm1', nn.InstanceNorm2d(num_input_features)),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.densenet_in",
        "documentation": {}
    },
    {
        "label": "SppBackbone",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "peekOfCode": "class SppBackbone(nn.Module):\n    \"\"\"\n    Contracting path of feature descriptor using Spatial Pyramid Pooling,\n    SPP followed by PSMNet (https://github.com/JiaRenChang/PSMNet)\n    \"\"\"\n    def __init__(self):\n        super(SppBackbone, self).__init__()\n        self.inplanes = 32\n        self.in_conv = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, padding=1, stride=2, bias=False),\n                                     nn.BatchNorm2d(16),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "peekOfCode": "def build_backbone(args):\n    return SppBackbone()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "documentation": {}
    },
    {
        "label": "SppBackbone",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "peekOfCode": "class SppBackbone(nn.Module):\n    \"\"\"\n    Contracting path of feature descriptor using Spatial Pyramid Pooling,\n    SPP followed by PSMNet (https://github.com/JiaRenChang/PSMNet)\n    \"\"\"\n    def __init__(self):\n        super(SppBackbone, self).__init__()\n        self.inplanes = 32\n        self.in_conv = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, padding=1, stride=2, bias=False),\n                                     nn.InstanceNorm2d(16),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "peekOfCode": "def build_backbone(args):\n    return SppBackbone()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "documentation": {}
    },
    {
        "label": "TransitionUp",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "peekOfCode": "class TransitionUp(nn.Module):\n    \"\"\"\n    Scale the resolution up by transposed convolution\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, scale: int = 2):\n        super().__init__()\n        if scale == 2:\n            self.convTrans = nn.ConvTranspose2d(\n                in_channels=in_channels, out_channels=out_channels,\n                kernel_size=3, stride=2, padding=0, bias=True)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "documentation": {}
    },
    {
        "label": "DoubleConv",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "peekOfCode": "class DoubleConv(nn.Module):\n    \"\"\"\n    Two conv2d-bn-relu modules\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int):\n        super(DoubleConv, self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "peekOfCode": "class Tokenizer(nn.Module):\n    \"\"\"\n    Expanding path of feature descriptor using DenseBlocks\n    \"\"\"\n    def __init__(self, block_config: list, backbone_feat_channel: list, hidden_dim: int, growth_rate: int):\n        super(Tokenizer, self).__init__()\n        backbone_feat_channel.reverse()  # reverse so we have high-level first (lowest-spatial res)\n        block_config.reverse()\n        self.num_resolution = len(backbone_feat_channel)\n        self.block_config = block_config",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "documentation": {}
    },
    {
        "label": "build_tokenizer",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "peekOfCode": "def build_tokenizer(args, layer_channel):\n    growth_rate = 4\n    block_config = [4, 4, 4, 4]\n    return Tokenizer(block_config, layer_channel, args.CHANNEL_DIM, growth_rate)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "documentation": {}
    },
    {
        "label": "Criterion",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.loss",
        "peekOfCode": "class Criterion(nn.Module):\n    \"\"\"\n    Compute loss and evaluation metrics\n    \"\"\"\n    def __init__(self, threshold: int = 3, validation_max_disp: int = -1, loss_weight: list = None):\n        super(Criterion, self).__init__()\n        if loss_weight is None:\n            loss_weight = {}\n        self.px_threshold = threshold\n        self.validation_max_disp = validation_max_disp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.loss",
        "documentation": {}
    },
    {
        "label": "build_criterion",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.loss",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.loss",
        "peekOfCode": "def build_criterion(args):\n    loss_weight = {}\n    for weight in args.loss_weight.split(','):\n        k, v = weight.split(':')\n        k = k.strip()\n        v = float(v)\n        loss_weight[k] = v\n    return Criterion(args.px_error_threshold, args.validation_max_disp, loss_weight)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.loss",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "class NestedTensor(object):\n    def __init__(self, left, right, disp=None, sampled_cols=None, sampled_rows=None, occ_mask=None,\n                 occ_mask_right=None):\n        self.left = left\n        self.right = right\n        self.disp = disp\n        self.occ_mask = occ_mask\n        self.occ_mask_right = occ_mask_right\n        self.sampled_cols = sampled_cols\n        self.sampled_rows = sampled_rows",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "center_crop",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def center_crop(layer, max_height, max_width):\n    _, _, h, w = layer.size()\n    xy1 = (w - max_width) // 2\n    xy2 = (h - max_height) // 2\n    return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]\ndef batched_index_select(source, dim, index):\n    views = [source.shape[0]] + [1 if i != dim else -1 for i in range(1, len(source.shape))]\n    expanse = list(source.shape)\n    expanse[0] = -1\n    expanse[dim] = -1",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "batched_index_select",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def batched_index_select(source, dim, index):\n    views = [source.shape[0]] + [1 if i != dim else -1 for i in range(1, len(source.shape))]\n    expanse = list(source.shape)\n    expanse[0] = -1\n    expanse[dim] = -1\n    index = index.view(views).expand(expanse)\n    return torch.gather(source, dim, index)\ndef torch_1d_sample(source, sample_points, mode='linear'):\n    \"\"\"\n    linearly sample source tensor along the last dimension",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "torch_1d_sample",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def torch_1d_sample(source, sample_points, mode='linear'):\n    \"\"\"\n    linearly sample source tensor along the last dimension\n    input:\n        source [N,D1,D2,D3...,Dn]\n        sample_points [N,D1,D2,....,Dn-1,1]\n    output:\n        [N,D1,D2...,Dn-1]\n    \"\"\"\n    idx_l = torch.floor(sample_points).long().clamp(0, source.size(-1) - 1)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "get_clones",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\ndef find_occ_mask(disp_left, disp_right):\n    \"\"\"\n    find occlusion map\n    1 indicates occlusion\n    disp range [0,w]\n    \"\"\"\n    w = disp_left.shape[-1]\n    # # left occlusion",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "find_occ_mask",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def find_occ_mask(disp_left, disp_right):\n    \"\"\"\n    find occlusion map\n    1 indicates occlusion\n    disp range [0,w]\n    \"\"\"\n    w = disp_left.shape[-1]\n    # # left occlusion\n    # find corresponding pixels in target image\n    coord = np.linspace(0, w - 1, w)[None,]  # 1xW",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "save_and_clear",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def save_and_clear(idx, output_file):\n    with open('output-' + str(idx) + '.dat', 'wb') as f:\n        torch.save(output_file, f)\n    idx += 1\n    # clear\n    for key in output_file:\n        output_file[key].clear()\n    return idx",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "PositionEncodingSine1DRelative",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.pos_encoder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.pos_encoder",
        "peekOfCode": "class PositionEncodingSine1DRelative(nn.Module):\n    \"\"\"\n    relative sine encoding 1D, partially inspired by DETR (https://github.com/facebookresearch/detr)\n    \"\"\"\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.pos_encoder",
        "documentation": {}
    },
    {
        "label": "no_pos_encoding",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.pos_encoder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.pos_encoder",
        "peekOfCode": "def no_pos_encoding(x):\n    return None\ndef build_position_encoding(args):\n    mode = args.POSITION_ENCODING\n    channel_dim = args.CHANNEL_DIM\n    if mode == 'sine1d_rel':\n        n_steps = channel_dim\n        position_encoding = PositionEncodingSine1DRelative(n_steps, normalize=False)\n    elif mode == 'none':\n        position_encoding = no_pos_encoding",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.pos_encoder",
        "documentation": {}
    },
    {
        "label": "build_position_encoding",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.pos_encoder",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.pos_encoder",
        "peekOfCode": "def build_position_encoding(args):\n    mode = args.POSITION_ENCODING\n    channel_dim = args.CHANNEL_DIM\n    if mode == 'sine1d_rel':\n        n_steps = channel_dim\n        position_encoding = PositionEncodingSine1DRelative(n_steps, normalize=False)\n    elif mode == 'none':\n        position_encoding = no_pos_encoding\n    else:\n        raise ValueError(f\"not supported {mode}\")",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.pos_encoder",
        "documentation": {}
    },
    {
        "label": "RegressionHead",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.regression_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.regression_head",
        "peekOfCode": "class RegressionHead(nn.Module):\n    \"\"\"\n    Regress disparity and occlusion mask\n    \"\"\"\n    def __init__(self, cal: nn.Module, ot: bool = True):\n        super(RegressionHead, self).__init__()\n        self.cal = cal\n        self.ot = ot\n        self.phi = nn.Parameter(torch.tensor(0.0, requires_grad=True))  # dustbin cost\n    def _compute_unscaled_pos_shift(self, w: int, device: torch.device):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.regression_head",
        "documentation": {}
    },
    {
        "label": "build_regression_head",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.regression_head",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.regression_head",
        "peekOfCode": "def build_regression_head(args):\n    cal = build_context_adjustment_layer(args)\n    if args.REGRESSION_HEAD == 'ot':\n        ot = True\n    elif args.REGRESSION_HEAD == 'softmax':\n        ot = False\n    else:\n        raise Exception('Regression head type not recognized: ', args.regression_head)\n    return RegressionHead(cal, ot)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.regression_head",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "peekOfCode": "class Transformer(nn.Module):\n    \"\"\"\n    Transformer computes self (intra image) and cross (inter image) attention\n    \"\"\"\n    def __init__(self, hidden_dim: int = 128, nhead: int = 8, num_attn_layers: int = 6):\n        super().__init__()\n        self_attn_layer = TransformerSelfAttnLayer(hidden_dim, nhead)\n        self.self_attn_layers = get_clones(self_attn_layer, num_attn_layers)\n        cross_attn_layer = TransformerCrossAttnLayer(hidden_dim, nhead)\n        self.cross_attn_layers = get_clones(cross_attn_layer, num_attn_layers)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerSelfAttnLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "peekOfCode": "class TransformerSelfAttnLayer(nn.Module):\n    \"\"\"\n    Self attention layer\n    \"\"\"\n    def __init__(self, hidden_dim: int, nhead: int):\n        super().__init__()\n        self.self_attn = MultiheadAttentionRelative(hidden_dim, nhead)\n        self.norm1 = nn.LayerNorm(hidden_dim)\n    def forward(self, feat: Tensor,\n                pos: Optional[Tensor] = None,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerCrossAttnLayer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "peekOfCode": "class TransformerCrossAttnLayer(nn.Module):\n    \"\"\"\n    Cross attention layer\n    \"\"\"\n    def __init__(self, hidden_dim: int, nhead: int):\n        super().__init__()\n        self.cross_attn = MultiheadAttentionRelative(hidden_dim, nhead)\n        self.norm1 = nn.LayerNorm(hidden_dim)\n        self.norm2 = nn.LayerNorm(hidden_dim)\n    def forward(self, feat_left: Tensor, feat_right: Tensor,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "peekOfCode": "def build_transformer(args):\n    return Transformer(\n        hidden_dim=args.CHANNEL_DIM,\n        nhead=args.NHEADS,\n        num_attn_layers=args.NUM_ATTN_LAYERS\n    )",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "documentation": {}
    },
    {
        "label": "layer_idx",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "peekOfCode": "layer_idx = 0\nclass Transformer(nn.Module):\n    \"\"\"\n    Transformer computes self (intra image) and cross (inter image) attention\n    \"\"\"\n    def __init__(self, hidden_dim: int = 128, nhead: int = 8, num_attn_layers: int = 6):\n        super().__init__()\n        self_attn_layer = TransformerSelfAttnLayer(hidden_dim, nhead)\n        self.self_attn_layers = get_clones(self_attn_layer, num_attn_layers)\n        cross_attn_layer = TransformerCrossAttnLayer(hidden_dim, nhead)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.utilities.transformer",
        "documentation": {}
    },
    {
        "label": "STTR",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.sttr",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.sttr",
        "peekOfCode": "class STTR(nn.Module):\n    \"\"\"\n    STTR: it consists of\n        - backbone: contracting path of feature descriptor\n        - tokenizer: expanding path of feature descriptor\n        - pos_encoder: generates relative sine pos encoding\n        - transformer: computes self and cross attention\n        - regression_head: regresses disparity and occlusion, including optimal transport\n    \"\"\"\n    def __init__(self, args):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.sttr",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.trainer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.trainer",
        "peekOfCode": "__all__ = {\n    'STTR': STTR,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.models.sttr.trainer",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.trainer_template",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.trainer_template",
        "peekOfCode": "class TrainerTemplate:\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer, model):\n        self.args = args\n        self.cfgs = cfgs\n        self.local_rank = local_rank\n        self.global_rank = global_rank\n        self.logger = logger\n        self.tb_writer = tb_writer\n        self.model = self.build_model(model)\n        if self.args.run_mode in ['train', 'eval']:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "ClipGrad",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.clip_grad",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.clip_grad",
        "peekOfCode": "class ClipGrad:\n    def __init__(self, clip_type=\"None\", clip_value=0.1, max_norm=35, norm_type=2):\n        self.clip_type = clip_type\n        self.clip_value = clip_value\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n    def __call__(self, model):\n        if self.clip_type == 'value':\n            torch.nn.utils.clip_grad_value_(model.parameters(), self.clip_value)\n        elif self.clip_type == 'norm':",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.clip_grad",
        "documentation": {}
    },
    {
        "label": "config_loader",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def config_loader(path):\n    with open(path, 'r') as stream:\n        src_cfgs = yaml.safe_load(stream)\n    return src_cfgs\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "set_random_seed",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\ndef create_logger(log_file=None, rank=0, log_level=logging.INFO):\n    logger = logging.getLogger(__name__)\n    logger.setLevel(log_level if rank == 0 else 'ERROR')",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "create_logger",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def create_logger(log_file=None, rank=0, log_level=logging.INFO):\n    logger = logging.getLogger(__name__)\n    logger.setLevel(log_level if rank == 0 else 'ERROR')\n    formatter = logging.Formatter('%(asctime)s  %(levelname)5s  %(message)s')\n    console = logging.StreamHandler()\n    console.setLevel(log_level if rank == 0 else 'ERROR')\n    console.setFormatter(formatter)\n    logger.addHandler(console)\n    if log_file is not None:\n        file_handler = logging.FileHandler(filename=log_file)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_valid_args",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def get_valid_args(obj, input_args, free_keys=None):\n    if free_keys is None:\n        free_keys = []\n    if inspect.isfunction(obj):\n        expected_keys = inspect.getfullargspec(obj)[0]\n    elif inspect.isclass(obj):\n        expected_keys = inspect.getfullargspec(obj.__init__)[0]\n    else:\n        raise ValueError('Just support function and class object!')\n    unexpect_keys = list()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "backup_source_code",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def backup_source_code(backup_dir):\n    # \n    ignore_hidden = shutil.ignore_patterns(\n        \".idea\", \".git*\", \"*pycache*\",\n        \"cfgs\", \"data\", \"output\")\n    if os.path.exists(backup_dir):\n        shutil.rmtree(backup_dir)\n    shutil.copytree('.', backup_dir, ignore=ignore_hidden)\n    # os.system(\"chmod -R g+w {}\".format(backup_dir))\ndef log_configs(cfgs, pre='cfgs', logger=None):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "log_configs",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def log_configs(cfgs, pre='cfgs', logger=None):\n    for key, val in cfgs.items():\n        if isinstance(cfgs[key], EasyDict):\n            logger.info('----------- %s -----------' % key)\n            log_configs(cfgs[key], pre=pre + '.' + key, logger=logger)\n            continue\n        logger.info('%s.%s: %s' % (pre, key, val))\ndef save_checkpoint(model, optimizer, scheduler, scaler, is_dist, epoch, filename='checkpoint'):\n    if is_dist:\n        model_state = model.module.state_dict()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def save_checkpoint(model, optimizer, scheduler, scaler, is_dist, epoch, filename='checkpoint'):\n    if is_dist:\n        model_state = model.module.state_dict()\n    else:\n        model_state = model.state_dict()\n    optim_state = optimizer.state_dict()\n    scheduler_state = scheduler.state_dict()\n    scaler_state = scaler.state_dict()\n    state = {'epoch': epoch,\n             'model_state': model_state,",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "freeze_bn",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def freeze_bn(module):\n    \"\"\"Freeze the batch normalization layers.\"\"\"\n    for m in module.modules():\n        classname = m.__class__.__name__\n        if classname.find('BatchNorm') != -1:\n            m.eval()\n    return module\n# def convert_state_dict(ori_state_dict, is_dist=True):\n#     new_state_dict = OrderedDict()\n#     if is_dist:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "load_params_from_file",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def load_params_from_file(model, filename, device, dist_mode, logger, strict=True):\n    checkpoint = torch.load(filename, map_location=device)\n    pretrained_state_dict = checkpoint['model_state']\n    tmp_model = model.module if dist_mode else model\n    state_dict = tmp_model.state_dict()\n    unused_state_dict = {}\n    update_state_dict = {}\n    unupdate_state_dict = {}\n    for key, val in pretrained_state_dict.items():\n        if key in state_dict and state_dict[key].shape == val.shape:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "color_map_tensorboard",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def color_map_tensorboard(disp_gt, pred, disp_max=192):\n    cm = plt.get_cmap('plasma')\n    disp_gt = disp_gt.detach().data.cpu().numpy()\n    pred = pred.detach().data.cpu().numpy()\n    error_map = np.abs(pred - disp_gt)\n    disp_gt = np.clip(disp_gt, a_min=0, a_max=disp_max)\n    pred = np.clip(pred, a_min=0, a_max=disp_max)\n    gt_tmp = 255.0 * disp_gt / disp_max\n    pred_tmp = 255.0 * pred / disp_max\n    error_map_tmp = 255.0 * error_map / np.max(error_map)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "write_tensorboard",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def write_tensorboard(tb_writer, tb_info, step):\n    for k, v in tb_info.items():\n        module_name = k.split('/')[0]\n        writer_module = getattr(tb_writer, 'add_' + module_name)\n        board_name = k.replace(module_name + \"/\", '')\n        v = v.detach() if torch.is_tensor(v) else v\n        if module_name == 'image' and v.dim() == 2:\n            writer_module(board_name, v, step, dataformats='HW')\n        else:\n            writer_module(board_name, v, step)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "draw_depth_2_image",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def draw_depth_2_image(disp, image, baseline=0.54, focallength=1.003556e+3):\n    # baseline \n    disp = disp.detach().data.cpu().numpy()  # [h, w]\n    image = image.detach().data.cpu().numpy()  # [3, h, w]\n    image = np.transpose(image, [1, 2, 0])  # [h, w, 3]\n    image = np.ascontiguousarray(image, dtype=np.uint8)\n    point_size = 2\n    point_colors = [(255, 255, 0), (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255)]\n    thickness = -1\n    depths = baseline * focallength / disp",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_pos_fullres",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "peekOfCode": "def get_pos_fullres(fx, w, h):\n    x_range = (np.linspace(0, w - 1, w) + 0.5 - w // 2) / fx\n    y_range = (np.linspace(0, h - 1, h) + 0.5 - h // 2) / fx\n    x, y = np.meshgrid(x_range, y_range)\n    z = np.ones_like(x)\n    pos_grid = np.stack([x, y, z], axis=0).astype(np.float32)\n    return pos_grid",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "disp_to_color",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.disp_color",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.disp_color",
        "peekOfCode": "def disp_to_color(disp, max_disp=None):\n    \"\"\"\n    Transfer disparity map to color map\n    Args:\n        disp (numpy.array): disparity map in (Height, Width) layout, value range [0, 255]\n        max_disp (int): max disparity, optionally specifies the scaling factor\n    Returns:\n        disparity color map (numpy.array): disparity map in (Height, Width, 3) layout, range [0,255]\n    \"\"\"\n    h, w = disp.shape",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.disp_color",
        "documentation": {}
    },
    {
        "label": "disp_map",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.disp_color",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.disp_color",
        "peekOfCode": "def disp_map(disp):\n    \"\"\"\n    Based on color histogram, convert the gray disp into color disp map.\n    The histogram consists of 7 bins, value of each is e.g. [114.0, 185.0, 114.0, 174.0, 114.0, 185.0, 114.0]\n    Accumulate each bin, named cbins, and scale it to [0,1], e.g. [0.114, 0.299, 0.413, 0.587, 0.701, 0.886, 1.0]\n    For each value in disp, we have to find which bin it belongs to\n    Therefore, we have to compare it with every value in cbins\n    Finally, we have to get the ratio of it accounts for the bin, and then we can interpolate it with the histogram map\n    For example, 0.780 belongs to the 5th bin, the ratio is (0.780-0.701)/0.114,\n    then we can interpolate it into 3 channel with the 5th [0, 1, 0] and 6th [0, 1, 1] channel-map",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.disp_color",
        "documentation": {}
    },
    {
        "label": "Lamb",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.lamb",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.lamb",
        "peekOfCode": "class Lamb(Optimizer):\n    r\"\"\"Implements Lamb algorithm.\n    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.lamb",
        "documentation": {}
    },
    {
        "label": "BaseWarmup",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "peekOfCode": "class BaseWarmup(object):\n    \"\"\"Base class for all warmup schedules\n    Arguments:\n        optimizer (Optimizer): an instance of a subclass of Optimizer\n        warmup_params (list): warmup paramters\n        last_step (int): The index of last step. (Default: -1)\n    \"\"\"\n    def __init__(self, optimizer, warmup_params, last_step=-1):\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "documentation": {}
    },
    {
        "label": "LinearWarmup",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "peekOfCode": "class LinearWarmup(BaseWarmup):\n    \"\"\"Linear warmup schedule.\n    Arguments:\n        optimizer (Optimizer): an instance of a subclass of Optimizer\n        warmup_period (int or list): Warmup period\n        last_step (int): The index of last step. (Default: -1)\n    \"\"\"\n    def __init__(self, optimizer, warmup_period, last_step=-1):\n        group_count = len(optimizer.param_groups)\n        warmup_params = get_warmup_params(warmup_period, group_count)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "documentation": {}
    },
    {
        "label": "ExponentialWarmup",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "peekOfCode": "class ExponentialWarmup(BaseWarmup):\n    \"\"\"Exponential warmup schedule.\n    Arguments:\n        optimizer (Optimizer): an instance of a subclass of Optimizer\n        warmup_period (int or list): Effective warmup period\n        last_step (int): The index of last step. (Default: -1)\n    \"\"\"\n    def __init__(self, optimizer, warmup_period, last_step=-1):\n        group_count = len(optimizer.param_groups)\n        warmup_params = get_warmup_params(warmup_period, group_count)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "documentation": {}
    },
    {
        "label": "get_warmup_params",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "peekOfCode": "def get_warmup_params(warmup_period, group_count):\n    if type(warmup_period) == list:\n        if len(warmup_period) != group_count:\n            raise ValueError('size of warmup_period does not equal {}.'.format(group_count))\n        for x in warmup_period:\n            if type(x) != int:\n                raise ValueError('An element in warmup_period, {}, is not an int.'.format(type(x).__name__))\n        warmup_params = [dict(warmup_period=x) for x in warmup_period]\n    elif type(warmup_period) == int:\n        warmup_params = [dict(warmup_period=warmup_period) for _ in range(group_count)]",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.stereo.utils.warmup",
        "documentation": {}
    },
    {
        "label": "read_one_img",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.check_data",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.check_data",
        "peekOfCode": "def read_one_img(img_path):\n    try:\n        img = cv2.imread(img_path)\n        if img is None:\n            print(\"File open error:\", img_path)\n            return img_path\n    except:\n        print(\"File does not exist:\", img_path)\n        return img_path\ndef read_all_images(input_txt,error_txt):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.check_data",
        "documentation": {}
    },
    {
        "label": "read_all_images",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.check_data",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.check_data",
        "peekOfCode": "def read_all_images(input_txt,error_txt):\n    with open(input_txt, 'r') as f:\n        img_paths = [path.strip() for path in f.readlines()]\n    print(\"Show first image path: \",img_paths[0])\n    print(\"Start Check: it may need hours according to your datasets\")\n    with ThreadPoolExecutor() as executor:\n        target_dir_list = tqdm.tqdm(executor.map(read_one_img, img_paths), total=len(img_paths))\n    empty_paths = sorted([x for x in target_dir_list if x is not None])\n    with open(error_txt, 'w') as f:\n        for path in empty_paths:",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.check_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.check_data",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.check_data",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Check image by cv2.imread\")\n    parser.add_argument('--input_path_file', type=str, help='Path to the txt file containing image paths')\n    parser.add_argument('--error_path_file', type=str, help='Path to save the error image files')\n    args = parser.parse_args()\n    input_txt = args.input_path_file\n    error_txt = args.error_path_file\n    read_all_images(input_txt,error_txt)\nif __name__ == \"__main__\":\n    main()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.check_data",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.eval",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.eval",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--dist_mode', action='store_true', default=False, help='torchrun ddp multi gpu')\n    parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for eval')\n    parser.add_argument('--eval_data_cfg_file', type=str, default=None)\n    parser.add_argument('--pretrained_model', type=str, default=None)\n    # dataloader\n    parser.add_argument('--workers', type=int, default=0, help='number of workers for dataloader')\n    parser.add_argument('--pin_memory', action='store_true', default=False, help='data loader pin memory')\n    parser.add_argument('--save_root_dir', type=str, default='./output', help='save root dir for this experiment')",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.eval",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.eval",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.eval",
        "peekOfCode": "def main():\n    args, cfgs = parse_config()\n    if args.dist_mode:\n        dist.init_process_group(backend='nccl')\n        local_rank = int(os.environ[\"LOCAL_RANK\"])\n        global_rank = int(os.environ[\"RANK\"])\n    else:\n        local_rank = 0\n        global_rank = 0\n    # env",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.eval",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.infer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.infer",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--dist_mode', action='store_true', default=False, help='torchrun ddp multi gpu')\n    parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for eval')\n    # data\n    parser.add_argument('--left_img_path', type=str, default=None)\n    parser.add_argument('--right_img_path', type=str, default=None)\n    parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n    parser.add_argument('--savename', type=str, default=None)\n    args = parser.parse_args()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.infer",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.infer",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.infer",
        "peekOfCode": "def main():\n    args, cfgs = parse_config()\n    if args.dist_mode:\n        dist.init_process_group(backend='nccl')\n        local_rank = int(os.environ[\"LOCAL_RANK\"])\n        global_rank = int(os.environ[\"RANK\"])\n    else:\n        local_rank = 0\n        global_rank = 0\n    # env",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.infer",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--dist_mode', action='store_true', default=False, help='torchrun ddp multi gpu')\n    parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for training')\n    args = parser.parse_args()\n    yaml_config = common_utils.config_loader(args.cfg_file)\n    cfgs = EasyDict(yaml_config)\n    args.run_mode = 'measure'\n    return args, cfgs\ndef main():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "peekOfCode": "def main():\n    args, cfgs = parse_config()\n    model = build_trainer(args, cfgs, local_rank=0, global_rank=0, logger=None, tb_writer=None).model\n    shape = [1, 3, 544, 960]\n    infer_time(model, shape)\n    measure(model, shape)\n@torch.no_grad()\ndef measure(model, shape):\n    model.eval()\n    inputs = {'left': torch.randn(shape).cuda(),",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "documentation": {}
    },
    {
        "label": "measure",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "peekOfCode": "def measure(model, shape):\n    model.eval()\n    inputs = {'left': torch.randn(shape).cuda(),\n              'right': torch.randn(shape).cuda()}\n    flops, params = thop.profile(model, inputs=(inputs,))\n    print(\"Number of calculates:%.2fGFlops\" % (flops / 1e9))\n    print(\"Number of parameters:%.2fM\" % (params / 1e6))\n@torch.no_grad()\ndef infer_time(model, shape):\n    model.eval()",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "documentation": {}
    },
    {
        "label": "infer_time",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "peekOfCode": "def infer_time(model, shape):\n    model.eval()\n    repetitions = 100\n    inputs = {'left': torch.randn(shape).cuda(),\n              'right': torch.randn(shape).cuda()}\n    # , GPU , \n    print('warm up ...\\n')\n    with torch.no_grad():\n        for _ in range(10):\n            _ = model(inputs)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.measure",
        "documentation": {}
    },
    {
        "label": "resize_image",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.resize",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.resize",
        "peekOfCode": "def resize_image(input_path, input_path_root,output_path_root,target_size=(768, 384)):\n    try:\n        img = cv2.imread(input_path)\n        if img is None:\n            print(\"File open error:\", input_path)\n            return\n    except:\n        print(\"File does not exist:\", input_path)\n        return\n    output_path = input_path.replace(input_path_root, output_path_root)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.resize",
        "documentation": {}
    },
    {
        "label": "resize_all_images",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.resize",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.resize",
        "peekOfCode": "def resize_all_images(all_img_txt,input_path_root,output_path_root):\n    # Use ThreadPoolExecutor to process files in parallel \n    with open(all_img_txt, 'r') as f:\n        input_files = sorted([file.strip() for file in f.readlines()])\n    with ThreadPoolExecutor() as executor:\n        with tqdm.tqdm(total=len(input_files), desc=\"Resizing images\", unit=\"img\") as pbar:\n            for _ in executor.map(lambda img_path: resize_image(img_path, input_path_root,output_path_root),input_files):\n                pbar.update(1)\ndef main():\n    parser = argparse.ArgumentParser(description=\"Resize all image paths from a txt file.\")",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.resize",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.resize",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.resize",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Resize all image paths from a txt file.\")\n    parser.add_argument('--input_path_file', type=str, help='Path to the txt file containing image paths')\n    parser.add_argument('--input_path_root', type=str, help='Root directory containing input images')\n    parser.add_argument('--output_path_root', type=str, help='Root directory to save output paths')\n    args = parser.parse_args()\n    all_img_txt = args.input_path_file\n    input_path_root = args.input_path_root\n    output_path_root = args.output_path_root\n    resize_all_images(all_img_txt,input_path_root,output_path_root)",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.resize",
        "documentation": {}
    },
    {
        "label": "KittiTestDataset",
        "kind": 6,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.test_kitti",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.test_kitti",
        "peekOfCode": "class KittiTestDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode='testing'):\n        super().__init__(data_info, data_cfg, mode)\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]\n        left_img_path, right_img_path = full_paths[:2]\n        left_img = np.array(Image.open(left_img_path).convert('RGB'), dtype=np.float32)\n        right_img = np.array(Image.open(right_img_path).convert('RGB'), dtype=np.float32)\n        sample = {",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.test_kitti",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.test_kitti",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.test_kitti",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--workers', type=int, default=0, help='number of workers for dataloader')\n    parser.add_argument('--pin_memory', action='store_true', default=False, help='data loader pin memory')\n    parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n    parser.add_argument('--data_cfg_file', type=str, default='cfgs/kitti_eval_test.yaml')\n    args = parser.parse_args()\n    args.output_dir = str(Path(args.pretrained_model).parent.parent)\n    args.kitti_result_dir = os.path.join(args.output_dir, 'disp_0')\n    if not os.path.exists(args.kitti_result_dir):",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.test_kitti",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.test_kitti",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.test_kitti",
        "peekOfCode": "def main():\n    args, cfgs = parse_config()\n    local_rank = 0\n    global_rank = 0\n    torch.cuda.set_device(local_rank)\n    # logger\n    log_file = os.path.join(args.output_dir, 'testkitti_%s.log' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n    logger = common_utils.create_logger(log_file, rank=local_rank)\n    # log args and cfgs\n    for key, val in vars(args).items():",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.test_kitti",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.train",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    # mode\n    parser.add_argument('--dist_mode', action='store_true', default=False, help='torchrun ddp multi gpu')\n    parser.add_argument('--cfg_file', type=str, default=None, required=True, help='specify the config for training')\n    parser.add_argument('--fix_random_seed', action='store_true', default=False, help='')\n    # save path\n    parser.add_argument('--save_root_dir', type=str, default='./output', help='save root dir for this experiment')\n    parser.add_argument('--extra_tag', type=str, default='default', help='extra tag for this experiment')\n    # dataloader",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.train",
        "description": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.train",
        "peekOfCode": "def main():\n    args, cfgs = parse_config()\n    if args.dist_mode:\n        dist.init_process_group(backend='nccl')\n        local_rank = int(os.environ[\"LOCAL_RANK\"])\n        global_rank = int(os.environ[\"RANK\"])\n        group_rank = int(os.environ[\"GROUP_RANK\"])\n    else:\n        local_rank = 0\n        global_rank = 0",
        "detail": "output.KittiDataset.LightStereo.lightstereo_l_kitti.default.code.tools.train",
        "documentation": {}
    },
    {
        "label": "check_layer",
        "kind": 2,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "def check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))\n    zp = int(layer.get(\"zp\", 0))\n    dynamic_range = maxv - minv\n    risk = \" \"\n    reason = []\n    # ",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "yaml_path",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "yaml_path = \"/home/extra/share/mgz/lightstereo_s_sceneflow_general_opt_256_512_sim_conv_quant_param_simplified.yaml\"    #  \n#  INT8 \nSCALE_HIGH = 0.3\nSCALE_MED = 0.1\nRANGE_HIGH = 128\nRANGE_MED = 60\nZP_MIN = 0\nZP_MAX = 255\nZP_CENTER = 128\nZP_OFFSET_WARN = 100",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "SCALE_HIGH",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "SCALE_HIGH = 0.3\nSCALE_MED = 0.1\nRANGE_HIGH = 128\nRANGE_MED = 60\nZP_MIN = 0\nZP_MAX = 255\nZP_CENTER = 128\nZP_OFFSET_WARN = 100\n# =====  =====\ndef check_layer(layer):",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "SCALE_MED",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "SCALE_MED = 0.1\nRANGE_HIGH = 128\nRANGE_MED = 60\nZP_MIN = 0\nZP_MAX = 255\nZP_CENTER = 128\nZP_OFFSET_WARN = 100\n# =====  =====\ndef check_layer(layer):\n    name = layer.get(\"layername\")",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "RANGE_HIGH",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "RANGE_HIGH = 128\nRANGE_MED = 60\nZP_MIN = 0\nZP_MAX = 255\nZP_CENTER = 128\nZP_OFFSET_WARN = 100\n# =====  =====\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "RANGE_MED",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "RANGE_MED = 60\nZP_MIN = 0\nZP_MAX = 255\nZP_CENTER = 128\nZP_OFFSET_WARN = 100\n# =====  =====\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "ZP_MIN",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "ZP_MIN = 0\nZP_MAX = 255\nZP_CENTER = 128\nZP_OFFSET_WARN = 100\n# =====  =====\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "ZP_MAX",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "ZP_MAX = 255\nZP_CENTER = 128\nZP_OFFSET_WARN = 100\n# =====  =====\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))\n    zp = int(layer.get(\"zp\", 0))",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "ZP_CENTER",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "ZP_CENTER = 128\nZP_OFFSET_WARN = 100\n# =====  =====\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))\n    zp = int(layer.get(\"zp\", 0))\n    dynamic_range = maxv - minv",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "ZP_OFFSET_WARN",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "ZP_OFFSET_WARN = 100\n# =====  =====\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))\n    zp = int(layer.get(\"zp\", 0))\n    dynamic_range = maxv - minv\n    risk = \" \"",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "layers",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "layers = data.get(\"layers\", data)\nresults = [check_layer(layer) for layer in layers]\n# =====  =====\nprint(f\"{'Layer':30s} {'Scale':>8s} {'Range':>10s} {'ZP':>6s} {'Risk':>10s}  Reason\")\nprint(\"-\" * 90)\nfor r in results:\n    print(f\"{r['layer'][:30]:30s} {r['scale']:8.3f} {r['dynamic_range']:10.2f} {r['zp']:6d} {r['risk']:>10s}  {r['reason']}\")\n# =====  =====\nhigh = [r for r in results if \"\" in r['risk']]\nmid = [r for r in results if \"\" in r['risk']]",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "results = [check_layer(layer) for layer in layers]\n# =====  =====\nprint(f\"{'Layer':30s} {'Scale':>8s} {'Range':>10s} {'ZP':>6s} {'Risk':>10s}  Reason\")\nprint(\"-\" * 90)\nfor r in results:\n    print(f\"{r['layer'][:30]:30s} {r['scale']:8.3f} {r['dynamic_range']:10.2f} {r['zp']:6d} {r['risk']:>10s}  {r['reason']}\")\n# =====  =====\nhigh = [r for r in results if \"\" in r['risk']]\nmid = [r for r in results if \"\" in r['risk']]\nprint(\"\\n===  ===\")",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "high",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "high = [r for r in results if \"\" in r['risk']]\nmid = [r for r in results if \"\" in r['risk']]\nprint(\"\\n===  ===\")\nprint(f\": {len(high)}\")\nprint(f\": {len(mid)}\")\nprint(f\": {len(results) - len(high) - len(mid)}\")\nif high:\n    print(\"\\n:\")\n    for r in high:\n        print(f\" - {r['layer']} ({r['reason']})\")",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "mid",
        "kind": 5,
        "importPath": "quant.analysis_asym_yaml",
        "description": "quant.analysis_asym_yaml",
        "peekOfCode": "mid = [r for r in results if \"\" in r['risk']]\nprint(\"\\n===  ===\")\nprint(f\": {len(high)}\")\nprint(f\": {len(mid)}\")\nprint(f\": {len(results) - len(high) - len(mid)}\")\nif high:\n    print(\"\\n:\")\n    for r in high:\n        print(f\" - {r['layer']} ({r['reason']})\")",
        "detail": "quant.analysis_asym_yaml",
        "documentation": {}
    },
    {
        "label": "check_layer",
        "kind": 2,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "def check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))\n    zp = int(layer.get(\"zp\", 0))\n    dynamic_range = maxv - minv\n    #  min  -max\n    asym_ratio = abs(abs(minv) - abs(maxv)) / (abs(maxv) + 1e-6)\n    risk = \" \"",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "yaml_path",
        "kind": 5,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "yaml_path = \"/home/extra/share/symc/lightstereo_ptq_out/lightstereo_s_sceneflow_general_opt_256_512_sim_conv_quant_param_simplified.yaml\"   #  \nSCALE_HIGH = 0.3      # scale > 0.3 \nSCALE_MED = 0.1       # scale > 0.1 \nRANGE_HIGH = 60       #  > 60 \nRANGE_MED = 30        #  > 30 \nASYM_THRESHOLD = 0.2  # |min|  max \n# ======  ======\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "SCALE_HIGH",
        "kind": 5,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "SCALE_HIGH = 0.3      # scale > 0.3 \nSCALE_MED = 0.1       # scale > 0.1 \nRANGE_HIGH = 60       #  > 60 \nRANGE_MED = 30        #  > 30 \nASYM_THRESHOLD = 0.2  # |min|  max \n# ======  ======\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "SCALE_MED",
        "kind": 5,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "SCALE_MED = 0.1       # scale > 0.1 \nRANGE_HIGH = 60       #  > 60 \nRANGE_MED = 30        #  > 30 \nASYM_THRESHOLD = 0.2  # |min|  max \n# ======  ======\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "RANGE_HIGH",
        "kind": 5,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "RANGE_HIGH = 60       #  > 60 \nRANGE_MED = 30        #  > 30 \nASYM_THRESHOLD = 0.2  # |min|  max \n# ======  ======\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))\n    zp = int(layer.get(\"zp\", 0))",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "RANGE_MED",
        "kind": 5,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "RANGE_MED = 30        #  > 30 \nASYM_THRESHOLD = 0.2  # |min|  max \n# ======  ======\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))\n    zp = int(layer.get(\"zp\", 0))\n    dynamic_range = maxv - minv",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "ASYM_THRESHOLD",
        "kind": 5,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "ASYM_THRESHOLD = 0.2  # |min|  max \n# ======  ======\ndef check_layer(layer):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))\n    zp = int(layer.get(\"zp\", 0))\n    dynamic_range = maxv - minv\n    #  min  -max",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "layers",
        "kind": 5,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "layers = data.get(\"layers\", [])\nresults = [check_layer(l) for l in layers]\n# ======  ======\nprint(f\"{'Layer':30s} {'Scale':>8s} {'Range':>10s} {'Risk':>10s}  Reason\")\nprint(\"-\" * 80)\nfor r in results:\n    print(f\"{r['layer'][:30]:30s} {r['scale']:8.3f} {r['range']:10.2f} {r['risk']:>10s}  {r['reason']}\")\n# ======  ======\nhigh = [r for r in results if \"\" in r[\"risk\"]]\nmid = [r for r in results if \"\" in r[\"risk\"]]",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "results = [check_layer(l) for l in layers]\n# ======  ======\nprint(f\"{'Layer':30s} {'Scale':>8s} {'Range':>10s} {'Risk':>10s}  Reason\")\nprint(\"-\" * 80)\nfor r in results:\n    print(f\"{r['layer'][:30]:30s} {r['scale']:8.3f} {r['range']:10.2f} {r['risk']:>10s}  {r['reason']}\")\n# ======  ======\nhigh = [r for r in results if \"\" in r[\"risk\"]]\nmid = [r for r in results if \"\" in r[\"risk\"]]\nprint(\"\\n===  ===\")",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "high",
        "kind": 5,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "high = [r for r in results if \"\" in r[\"risk\"]]\nmid = [r for r in results if \"\" in r[\"risk\"]]\nprint(\"\\n===  ===\")\nprint(f\": {len(high)}\")\nprint(f\": {len(mid)}\")\nprint(f\": {len(results) - len(high) - len(mid)}\")\nif high:\n    print(\"\\n:\")\n    for r in high:\n        print(f\" - {r['layer']} ({r['reason']})\")",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "mid",
        "kind": 5,
        "importPath": "quant.analysis_sym_yaml",
        "description": "quant.analysis_sym_yaml",
        "peekOfCode": "mid = [r for r in results if \"\" in r[\"risk\"]]\nprint(\"\\n===  ===\")\nprint(f\": {len(high)}\")\nprint(f\": {len(mid)}\")\nprint(f\": {len(results) - len(high) - len(mid)}\")\nif high:\n    print(\"\\n:\")\n    for r in high:\n        print(f\" - {r['layer']} ({r['reason']})\")",
        "detail": "quant.analysis_sym_yaml",
        "documentation": {}
    },
    {
        "label": "analyze_layer",
        "kind": 2,
        "importPath": "quant.analysis_yaml",
        "description": "quant.analysis_yaml",
        "peekOfCode": "def analyze_layer(layer, dtype=\"int8\", symmetry=True):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))\n    zp = int(layer.get(\"zp\", 0))\n    dynamic_range = maxv - minv\n    risk = \" \"\n    reason = []\n    rec = []  # ",
        "detail": "quant.analysis_yaml",
        "documentation": {}
    },
    {
        "label": "analyze_yaml",
        "kind": 2,
        "importPath": "quant.analysis_yaml",
        "description": "quant.analysis_yaml",
        "peekOfCode": "def analyze_yaml(yaml_path, dtype=\"int8\", symmetry=True):\n    with open(yaml_path, \"r\") as f:\n        data = yaml.safe_load(f)\n    layers = data.get(\"layers\", data)\n    results = [analyze_layer(layer, dtype, symmetry) for layer in layers]\n    # \n    print(f\"\\n===  ({dtype.upper()} | symmetry={symmetry}) ===\\n\")\n    print(f\"{'Layer':30s} {'Scale':>8s} {'Range':>10s} {'ZP':>6s} {'Risk':>10s}  {'Error':>8s}  Reason / \")\n    print(\"-\" * 120)\n    for r in results:",
        "detail": "quant.analysis_yaml",
        "documentation": {}
    },
    {
        "label": "SCALE_MED",
        "kind": 5,
        "importPath": "quant.analysis_yaml",
        "description": "quant.analysis_yaml",
        "peekOfCode": "SCALE_MED = 0.1\nSCALE_HIGH = 0.3\nRANGE_MED = 60\nRANGE_HIGH = 128\nASYM_THRESHOLD = 0.3       # min/max30%\nZP_CENTER_U8 = 128\nZP_CENTER_I8 = 0\nZP_WARN_OFFSET = 100\n# ==============================\n# ",
        "detail": "quant.analysis_yaml",
        "documentation": {}
    },
    {
        "label": "SCALE_HIGH",
        "kind": 5,
        "importPath": "quant.analysis_yaml",
        "description": "quant.analysis_yaml",
        "peekOfCode": "SCALE_HIGH = 0.3\nRANGE_MED = 60\nRANGE_HIGH = 128\nASYM_THRESHOLD = 0.3       # min/max30%\nZP_CENTER_U8 = 128\nZP_CENTER_I8 = 0\nZP_WARN_OFFSET = 100\n# ==============================\n# \n# ==============================",
        "detail": "quant.analysis_yaml",
        "documentation": {}
    },
    {
        "label": "RANGE_MED",
        "kind": 5,
        "importPath": "quant.analysis_yaml",
        "description": "quant.analysis_yaml",
        "peekOfCode": "RANGE_MED = 60\nRANGE_HIGH = 128\nASYM_THRESHOLD = 0.3       # min/max30%\nZP_CENTER_U8 = 128\nZP_CENTER_I8 = 0\nZP_WARN_OFFSET = 100\n# ==============================\n# \n# ==============================\ndef analyze_layer(layer, dtype=\"int8\", symmetry=True):",
        "detail": "quant.analysis_yaml",
        "documentation": {}
    },
    {
        "label": "RANGE_HIGH",
        "kind": 5,
        "importPath": "quant.analysis_yaml",
        "description": "quant.analysis_yaml",
        "peekOfCode": "RANGE_HIGH = 128\nASYM_THRESHOLD = 0.3       # min/max30%\nZP_CENTER_U8 = 128\nZP_CENTER_I8 = 0\nZP_WARN_OFFSET = 100\n# ==============================\n# \n# ==============================\ndef analyze_layer(layer, dtype=\"int8\", symmetry=True):\n    name = layer.get(\"layername\")",
        "detail": "quant.analysis_yaml",
        "documentation": {}
    },
    {
        "label": "ASYM_THRESHOLD",
        "kind": 5,
        "importPath": "quant.analysis_yaml",
        "description": "quant.analysis_yaml",
        "peekOfCode": "ASYM_THRESHOLD = 0.3       # min/max30%\nZP_CENTER_U8 = 128\nZP_CENTER_I8 = 0\nZP_WARN_OFFSET = 100\n# ==============================\n# \n# ==============================\ndef analyze_layer(layer, dtype=\"int8\", symmetry=True):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))",
        "detail": "quant.analysis_yaml",
        "documentation": {}
    },
    {
        "label": "ZP_CENTER_U8",
        "kind": 5,
        "importPath": "quant.analysis_yaml",
        "description": "quant.analysis_yaml",
        "peekOfCode": "ZP_CENTER_U8 = 128\nZP_CENTER_I8 = 0\nZP_WARN_OFFSET = 100\n# ==============================\n# \n# ==============================\ndef analyze_layer(layer, dtype=\"int8\", symmetry=True):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))",
        "detail": "quant.analysis_yaml",
        "documentation": {}
    },
    {
        "label": "ZP_CENTER_I8",
        "kind": 5,
        "importPath": "quant.analysis_yaml",
        "description": "quant.analysis_yaml",
        "peekOfCode": "ZP_CENTER_I8 = 0\nZP_WARN_OFFSET = 100\n# ==============================\n# \n# ==============================\ndef analyze_layer(layer, dtype=\"int8\", symmetry=True):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))",
        "detail": "quant.analysis_yaml",
        "documentation": {}
    },
    {
        "label": "ZP_WARN_OFFSET",
        "kind": 5,
        "importPath": "quant.analysis_yaml",
        "description": "quant.analysis_yaml",
        "peekOfCode": "ZP_WARN_OFFSET = 100\n# ==============================\n# \n# ==============================\ndef analyze_layer(layer, dtype=\"int8\", symmetry=True):\n    name = layer.get(\"layername\")\n    minv = float(layer.get(\"min\", 0))\n    maxv = float(layer.get(\"max\", 0))\n    scale = float(layer.get(\"scale\", 0))\n    zp = int(layer.get(\"zp\", 0))",
        "detail": "quant.analysis_yaml",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "def preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1],input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])\n    return np.expand_dims(t(img).numpy(), axis=0)  # shape (1,3,H,W)\n# ==========  ==========\ndef quantize_int16(arr, scale):",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "quantize_int16",
        "kind": 2,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "def quantize_int16(arr, scale):\n    arr_q = np.clip(np.round(arr / scale), -32768, 32767).astype(np.int16)\n    return arr_q\n# ==========  ==========\nleft_f32 = preprocess(in_left)\nright_f32 = preprocess(in_right)\nleft_i16 = quantize_int16(left_f32, scale_left)\nright_i16 = quantize_int16(right_f32, scale_right)\n# \nnp.save(out_dir + \"left_32.npy\", left_f32)",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "in_left",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "in_left = \"/home/fays007/lc/share/mgz/datas/left.png\"\nin_right = \"/home/fays007/lc/share/mgz/datas/right.png\"\nout_dir = \"output/quant/\"\nos.makedirs(out_dir, exist_ok=True)\ninput_size = (512, 256)  # (W,H)\n# input_size = (256, 160)  # (W,H)\n#  scale\nscale_left = 0.000081\nscale_right = 0.000081\nnormalize_mean = [0.485, 0.456, 0.406]",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "in_right",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "in_right = \"/home/fays007/lc/share/mgz/datas/right.png\"\nout_dir = \"output/quant/\"\nos.makedirs(out_dir, exist_ok=True)\ninput_size = (512, 256)  # (W,H)\n# input_size = (256, 160)  # (W,H)\n#  scale\nscale_left = 0.000081\nscale_right = 0.000081\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "out_dir",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "out_dir = \"output/quant/\"\nos.makedirs(out_dir, exist_ok=True)\ninput_size = (512, 256)  # (W,H)\n# input_size = (256, 160)  # (W,H)\n#  scale\nscale_left = 0.000081\nscale_right = 0.000081\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "input_size",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "input_size = (512, 256)  # (W,H)\n# input_size = (256, 160)  # (W,H)\n#  scale\nscale_left = 0.000081\nscale_right = 0.000081\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "scale_left",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "scale_left = 0.000081\nscale_right = 0.000081\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1],input_size[0])),\n        transforms.ToTensor(),",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "scale_right",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "scale_right = 0.000081\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1],input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "normalize_mean",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "normalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1],input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "normalize_std",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "normalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1],input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])\n    return np.expand_dims(t(img).numpy(), axis=0)  # shape (1,3,H,W)",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "left_f32",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "left_f32 = preprocess(in_left)\nright_f32 = preprocess(in_right)\nleft_i16 = quantize_int16(left_f32, scale_left)\nright_i16 = quantize_int16(right_f32, scale_right)\n# \nnp.save(out_dir + \"left_32.npy\", left_f32)\nnp.save(out_dir + \"right_32.npy\", right_f32)\nleft_i16.tofile(out_dir + f\"left_16.bin\")\nright_i16.tofile(out_dir + f\"right_16.bin\")\nnp.save(out_dir + f\"left_16.npy\", left_i16.astype(np.float32))",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "right_f32",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "right_f32 = preprocess(in_right)\nleft_i16 = quantize_int16(left_f32, scale_left)\nright_i16 = quantize_int16(right_f32, scale_right)\n# \nnp.save(out_dir + \"left_32.npy\", left_f32)\nnp.save(out_dir + \"right_32.npy\", right_f32)\nleft_i16.tofile(out_dir + f\"left_16.bin\")\nright_i16.tofile(out_dir + f\"right_16.bin\")\nnp.save(out_dir + f\"left_16.npy\", left_i16.astype(np.float32))\nnp.save(out_dir + f\"right_16.npy\", right_i16.astype(np.float32))",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "left_i16",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "left_i16 = quantize_int16(left_f32, scale_left)\nright_i16 = quantize_int16(right_f32, scale_right)\n# \nnp.save(out_dir + \"left_32.npy\", left_f32)\nnp.save(out_dir + \"right_32.npy\", right_f32)\nleft_i16.tofile(out_dir + f\"left_16.bin\")\nright_i16.tofile(out_dir + f\"right_16.bin\")\nnp.save(out_dir + f\"left_16.npy\", left_i16.astype(np.float32))\nnp.save(out_dir + f\"right_16.npy\", right_i16.astype(np.float32))\nprint(\" int16 \")",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "right_i16",
        "kind": 5,
        "importPath": "quant.convertInt16",
        "description": "quant.convertInt16",
        "peekOfCode": "right_i16 = quantize_int16(right_f32, scale_right)\n# \nnp.save(out_dir + \"left_32.npy\", left_f32)\nnp.save(out_dir + \"right_32.npy\", right_f32)\nleft_i16.tofile(out_dir + f\"left_16.bin\")\nright_i16.tofile(out_dir + f\"right_16.bin\")\nnp.save(out_dir + f\"left_16.npy\", left_i16.astype(np.float32))\nnp.save(out_dir + f\"right_16.npy\", right_i16.astype(np.float32))\nprint(\" int16 \")\nprint(f\"Left scale = {scale_left}, Right scale = {scale_right}\")",
        "detail": "quant.convertInt16",
        "documentation": {}
    },
    {
        "label": "input_size",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "input_size = (512,256)  # (W,H)\nfrom torchvision import transforms\n# left_img = Image.open('/home/lc/share/mgz/datas/left.png').convert(\"RGB\")\n# right_img = Image.open('/home/lc/share/mgz/datas/right.png').convert(\"RGB\")\nleft_img = Image.open('/home/fays007/lc/share/mgz/datas/left.png').convert(\"RGB\")\nright_img = Image.open('/home/fays007/lc/share/mgz/datas/right.png').convert(\"RGB\")\n#### RGB uint8\nleft_img_array = np.array(left_img.resize(input_size))  # PILresize (, )\nleft_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize(input_size))  # PILresize (, )",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_img",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_img = Image.open('/home/fays007/lc/share/mgz/datas/left.png').convert(\"RGB\")\nright_img = Image.open('/home/fays007/lc/share/mgz/datas/right.png').convert(\"RGB\")\n#### RGB uint8\nleft_img_array = np.array(left_img.resize(input_size))  # PILresize (, )\nleft_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize(input_size))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_img",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_img = Image.open('/home/fays007/lc/share/mgz/datas/right.png').convert(\"RGB\")\n#### RGB uint8\nleft_img_array = np.array(left_img.resize(input_size))  # PILresize (, )\nleft_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize(input_size))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_img_array",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_img_array = np.array(left_img.resize(input_size))  # PILresize (, )\nleft_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize(input_size))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_img_uint8",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize(input_size))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_img_array",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_img_array = np.array(right_img.resize(input_size))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_img_uint8",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")\nprint(f\" -----------------------------------------------------\")",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_uint8_npy",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")\nprint(f\" -----------------------------------------------------\")\nnormalize_mean=[0.485, 0.456, 0.406]",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_uint8_npy",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")\nprint(f\" -----------------------------------------------------\")\nnormalize_mean=[0.485, 0.456, 0.406]\nnormalize_std=[0.229, 0.224, 0.225]",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "transform = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])), \n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])        \nleft_img = transform(left_img)\nright_img = transform(right_img)\nleft_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_img",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_img = transform(left_img)\nright_img = transform(right_img)\nleft_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_img",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_img = transform(right_img)\nleft_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_npy",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_npy",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)\nprint(left_npy_4)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_npy_4",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)\nprint(left_npy_4)\nprint(right_npy_4)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_npy_4",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)\nprint(left_npy_4)\nprint(right_npy_4)\nprint(f\" -----------------------------------------------------\")",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "scale_left",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "scale_left = 0.020706  \nscale_right = 0.020706\nleft_scale_str = f\"{scale_left:.5f}\".replace(\".\", \"\")\nright_scale_str = f\"{scale_right:.5f}\".replace(\".\", \"\")\nleft_npy_2 = left_npy/scale_left\nright_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "scale_right",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "scale_right = 0.020706\nleft_scale_str = f\"{scale_left:.5f}\".replace(\".\", \"\")\nright_scale_str = f\"{scale_right:.5f}\".replace(\".\", \"\")\nleft_npy_2 = left_npy/scale_left\nright_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_scale_str",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_scale_str = f\"{scale_left:.5f}\".replace(\".\", \"\")\nright_scale_str = f\"{scale_right:.5f}\".replace(\".\", \"\")\nleft_npy_2 = left_npy/scale_left\nright_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_scale_str",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_scale_str = f\"{scale_right:.5f}\".replace(\".\", \"\")\nleft_npy_2 = left_npy/scale_left\nright_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_npy_2",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_npy_2 = left_npy/scale_left\nright_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) ",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_npy_2",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_int8",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8.bin\")\nright_int8.tofile(f\"output/quant/right_8.bin\")",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_int8",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8.bin\")\nright_int8.tofile(f\"output/quant/right_8.bin\")\n# np.save('output/quant/right_8.npy', right_int8)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_int8",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8.bin\")\nright_int8.tofile(f\"output/quant/right_8.bin\")\n# np.save('output/quant/right_8.npy', right_int8)\nnp.save(f\"output/quant/left_8_float_{left_scale_str}.npy\", left_int8.astype(np.float32))\nnp.save(f\"output/quant/right_8_float_{right_scale_str}.npy\", right_int8.astype(np.float32))\nprint(f\" -----------------------------------------------------\")",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_int8",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8.bin\")\nright_int8.tofile(f\"output/quant/right_8.bin\")\n# np.save('output/quant/right_8.npy', right_int8)\nnp.save(f\"output/quant/left_8_float_{left_scale_str}.npy\", left_int8.astype(np.float32))\nnp.save(f\"output/quant/right_8_float_{right_scale_str}.npy\", right_int8.astype(np.float32))\nprint(f\" -----------------------------------------------------\")\n# ",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_float",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_float = left_int8.astype(np.float32) \nright_float = right_int8.astype(np.float32) \nprint(left_float)\nprint(right_float)\nprint(left_npy_4.shape)\nprint(left_float.shape)\nprint(right_npy_4.shape)\nprint(right_float.shape)\nleft_num = float(np.dot(left_npy_4.flatten(),left_float.flatten()))\nleft_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_float",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_float = right_int8.astype(np.float32) \nprint(left_float)\nprint(right_float)\nprint(left_npy_4.shape)\nprint(left_float.shape)\nprint(right_npy_4.shape)\nprint(right_float.shape)\nleft_num = float(np.dot(left_npy_4.flatten(),left_float.flatten()))\nleft_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())\nprint(left_num)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_num",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_num = float(np.dot(left_npy_4.flatten(),left_float.flatten()))\nleft_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())\nprint(left_num)\nprint(left_num/left_demon)\nright_num = float(np.dot(right_npy_4.flatten(),right_float.flatten()))\nright_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_demon",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "left_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())\nprint(left_num)\nprint(left_num/left_demon)\nright_num = float(np.dot(right_npy_4.flatten(),right_float.flatten()))\nright_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_num",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_num = float(np.dot(right_npy_4.flatten(),right_float.flatten()))\nright_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "right_demon",
        "kind": 5,
        "importPath": "quant.convertInt8",
        "description": "quant.convertInt8",
        "peekOfCode": "right_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertInt8",
        "documentation": {}
    },
    {
        "label": "left_img",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_img = Image.open('/home/lc/share/datas/left.png').convert(\"RGB\")\nright_img = Image.open('/home/lc/share/datas/right.png').convert(\"RGB\")\n#### RGB uint8\nleft_img_array = np.array(left_img.resize((512, 256)))  # PILresize (, )\nleft_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize((512, 256)))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_img",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_img = Image.open('/home/lc/share/datas/right.png').convert(\"RGB\")\n#### RGB uint8\nleft_img_array = np.array(left_img.resize((512, 256)))  # PILresize (, )\nleft_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize((512, 256)))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_img_array",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_img_array = np.array(left_img.resize((512, 256)))  # PILresize (, )\nleft_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize((512, 256)))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_img_uint8",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize((512, 256)))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_img_array",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_img_array = np.array(right_img.resize((512, 256)))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_img_uint8",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")\nprint(f\" -----------------------------------------------------\")",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_uint8_npy",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")\nprint(f\" -----------------------------------------------------\")\nnormalize_mean=[0.485, 0.456, 0.406]",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_uint8_npy",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")\nprint(f\" -----------------------------------------------------\")\nnormalize_mean=[0.485, 0.456, 0.406]\nnormalize_std=[0.229, 0.224, 0.225]",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "transform = transforms.Compose([\n        transforms.Resize((256, 512)), \n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])        \nleft_img = transform(left_img)\nright_img = transform(right_img)\nleft_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_img",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_img = transform(left_img)\nright_img = transform(right_img)\nleft_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_img",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_img = transform(right_img)\nleft_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_npy",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_npy",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)\nprint(left_npy_4)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_npy_4",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)\nprint(left_npy_4)\nprint(right_npy_4)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_npy_4",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)\nprint(left_npy_4)\nprint(right_npy_4)\nprint(f\" -----------------------------------------------------\")",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "scale_left",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "scale_left = 0.019612\nscale_right = 0.018929\nleft_scale_str = f\"{scale_left:.5f}\".replace(\".\", \"\")\nright_scale_str = f\"{scale_right:.5f}\".replace(\".\", \"\")\nleft_npy_2 = left_npy/scale_left\nright_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "scale_right",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "scale_right = 0.018929\nleft_scale_str = f\"{scale_left:.5f}\".replace(\".\", \"\")\nright_scale_str = f\"{scale_right:.5f}\".replace(\".\", \"\")\nleft_npy_2 = left_npy/scale_left\nright_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_scale_str",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_scale_str = f\"{scale_left:.5f}\".replace(\".\", \"\")\nright_scale_str = f\"{scale_right:.5f}\".replace(\".\", \"\")\nleft_npy_2 = left_npy/scale_left\nright_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_scale_str",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_scale_str = f\"{scale_right:.5f}\".replace(\".\", \"\")\nleft_npy_2 = left_npy/scale_left\nright_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_npy_2",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_npy_2 = left_npy/scale_left\nright_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) ",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_npy_2",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_npy_2 = right_npy/scale_right\nprint(left_npy_2.dtype, left_npy_2.min(), left_npy_2.max())\nprint(right_npy_2.dtype, right_npy_2.min(), right_npy_2.max())\nleft_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_int8",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_int8 = np.clip(np.round(left_npy / scale_left), -128, 127).astype(np.int8)\nright_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8_{left_scale_str}.bin\")\nright_int8.tofile(f\"output/quant/right_8_{right_scale_str}.bin\")",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_int8",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_int8 = np.clip(np.round(right_npy / scale_right), -128, 127).astype(np.int8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8_{left_scale_str}.bin\")\nright_int8.tofile(f\"output/quant/right_8_{right_scale_str}.bin\")\n# np.save('output/quant/right_8.npy', right_int8)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_int8",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8_{left_scale_str}.bin\")\nright_int8.tofile(f\"output/quant/right_8_{right_scale_str}.bin\")\n# np.save('output/quant/right_8.npy', right_int8)\nnp.save(f\"output/quant/left_8_float_{left_scale_str}.npy\", left_int8.astype(np.float32))\nnp.save(f\"output/quant/right_8_float_{right_scale_str}.npy\", right_int8.astype(np.float32))\nprint(f\" -----------------------------------------------------\")",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_int8",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8_{left_scale_str}.bin\")\nright_int8.tofile(f\"output/quant/right_8_{right_scale_str}.bin\")\n# np.save('output/quant/right_8.npy', right_int8)\nnp.save(f\"output/quant/left_8_float_{left_scale_str}.npy\", left_int8.astype(np.float32))\nnp.save(f\"output/quant/right_8_float_{right_scale_str}.npy\", right_int8.astype(np.float32))\nprint(f\" -----------------------------------------------------\")\n# ",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_float",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_float = left_int8.astype(np.float32) \nright_float = right_int8.astype(np.float32) \nprint(left_float)\nprint(right_float)\nprint(left_npy_4.shape)\nprint(left_float.shape)\nprint(right_npy_4.shape)\nprint(right_float.shape)\nleft_num = float(np.dot(left_npy_4.flatten(),left_float.flatten()))\nleft_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_float",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_float = right_int8.astype(np.float32) \nprint(left_float)\nprint(right_float)\nprint(left_npy_4.shape)\nprint(left_float.shape)\nprint(right_npy_4.shape)\nprint(right_float.shape)\nleft_num = float(np.dot(left_npy_4.flatten(),left_float.flatten()))\nleft_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())\nprint(left_num)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_num",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_num = float(np.dot(left_npy_4.flatten(),left_float.flatten()))\nleft_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())\nprint(left_num)\nprint(left_num/left_demon)\nright_num = float(np.dot(right_npy_4.flatten(),right_float.flatten()))\nright_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_demon",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "left_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())\nprint(left_num)\nprint(left_num/left_demon)\nright_num = float(np.dot(right_npy_4.flatten(),right_float.flatten()))\nright_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_num",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_num = float(np.dot(right_npy_4.flatten(),right_float.flatten()))\nright_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "right_demon",
        "kind": 5,
        "importPath": "quant.convertTotal",
        "description": "quant.convertTotal",
        "peekOfCode": "right_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertTotal",
        "documentation": {}
    },
    {
        "label": "left_img",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_img = Image.open('/home/lc/share/datas/left.png').convert(\"RGB\")\nright_img = Image.open('/home/lc/share/datas/right.png').convert(\"RGB\")\n#### RGB uint8\nleft_img_array = np.array(left_img.resize((512, 256)))  # PILresize (, )\nleft_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize((512, 256)))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_img",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_img = Image.open('/home/lc/share/datas/right.png').convert(\"RGB\")\n#### RGB uint8\nleft_img_array = np.array(left_img.resize((512, 256)))  # PILresize (, )\nleft_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize((512, 256)))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_img_array",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_img_array = np.array(left_img.resize((512, 256)))  # PILresize (, )\nleft_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize((512, 256)))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_img_uint8",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_img_uint8 = torch.from_numpy(left_img_array).permute(2, 0, 1)  #  CHW \nright_img_array = np.array(right_img.resize((512, 256)))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_img_array",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_img_array = np.array(right_img.resize((512, 256)))  # PILresize (, )\nright_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_img_uint8",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_img_uint8 = torch.from_numpy(right_img_array).permute(2, 0, 1)  #  CHW \nleft_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")\nprint(f\" -----------------------------------------------------\")",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_uint8_npy",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_uint8_npy = np.expand_dims(left_img_uint8, axis=0)\nright_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")\nprint(f\" -----------------------------------------------------\")\nnormalize_mean=[0.485, 0.456, 0.406]",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_uint8_npy",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_uint8_npy = np.expand_dims(right_img_uint8, axis=0) \n# print(left_uint8_npy.shape)\n# print(right_uint8_npy.shape)\n# print(left_uint8_npy)\n# print(right_uint8_npy)\n# left_uint8_npy.tofile(\"output/quant/left_uint8.npy\")\n# right_uint8_npy.tofile(\"output/quant/right_uint8.npy\")\nprint(f\" -----------------------------------------------------\")\nnormalize_mean=[0.485, 0.456, 0.406]\nnormalize_std=[0.229, 0.224, 0.225]",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "transform",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "transform = transforms.Compose([\n        transforms.Resize((256, 512)), \n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])        \nleft_img = transform(left_img)\nright_img = transform(right_img)\nleft_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_img",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_img = transform(left_img)\nright_img = transform(right_img)\nleft_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_img",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_img = transform(right_img)\nleft_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_npy",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_npy = left_img.numpy()\nright_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_npy",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_npy = right_img.numpy()\nleft_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)\nprint(left_npy_4)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_npy_4",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_npy_4 = np.expand_dims(left_npy, axis=0)\nright_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)\nprint(left_npy_4)\nprint(right_npy_4)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_npy_4",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_npy_4 = np.expand_dims(right_npy, axis=0) \n# float32\nnp.save('output/quant/left_32.npy', left_npy_4)\nnp.save('output/quant/right_32.npy', right_npy_4)\nprint(' float32 numpy:')\nprint(left_npy_4.shape)\nprint(right_npy_4.shape)\nprint(left_npy_4)\nprint(right_npy_4)\nprint(f\" -----------------------------------------------------\")",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "scale_left",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "scale_left = 0.017674\nzp_left = 120\nscale_right = 0.017207\nzp_right = 123\nleft_int8 = np.clip(np.round(left_npy / scale_left + zp_left), 0, 255).astype(np.uint8)\nright_int8 = np.clip(np.round(right_npy / scale_right + zp_right), 0, 255).astype(np.uint8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) ",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "zp_left",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "zp_left = 120\nscale_right = 0.017207\nzp_right = 123\nleft_int8 = np.clip(np.round(left_npy / scale_left + zp_left), 0, 255).astype(np.uint8)\nright_int8 = np.clip(np.round(right_npy / scale_right + zp_right), 0, 255).astype(np.uint8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "scale_right",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "scale_right = 0.017207\nzp_right = 123\nleft_int8 = np.clip(np.round(left_npy / scale_left + zp_left), 0, 255).astype(np.uint8)\nright_int8 = np.clip(np.round(right_npy / scale_right + zp_right), 0, 255).astype(np.uint8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "zp_right",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "zp_right = 123\nleft_int8 = np.clip(np.round(left_npy / scale_left + zp_left), 0, 255).astype(np.uint8)\nright_int8 = np.clip(np.round(right_npy / scale_right + zp_right), 0, 255).astype(np.uint8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8.bin\")",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_int8",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_int8 = np.clip(np.round(left_npy / scale_left + zp_left), 0, 255).astype(np.uint8)\nright_int8 = np.clip(np.round(right_npy / scale_right + zp_right), 0, 255).astype(np.uint8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8.bin\")\nright_int8.tofile(f\"output/quant/right_8.bin\")",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_int8",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_int8 = np.clip(np.round(right_npy / scale_right + zp_right), 0, 255).astype(np.uint8)\nprint(left_int8.dtype, left_int8.min(), left_int8.max())\nprint(right_int8.dtype, right_int8.min(), right_int8.max())\nleft_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8.bin\")\nright_int8.tofile(f\"output/quant/right_8.bin\")\n# np.save('output/quant/right_8.npy', right_int8)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_int8",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_int8 = np.expand_dims(left_int8, axis=0)\nright_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8.bin\")\nright_int8.tofile(f\"output/quant/right_8.bin\")\n# np.save('output/quant/right_8.npy', right_int8)\nnp.save(f\"output/quant/left_8_float.npy\", left_int8.astype(np.float32))\nnp.save(f\"output/quant/right_8_float.npy\", right_int8.astype(np.float32))\nprint(f\" -----------------------------------------------------\")",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_int8",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_int8 = np.expand_dims(right_int8, axis=0) \nprint(left_int8)\nprint(right_int8)\nleft_int8.tofile(f\"output/quant/left_8.bin\")\nright_int8.tofile(f\"output/quant/right_8.bin\")\n# np.save('output/quant/right_8.npy', right_int8)\nnp.save(f\"output/quant/left_8_float.npy\", left_int8.astype(np.float32))\nnp.save(f\"output/quant/right_8_float.npy\", right_int8.astype(np.float32))\nprint(f\" -----------------------------------------------------\")\n# ",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_float",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_float = (left_int8.astype(np.float32) - zp_left) * scale_left\nright_float = (right_int8.astype(np.float32) - zp_right) * scale_right\nprint(left_float)\nprint(right_float)\nprint(left_npy_4.shape)\nprint(left_float.shape)\nprint(right_npy_4.shape)\nprint(right_float.shape)\nleft_num = float(np.dot(left_npy_4.flatten(),left_float.flatten()))\nleft_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_float",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_float = (right_int8.astype(np.float32) - zp_right) * scale_right\nprint(left_float)\nprint(right_float)\nprint(left_npy_4.shape)\nprint(left_float.shape)\nprint(right_npy_4.shape)\nprint(right_float.shape)\nleft_num = float(np.dot(left_npy_4.flatten(),left_float.flatten()))\nleft_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())\nprint(left_num)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_num",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_num = float(np.dot(left_npy_4.flatten(),left_float.flatten()))\nleft_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())\nprint(left_num)\nprint(left_num/left_demon)\nright_num = float(np.dot(right_npy_4.flatten(),right_float.flatten()))\nright_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "left_demon",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "left_demon = np.linalg.norm(left_npy_4.flatten())*np.linalg.norm(left_float.flatten())\nprint(left_num)\nprint(left_num/left_demon)\nright_num = float(np.dot(right_npy_4.flatten(),right_float.flatten()))\nright_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_num",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_num = float(np.dot(right_npy_4.flatten(),right_float.flatten()))\nright_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "right_demon",
        "kind": 5,
        "importPath": "quant.convertTotalAsym",
        "description": "quant.convertTotalAsym",
        "peekOfCode": "right_demon = np.linalg.norm(right_npy_4.flatten())*np.linalg.norm(right_float.flatten())\nprint(right_num/right_demon)",
        "detail": "quant.convertTotalAsym",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "def preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])\n    #  (1, C, H, W) np.float32\n    return np.expand_dims(t(img).numpy().astype(np.float32), axis=0)\n# ==========  ( uint16 ) ==========",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "quantize_to_uint16",
        "kind": 2,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "def quantize_to_uint16(arr_float, scale, zero_point=None, verbose=True):\n    \"\"\"\n    arr_float: numpy array, float32, \n    scale:  float \n    zero_point:  None 2**15 (32768)\n    : uint16 numpy array\n    \"\"\"\n    if scale == 0 or zero_point is None:\n        raise ValueError(\"scale must be non-zero\")\n    # ",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "in_left",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "in_left = \"/home/fays007/lc/share/mgz/datas/left.png\"\nin_right = \"/home/fays007/lc/share/mgz/datas/right.png\"\nout_dir = \"output/quant/\"\nos.makedirs(out_dir, exist_ok=True)\ninput_size = (512,256)  # (W,H)\nscale_left = 0.000073\nscale_right = 0.000081\nzp_left = 29172\nzp_right = 2**15\nnormalize_mean = [0.485, 0.456, 0.406]",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "in_right",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "in_right = \"/home/fays007/lc/share/mgz/datas/right.png\"\nout_dir = \"output/quant/\"\nos.makedirs(out_dir, exist_ok=True)\ninput_size = (512,256)  # (W,H)\nscale_left = 0.000073\nscale_right = 0.000081\nzp_left = 29172\nzp_right = 2**15\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "out_dir",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "out_dir = \"output/quant/\"\nos.makedirs(out_dir, exist_ok=True)\ninput_size = (512,256)  # (W,H)\nscale_left = 0.000073\nscale_right = 0.000081\nzp_left = 29172\nzp_right = 2**15\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "input_size",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "input_size = (512,256)  # (W,H)\nscale_left = 0.000073\nscale_right = 0.000081\nzp_left = 29172\nzp_right = 2**15\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "scale_left",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "scale_left = 0.000073\nscale_right = 0.000081\nzp_left = 29172\nzp_right = 2**15\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "scale_right",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "scale_right = 0.000081\nzp_left = 29172\nzp_right = 2**15\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "zp_left",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "zp_left = 29172\nzp_right = 2**15\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),\n        transforms.ToTensor(),",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "zp_right",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "zp_right = 2**15\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "normalize_mean",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "normalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "normalize_std",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "normalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])\n    #  (1, C, H, W) np.float32",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "left_f32",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "left_f32 = preprocess(in_left)   # shape (1,3,H,W)\nright_f32 = preprocess(in_right)\nleft_u16 = quantize_to_uint16(left_f32, scale_left, zp_left, verbose=True)\nright_u16 = quantize_to_uint16(right_f32, scale_right, zp_right, verbose=True)\n#  float32numpy\n# np.save(os.path.join(out_dir, \"left_32.npy\"), left_f32)\n# np.save(os.path.join(out_dir, \"right_32.npy\"), right_f32)\n#  little-endian  dtype  '<u2'\n# left_u16_le = left_u16.astype('<u2')\n# right_u16_le = right_u16.astype('<u2')",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "right_f32",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "right_f32 = preprocess(in_right)\nleft_u16 = quantize_to_uint16(left_f32, scale_left, zp_left, verbose=True)\nright_u16 = quantize_to_uint16(right_f32, scale_right, zp_right, verbose=True)\n#  float32numpy\n# np.save(os.path.join(out_dir, \"left_32.npy\"), left_f32)\n# np.save(os.path.join(out_dir, \"right_32.npy\"), right_f32)\n#  little-endian  dtype  '<u2'\n# left_u16_le = left_u16.astype('<u2')\n# right_u16_le = right_u16.astype('<u2')\nleft_u16.tofile(os.path.join(out_dir, \"left_u16.bin\"))",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "left_u16",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "left_u16 = quantize_to_uint16(left_f32, scale_left, zp_left, verbose=True)\nright_u16 = quantize_to_uint16(right_f32, scale_right, zp_right, verbose=True)\n#  float32numpy\n# np.save(os.path.join(out_dir, \"left_32.npy\"), left_f32)\n# np.save(os.path.join(out_dir, \"right_32.npy\"), right_f32)\n#  little-endian  dtype  '<u2'\n# left_u16_le = left_u16.astype('<u2')\n# right_u16_le = right_u16.astype('<u2')\nleft_u16.tofile(os.path.join(out_dir, \"left_u16.bin\"))\nright_u16.tofile(os.path.join(out_dir, \"right_u16.bin\"))",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "right_u16",
        "kind": 5,
        "importPath": "quant.convertUInt16",
        "description": "quant.convertUInt16",
        "peekOfCode": "right_u16 = quantize_to_uint16(right_f32, scale_right, zp_right, verbose=True)\n#  float32numpy\n# np.save(os.path.join(out_dir, \"left_32.npy\"), left_f32)\n# np.save(os.path.join(out_dir, \"right_32.npy\"), right_f32)\n#  little-endian  dtype  '<u2'\n# left_u16_le = left_u16.astype('<u2')\n# right_u16_le = right_u16.astype('<u2')\nleft_u16.tofile(os.path.join(out_dir, \"left_u16.bin\"))\nright_u16.tofile(os.path.join(out_dir, \"right_u16.bin\"))\n#  numpy  uint16  float32 ",
        "detail": "quant.convertUInt16",
        "documentation": {}
    },
    {
        "label": "preprocess",
        "kind": 2,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "def preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])\n    return np.expand_dims(t(img).numpy().astype(np.float32), axis=0)  # (1,3,H,W)\n# ==========  (uint8) ==========\ndef quantize_to_uint8(arr_float, scale, zero_point, verbose=True):",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "quantize_to_uint8",
        "kind": 2,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "def quantize_to_uint8(arr_float, scale, zero_point, verbose=True):\n    \"\"\"\n    arr_float: numpy array (float32)\n    scale: float \n    zero_point: int, 0~255\n    \"\"\"\n    if scale == 0:\n        raise ValueError(\"scale must be non-zero\")\n    arr = arr_float.astype(np.float64)\n    q = np.round(arr / float(scale)) + float(zero_point)",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "in_left",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "in_left = \"/home/fays007/lc/share/mgz/datas/left.png\"\nin_right = \"/home/fays007/lc/share/mgz/datas/right.png\"\nout_dir = \"output/quant/\"\nos.makedirs(out_dir, exist_ok=True)\ninput_size = (512, 256)  # (W,H)\nscale_left = 0.018524\nscale_right = 0.018524\nzp_left = 112   #  zero_point\nzp_right = 112  # 128\nnormalize_mean = [0.485, 0.456, 0.406]",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "in_right",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "in_right = \"/home/fays007/lc/share/mgz/datas/right.png\"\nout_dir = \"output/quant/\"\nos.makedirs(out_dir, exist_ok=True)\ninput_size = (512, 256)  # (W,H)\nscale_left = 0.018524\nscale_right = 0.018524\nzp_left = 112   #  zero_point\nzp_right = 112  # 128\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "out_dir",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "out_dir = \"output/quant/\"\nos.makedirs(out_dir, exist_ok=True)\ninput_size = (512, 256)  # (W,H)\nscale_left = 0.018524\nscale_right = 0.018524\nzp_left = 112   #  zero_point\nzp_right = 112  # 128\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "input_size",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "input_size = (512, 256)  # (W,H)\nscale_left = 0.018524\nscale_right = 0.018524\nzp_left = 112   #  zero_point\nzp_right = 112  # 128\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "scale_left",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "scale_left = 0.018524\nscale_right = 0.018524\nzp_left = 112   #  zero_point\nzp_right = 112  # 128\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "scale_right",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "scale_right = 0.018524\nzp_left = 112   #  zero_point\nzp_right = 112  # 128\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "zp_left",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "zp_left = 112   #  zero_point\nzp_right = 112  # 128\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),\n        transforms.ToTensor(),",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "zp_right",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "zp_right = 112  # 128\nnormalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "normalize_mean",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "normalize_mean = [0.485, 0.456, 0.406]\nnormalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "normalize_std",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "normalize_std = [0.229, 0.224, 0.225]\n# ==========  ==========\ndef preprocess(img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    t = transforms.Compose([\n        transforms.Resize((input_size[1], input_size[0])),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std)\n    ])\n    return np.expand_dims(t(img).numpy().astype(np.float32), axis=0)  # (1,3,H,W)",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "left_f32",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "left_f32 = preprocess(in_left)\nright_f32 = preprocess(in_right)\nleft_u8 = quantize_to_uint8(left_f32, scale_left, zp_left, verbose=True)\nright_u8 = quantize_to_uint8(right_f32, scale_right, zp_right, verbose=True)\n# \nleft_u8.tofile(os.path.join(out_dir, \"left_u8.bin\"))\nright_u8.tofile(os.path.join(out_dir, \"right_u8.bin\"))\n#  numpy \nnp.save(os.path.join(out_dir, \"left_u8.npy\"), left_u8.astype(np.float32))\nnp.save(os.path.join(out_dir, \"right_u8.npy\"), right_u8.astype(np.float32))",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "right_f32",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "right_f32 = preprocess(in_right)\nleft_u8 = quantize_to_uint8(left_f32, scale_left, zp_left, verbose=True)\nright_u8 = quantize_to_uint8(right_f32, scale_right, zp_right, verbose=True)\n# \nleft_u8.tofile(os.path.join(out_dir, \"left_u8.bin\"))\nright_u8.tofile(os.path.join(out_dir, \"right_u8.bin\"))\n#  numpy \nnp.save(os.path.join(out_dir, \"left_u8.npy\"), left_u8.astype(np.float32))\nnp.save(os.path.join(out_dir, \"right_u8.npy\"), right_u8.astype(np.float32))\nprint(\" uint8 \")",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "left_u8",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "left_u8 = quantize_to_uint8(left_f32, scale_left, zp_left, verbose=True)\nright_u8 = quantize_to_uint8(right_f32, scale_right, zp_right, verbose=True)\n# \nleft_u8.tofile(os.path.join(out_dir, \"left_u8.bin\"))\nright_u8.tofile(os.path.join(out_dir, \"right_u8.bin\"))\n#  numpy \nnp.save(os.path.join(out_dir, \"left_u8.npy\"), left_u8.astype(np.float32))\nnp.save(os.path.join(out_dir, \"right_u8.npy\"), right_u8.astype(np.float32))\nprint(\" uint8 \")\nprint(f\"Left shape = {left_u8.shape}, Right shape = {right_u8.shape}\")",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "right_u8",
        "kind": 5,
        "importPath": "quant.convertUInt8",
        "description": "quant.convertUInt8",
        "peekOfCode": "right_u8 = quantize_to_uint8(right_f32, scale_right, zp_right, verbose=True)\n# \nleft_u8.tofile(os.path.join(out_dir, \"left_u8.bin\"))\nright_u8.tofile(os.path.join(out_dir, \"right_u8.bin\"))\n#  numpy \nnp.save(os.path.join(out_dir, \"left_u8.npy\"), left_u8.astype(np.float32))\nnp.save(os.path.join(out_dir, \"right_u8.npy\"), right_u8.astype(np.float32))\nprint(\" uint8 \")\nprint(f\"Left shape = {left_u8.shape}, Right shape = {right_u8.shape}\")",
        "detail": "quant.convertUInt8",
        "documentation": {}
    },
    {
        "label": "compare_layers",
        "kind": 2,
        "importPath": "quant.disparity_diff",
        "description": "quant.disparity_diff",
        "peekOfCode": "def compare_layers(board_file='./output/quant/disparity.npy',\n                   onnx_file='./output/export_disparity.npy',\n                   threshold=1e-3):\n    \"\"\"\n     + \n    :\n        board_file:  .npy \n        onnx_file:  ONNX  .npy \n        threshold:   ( 1e-3)\n    \"\"\"",
        "detail": "quant.disparity_diff",
        "documentation": {}
    },
    {
        "label": "get_min",
        "kind": 2,
        "importPath": "quant.disparity_yaml",
        "description": "quant.disparity_yaml",
        "peekOfCode": "def get_min(val):\n    \"\"\" val val  float  list/tuple\"\"\"\n    if isinstance(val, (list, tuple)):\n        return min(val)\n    else:\n        return val\ndef get_max(val):\n    \"\"\" val val  float  list/tuple\"\"\"\n    if isinstance(val, (list, tuple)):\n        return max(val)",
        "detail": "quant.disparity_yaml",
        "documentation": {}
    },
    {
        "label": "get_max",
        "kind": 2,
        "importPath": "quant.disparity_yaml",
        "description": "quant.disparity_yaml",
        "peekOfCode": "def get_max(val):\n    \"\"\" val val  float  list/tuple\"\"\"\n    if isinstance(val, (list, tuple)):\n        return max(val)\n    else:\n        return val\n#  YAML \nwith open(\"/home/extra/share/mgz/lightstereo_ptq_out/lightstereo_s_sceneflow_general_opt_256_512_sim_conv_quant_param.yaml\", \"r\", encoding=\"utf-8\") as f:\n    quant_data  = yaml.safe_load(f)\n# ",
        "detail": "quant.disparity_yaml",
        "documentation": {}
    },
    {
        "label": "layers",
        "kind": 5,
        "importPath": "quant.disparity_yaml",
        "description": "quant.disparity_yaml",
        "peekOfCode": "layers = quant_data[\"layers\"]\nprint(f\"\\n {len(layers)} \\n\")\nlayer_diffs = []\n# \nfor i, layer in enumerate(layers, start=1):\n    layer_min = get_min(layer['min'])\n    layer_max = get_max(layer['max'])\n    diff = layer_max - layer_min\n    layer_diffs.append({\n        \"index\": i,",
        "detail": "quant.disparity_yaml",
        "documentation": {}
    },
    {
        "label": "layer_diffs",
        "kind": 5,
        "importPath": "quant.disparity_yaml",
        "description": "quant.disparity_yaml",
        "peekOfCode": "layer_diffs = []\n# \nfor i, layer in enumerate(layers, start=1):\n    layer_min = get_min(layer['min'])\n    layer_max = get_max(layer['max'])\n    diff = layer_max - layer_min\n    layer_diffs.append({\n        \"index\": i,\n        \"layername\": layer['layername'],\n        \"min\": layer_min,",
        "detail": "quant.disparity_yaml",
        "documentation": {}
    },
    {
        "label": "sorted_by_diff",
        "kind": 5,
        "importPath": "quant.disparity_yaml",
        "description": "quant.disparity_yaml",
        "peekOfCode": "sorted_by_diff = sorted(layer_diffs, key=lambda x: x['diff'], reverse=True)\nprint(\"\\n diff \")\nfor info in sorted_by_diff[:300]:\n    print(f\"{info['index']:4d} Layer: {info['layername']}, max: {info['max']}, min: {info['min']}, diff: {info['diff']}\")",
        "detail": "quant.disparity_yaml",
        "documentation": {}
    },
    {
        "label": "print_full_onnx_info",
        "kind": 2,
        "importPath": "quant.inspect_quantized_weights",
        "description": "quant.inspect_quantized_weights",
        "peekOfCode": "def print_full_onnx_info(model_path):\n    # =====  =====\n    model = onnx.load(model_path)\n    graph = model.graph\n    print(\"=\" * 120)\n    print(f\" ONNX Model: {model_path}\")\n    print(\"=\" * 120)\n    # =====  =====\n    print(\" [Model Info]\")\n    print(f\"IR version       : {model.ir_version}\")",
        "detail": "quant.inspect_quantized_weights",
        "documentation": {}
    },
    {
        "label": "get_min",
        "kind": 2,
        "importPath": "quant.mix_yaml",
        "description": "quant.mix_yaml",
        "peekOfCode": "def get_min(val):\n    if isinstance(val, (list, tuple)):\n        return min(val)\n    return val\ndef get_max(val):\n    if isinstance(val, (list, tuple)):\n        return max(val)\n    return val\nyaml_path = \"/home/fays007/lc/share/mix/lightstereo_ptq_out/lightstereo_s_sceneflow_general_opt_256_512_sim_conv_quant_param.yaml\"\n#  YAML ",
        "detail": "quant.mix_yaml",
        "documentation": {}
    },
    {
        "label": "get_max",
        "kind": 2,
        "importPath": "quant.mix_yaml",
        "description": "quant.mix_yaml",
        "peekOfCode": "def get_max(val):\n    if isinstance(val, (list, tuple)):\n        return max(val)\n    return val\nyaml_path = \"/home/fays007/lc/share/mix/lightstereo_ptq_out/lightstereo_s_sceneflow_general_opt_256_512_sim_conv_quant_param.yaml\"\n#  YAML \nwith open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n    quant_data = yaml.safe_load(f)\nlayers = quant_data[\"layers\"]\nmodified_layers = 0",
        "detail": "quant.mix_yaml",
        "documentation": {}
    },
    {
        "label": "yaml_path",
        "kind": 5,
        "importPath": "quant.mix_yaml",
        "description": "quant.mix_yaml",
        "peekOfCode": "yaml_path = \"/home/fays007/lc/share/mix/lightstereo_ptq_out/lightstereo_s_sceneflow_general_opt_256_512_sim_conv_quant_param.yaml\"\n#  YAML \nwith open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n    quant_data = yaml.safe_load(f)\nlayers = quant_data[\"layers\"]\nmodified_layers = 0\nfor layer in layers:\n    name = layer[\"layername\"]\n    if \"weight\" in name.lower():\n        continue  #  weight ",
        "detail": "quant.mix_yaml",
        "documentation": {}
    },
    {
        "label": "layers",
        "kind": 5,
        "importPath": "quant.mix_yaml",
        "description": "quant.mix_yaml",
        "peekOfCode": "layers = quant_data[\"layers\"]\nmodified_layers = 0\nfor layer in layers:\n    name = layer[\"layername\"]\n    if \"weight\" in name.lower():\n        continue  #  weight \n    layer_min = get_min(layer[\"min\"])\n    layer_max = get_max(layer[\"max\"])\n    diff = layer_max - layer_min\n    max_range = max(abs(layer_min), abs(layer_max))",
        "detail": "quant.mix_yaml",
        "documentation": {}
    },
    {
        "label": "modified_layers",
        "kind": 5,
        "importPath": "quant.mix_yaml",
        "description": "quant.mix_yaml",
        "peekOfCode": "modified_layers = 0\nfor layer in layers:\n    name = layer[\"layername\"]\n    if \"weight\" in name.lower():\n        continue  #  weight \n    layer_min = get_min(layer[\"min\"])\n    layer_max = get_max(layer[\"max\"])\n    diff = layer_max - layer_min\n    max_range = max(abs(layer_min), abs(layer_max))\n    if max_range > 10:",
        "detail": "quant.mix_yaml",
        "documentation": {}
    },
    {
        "label": "out_path",
        "kind": 5,
        "importPath": "quant.mix_yaml",
        "description": "quant.mix_yaml",
        "peekOfCode": "out_path = yaml_path.replace(\".yaml\", \"_modified.yaml\")\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    yaml.dump(quant_data, f, allow_unicode=True, sort_keys=False)\nprint(f\": {out_path}\")",
        "detail": "quant.mix_yaml",
        "documentation": {}
    },
    {
        "label": "KITTIStereoDataset",
        "kind": 6,
        "importPath": "quant.onnx_int8",
        "description": "quant.onnx_int8",
        "peekOfCode": "class KITTIStereoDataset(Dataset):\n    def __init__(self, root, transform=None):\n        self.left_images = sorted(glob.glob(os.path.join(root, \"image_2\", \"*.png\")))\n        self.right_images = sorted(glob.glob(os.path.join(root, \"image_3\", \"*.png\")))\n        assert len(self.left_images) == len(self.right_images), \"\"\n        self.transform = transform\n    def __len__(self):\n        return len(self.left_images)\n    def __getitem__(self, idx):\n        left_img = Image.open(self.left_images[idx]).convert(\"RGB\")",
        "detail": "quant.onnx_int8",
        "documentation": {}
    },
    {
        "label": "KITTICalibDataReader",
        "kind": 6,
        "importPath": "quant.onnx_int8",
        "description": "quant.onnx_int8",
        "peekOfCode": "class KITTICalibDataReader(CalibrationDataReader):\n    def __init__(self, dataloader, input_names):\n        self.enum_data = None\n        self.dataloader = dataloader\n        self.input_names = input_names  # [left_name, right_name]\n    def get_next(self):\n        if self.enum_data is None:\n            np_batches = []\n            for left, right in self.dataloader:\n                left_np = left.numpy().astype(np.float32)   # [1,3,H,W]",
        "detail": "quant.onnx_int8",
        "documentation": {}
    },
    {
        "label": "build_dataloader",
        "kind": 2,
        "importPath": "quant.onnx_int8",
        "description": "quant.onnx_int8",
        "peekOfCode": "def build_dataloader(root, num_samples):\n    normalize_mean = [0.485, 0.456, 0.406]\n    normalize_std = [0.229, 0.224, 0.225]\n    transform = transforms.Compose([\n        transforms.Resize((256, 512)),\n        transforms.ToTensor(),\n        transforms.Normalize(normalize_mean, normalize_std),\n    ])\n    dataset = KITTIStereoDataset(root=root, transform=transform)\n    sampler = RandomSampler(dataset, replacement=True, num_samples=num_samples)",
        "detail": "quant.onnx_int8",
        "documentation": {}
    },
    {
        "label": "quantize_and_infer",
        "kind": 2,
        "importPath": "quant.onnx_int8",
        "description": "quant.onnx_int8",
        "peekOfCode": "def quantize_and_infer(model_fp32, model_int8, calib_loader,quant_format):\n    # \n    onnx_model = onnx.load(model_fp32)\n    input_names = [inp.name for inp in onnx_model.graph.input]\n    print(\"[]:\", input_names)\n    # \n    dr = KITTICalibDataReader(calib_loader, input_names)\n    quantize_static(\n        model_input=model_fp32,\n        model_output=model_int8,",
        "detail": "quant.onnx_int8",
        "documentation": {}
    },
    {
        "label": "yaml_path",
        "kind": 5,
        "importPath": "quant.simple_yaml",
        "description": "quant.simple_yaml",
        "peekOfCode": "yaml_path = \"/home/extra/share/symc/lightstereo_ptq_out/lightstereo_s_sceneflow_general_opt_256_512_sim_conv_quant_param.yaml\"\n#  YAML \nwith open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n    quant_data = yaml.safe_load(f)\nlayers = quant_data[\"layers\"]\nsimplified_layers = []\ncount = 0\nw_count = 0\n#  minmaxscalezp \nfor layer in layers:",
        "detail": "quant.simple_yaml",
        "documentation": {}
    },
    {
        "label": "layers",
        "kind": 5,
        "importPath": "quant.simple_yaml",
        "description": "quant.simple_yaml",
        "peekOfCode": "layers = quant_data[\"layers\"]\nsimplified_layers = []\ncount = 0\nw_count = 0\n#  minmaxscalezp \nfor layer in layers:\n    if \"weight\" in layer.get(\"layername\").lower():\n        w_count += 1\n        continue\n    new_layer = {",
        "detail": "quant.simple_yaml",
        "documentation": {}
    },
    {
        "label": "simplified_layers",
        "kind": 5,
        "importPath": "quant.simple_yaml",
        "description": "quant.simple_yaml",
        "peekOfCode": "simplified_layers = []\ncount = 0\nw_count = 0\n#  minmaxscalezp \nfor layer in layers:\n    if \"weight\" in layer.get(\"layername\").lower():\n        w_count += 1\n        continue\n    new_layer = {\n        \"layername\": layer[\"layername\"],",
        "detail": "quant.simple_yaml",
        "documentation": {}
    },
    {
        "label": "count",
        "kind": 5,
        "importPath": "quant.simple_yaml",
        "description": "quant.simple_yaml",
        "peekOfCode": "count = 0\nw_count = 0\n#  minmaxscalezp \nfor layer in layers:\n    if \"weight\" in layer.get(\"layername\").lower():\n        w_count += 1\n        continue\n    new_layer = {\n        \"layername\": layer[\"layername\"],\n    }",
        "detail": "quant.simple_yaml",
        "documentation": {}
    },
    {
        "label": "w_count",
        "kind": 5,
        "importPath": "quant.simple_yaml",
        "description": "quant.simple_yaml",
        "peekOfCode": "w_count = 0\n#  minmaxscalezp \nfor layer in layers:\n    if \"weight\" in layer.get(\"layername\").lower():\n        w_count += 1\n        continue\n    new_layer = {\n        \"layername\": layer[\"layername\"],\n    }\n    # ",
        "detail": "quant.simple_yaml",
        "documentation": {}
    },
    {
        "label": "quant_data[\"layers\"]",
        "kind": 5,
        "importPath": "quant.simple_yaml",
        "description": "quant.simple_yaml",
        "peekOfCode": "quant_data[\"layers\"] = simplified_layers\nout_path = yaml_path.replace(\".yaml\", \"_simplified.yaml\")\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    yaml.dump(quant_data, f, allow_unicode=True, sort_keys=False)\nprint(f\"  min/max/scale/zp \")\nprint(f\": {out_path} with weight:{w_count}  active:{count}\")",
        "detail": "quant.simple_yaml",
        "documentation": {}
    },
    {
        "label": "out_path",
        "kind": 5,
        "importPath": "quant.simple_yaml",
        "description": "quant.simple_yaml",
        "peekOfCode": "out_path = yaml_path.replace(\".yaml\", \"_simplified.yaml\")\nwith open(out_path, \"w\", encoding=\"utf-8\") as f:\n    yaml.dump(quant_data, f, allow_unicode=True, sort_keys=False)\nprint(f\"  min/max/scale/zp \")\nprint(f\": {out_path} with weight:{w_count}  active:{count}\")",
        "detail": "quant.simple_yaml",
        "documentation": {}
    },
    {
        "label": "npy_file",
        "kind": 5,
        "importPath": "quant.tensor_diff",
        "description": "quant.tensor_diff",
        "peekOfCode": "npy_file = '/home/wanglin/tensors/npy/disp_pred_quanted.npy'\nbin_file = '/home/wanglin/tensors/tensor/ReduceSum_1138'\narr_bin = np.fromfile(bin_file, dtype=np.int8).astype(np.float32)\nprint(arr_bin)\narr_npy = np.load(npy_file)\nprint(arr_npy)\narr_bin_flat = arr_bin.flatten()\narr_npy_flat = arr_npy.flatten()\nnum = float(np.dot(arr_bin_flat,arr_npy_flat))\ndemon = np.linalg.norm(arr_bin_flat)*np.linalg.norm(arr_npy_flat)",
        "detail": "quant.tensor_diff",
        "documentation": {}
    },
    {
        "label": "bin_file",
        "kind": 5,
        "importPath": "quant.tensor_diff",
        "description": "quant.tensor_diff",
        "peekOfCode": "bin_file = '/home/wanglin/tensors/tensor/ReduceSum_1138'\narr_bin = np.fromfile(bin_file, dtype=np.int8).astype(np.float32)\nprint(arr_bin)\narr_npy = np.load(npy_file)\nprint(arr_npy)\narr_bin_flat = arr_bin.flatten()\narr_npy_flat = arr_npy.flatten()\nnum = float(np.dot(arr_bin_flat,arr_npy_flat))\ndemon = np.linalg.norm(arr_bin_flat)*np.linalg.norm(arr_npy_flat)\nprint(num/demon)",
        "detail": "quant.tensor_diff",
        "documentation": {}
    },
    {
        "label": "arr_bin",
        "kind": 5,
        "importPath": "quant.tensor_diff",
        "description": "quant.tensor_diff",
        "peekOfCode": "arr_bin = np.fromfile(bin_file, dtype=np.int8).astype(np.float32)\nprint(arr_bin)\narr_npy = np.load(npy_file)\nprint(arr_npy)\narr_bin_flat = arr_bin.flatten()\narr_npy_flat = arr_npy.flatten()\nnum = float(np.dot(arr_bin_flat,arr_npy_flat))\ndemon = np.linalg.norm(arr_bin_flat)*np.linalg.norm(arr_npy_flat)\nprint(num/demon)\neuclidean_distance = np.linalg.norm(arr_bin_flat - arr_npy_flat)",
        "detail": "quant.tensor_diff",
        "documentation": {}
    },
    {
        "label": "arr_npy",
        "kind": 5,
        "importPath": "quant.tensor_diff",
        "description": "quant.tensor_diff",
        "peekOfCode": "arr_npy = np.load(npy_file)\nprint(arr_npy)\narr_bin_flat = arr_bin.flatten()\narr_npy_flat = arr_npy.flatten()\nnum = float(np.dot(arr_bin_flat,arr_npy_flat))\ndemon = np.linalg.norm(arr_bin_flat)*np.linalg.norm(arr_npy_flat)\nprint(num/demon)\neuclidean_distance = np.linalg.norm(arr_bin_flat - arr_npy_flat)\nprint(\":\", euclidean_distance.max())\nprint(\":\", euclidean_distance.mean())",
        "detail": "quant.tensor_diff",
        "documentation": {}
    },
    {
        "label": "arr_bin_flat",
        "kind": 5,
        "importPath": "quant.tensor_diff",
        "description": "quant.tensor_diff",
        "peekOfCode": "arr_bin_flat = arr_bin.flatten()\narr_npy_flat = arr_npy.flatten()\nnum = float(np.dot(arr_bin_flat,arr_npy_flat))\ndemon = np.linalg.norm(arr_bin_flat)*np.linalg.norm(arr_npy_flat)\nprint(num/demon)\neuclidean_distance = np.linalg.norm(arr_bin_flat - arr_npy_flat)\nprint(\":\", euclidean_distance.max())\nprint(\":\", euclidean_distance.mean())\nprint(f\": {(euclidean_distance > 1e-3).mean():.2%}\")",
        "detail": "quant.tensor_diff",
        "documentation": {}
    },
    {
        "label": "arr_npy_flat",
        "kind": 5,
        "importPath": "quant.tensor_diff",
        "description": "quant.tensor_diff",
        "peekOfCode": "arr_npy_flat = arr_npy.flatten()\nnum = float(np.dot(arr_bin_flat,arr_npy_flat))\ndemon = np.linalg.norm(arr_bin_flat)*np.linalg.norm(arr_npy_flat)\nprint(num/demon)\neuclidean_distance = np.linalg.norm(arr_bin_flat - arr_npy_flat)\nprint(\":\", euclidean_distance.max())\nprint(\":\", euclidean_distance.mean())\nprint(f\": {(euclidean_distance > 1e-3).mean():.2%}\")",
        "detail": "quant.tensor_diff",
        "documentation": {}
    },
    {
        "label": "num",
        "kind": 5,
        "importPath": "quant.tensor_diff",
        "description": "quant.tensor_diff",
        "peekOfCode": "num = float(np.dot(arr_bin_flat,arr_npy_flat))\ndemon = np.linalg.norm(arr_bin_flat)*np.linalg.norm(arr_npy_flat)\nprint(num/demon)\neuclidean_distance = np.linalg.norm(arr_bin_flat - arr_npy_flat)\nprint(\":\", euclidean_distance.max())\nprint(\":\", euclidean_distance.mean())\nprint(f\": {(euclidean_distance > 1e-3).mean():.2%}\")",
        "detail": "quant.tensor_diff",
        "documentation": {}
    },
    {
        "label": "demon",
        "kind": 5,
        "importPath": "quant.tensor_diff",
        "description": "quant.tensor_diff",
        "peekOfCode": "demon = np.linalg.norm(arr_bin_flat)*np.linalg.norm(arr_npy_flat)\nprint(num/demon)\neuclidean_distance = np.linalg.norm(arr_bin_flat - arr_npy_flat)\nprint(\":\", euclidean_distance.max())\nprint(\":\", euclidean_distance.mean())\nprint(f\": {(euclidean_distance > 1e-3).mean():.2%}\")",
        "detail": "quant.tensor_diff",
        "documentation": {}
    },
    {
        "label": "euclidean_distance",
        "kind": 5,
        "importPath": "quant.tensor_diff",
        "description": "quant.tensor_diff",
        "peekOfCode": "euclidean_distance = np.linalg.norm(arr_bin_flat - arr_npy_flat)\nprint(\":\", euclidean_distance.max())\nprint(\":\", euclidean_distance.mean())\nprint(f\": {(euclidean_distance > 1e-3).mean():.2%}\")",
        "detail": "quant.tensor_diff",
        "documentation": {}
    },
    {
        "label": "input_file",
        "kind": 5,
        "importPath": "quant.tensor_file_diff",
        "description": "quant.tensor_file_diff",
        "peekOfCode": "input_file = \"/home/fays007/lc/share/mgz4/compare_light_out/compare_result.csv\"\n#  CSV\ndf = pd.read_csv(input_file)\n# \npairs = list(zip(df.iloc[:, 1], df.iloc[:, 2]))\nfor layer, node in pairs:\n    # print(layer, \" -> \", node)\n    npy_file = '/home/fays007/lc/share/mgz4/compare_light_out/layerdump/' + layer + '_quanted.npy'\n    bin_file = '/home/fays007/Downloads/tensors_int16/' + node\n    if os.path.exists(npy_file) == False:",
        "detail": "quant.tensor_file_diff",
        "documentation": {}
    },
    {
        "label": "df",
        "kind": 5,
        "importPath": "quant.tensor_file_diff",
        "description": "quant.tensor_file_diff",
        "peekOfCode": "df = pd.read_csv(input_file)\n# \npairs = list(zip(df.iloc[:, 1], df.iloc[:, 2]))\nfor layer, node in pairs:\n    # print(layer, \" -> \", node)\n    npy_file = '/home/fays007/lc/share/mgz4/compare_light_out/layerdump/' + layer + '_quanted.npy'\n    bin_file = '/home/fays007/Downloads/tensors_int16/' + node\n    if os.path.exists(npy_file) == False:\n        print(f\"{npy_file} \")\n        continue",
        "detail": "quant.tensor_file_diff",
        "documentation": {}
    },
    {
        "label": "pairs",
        "kind": 5,
        "importPath": "quant.tensor_file_diff",
        "description": "quant.tensor_file_diff",
        "peekOfCode": "pairs = list(zip(df.iloc[:, 1], df.iloc[:, 2]))\nfor layer, node in pairs:\n    # print(layer, \" -> \", node)\n    npy_file = '/home/fays007/lc/share/mgz4/compare_light_out/layerdump/' + layer + '_quanted.npy'\n    bin_file = '/home/fays007/Downloads/tensors_int16/' + node\n    if os.path.exists(npy_file) == False:\n        print(f\"{npy_file} \")\n        continue\n    if os.path.exists(bin_file) == False:\n        print(f\"{bin_file} \")",
        "detail": "quant.tensor_file_diff",
        "documentation": {}
    },
    {
        "label": "txt_to_png",
        "kind": 2,
        "importPath": "quant.tensor_show",
        "description": "quant.tensor_show",
        "peekOfCode": "def txt_to_png(txt_file_path, output_png_path, width=input_size[0], height=input_size[1]):\n    # \n    with open(txt_file_path, 'r') as f:\n        lines = f.readlines()\n    # \n    disp_values = [float(line.strip()) for line in lines]\n    # \n    expected_size = width * height\n    if len(disp_values) != expected_size:\n        print(f\"({len(disp_values)})({expected_size})\")",
        "detail": "quant.tensor_show",
        "documentation": {}
    },
    {
        "label": "input_size",
        "kind": 5,
        "importPath": "quant.tensor_show",
        "description": "quant.tensor_show",
        "peekOfCode": "input_size = (512,256)\ndef txt_to_png(txt_file_path, output_png_path, width=input_size[0], height=input_size[1]):\n    # \n    with open(txt_file_path, 'r') as f:\n        lines = f.readlines()\n    # \n    disp_values = [float(line.strip()) for line in lines]\n    # \n    expected_size = width * height\n    if len(disp_values) != expected_size:",
        "detail": "quant.tensor_show",
        "documentation": {}
    },
    {
        "label": "load_int8_bin",
        "kind": 2,
        "importPath": "quant.visualInt8",
        "description": "quant.visualInt8",
        "peekOfCode": "def load_int8_bin(file_path: str, shape=INPUT_SHAPE) -> np.ndarray:\n    \"\"\"\n     bin  int8  reshape \n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} \")\n    data = np.fromfile(file_path, dtype=np.int8)\n    if data.size != np.prod(shape):\n        raise ValueError(f\"Bin  {data.size}  shape {shape} \")\n    return data.reshape(shape)",
        "detail": "quant.visualInt8",
        "documentation": {}
    },
    {
        "label": "dequantize_int8",
        "kind": 2,
        "importPath": "quant.visualInt8",
        "description": "quant.visualInt8",
        "peekOfCode": "def dequantize_int8(int8_tensor: np.ndarray, scale: float, zero_point: int = 0) -> np.ndarray:\n    \"\"\"\n     int8  float32\n    \"\"\"\n    return (int8_tensor.astype(np.float32) - zero_point) * scale\ndef int8_to_u8c3(int8_tensor: np.ndarray, scale: float, zero_point: int = 0) -> np.ndarray:\n    \"\"\"\n     int8 tensor  U8C3 RGB \n    int8_tensor: (1,3,H,W)  (3,H,W)\n    : (H,W,3) uint8",
        "detail": "quant.visualInt8",
        "documentation": {}
    },
    {
        "label": "int8_to_u8c3",
        "kind": 2,
        "importPath": "quant.visualInt8",
        "description": "quant.visualInt8",
        "peekOfCode": "def int8_to_u8c3(int8_tensor: np.ndarray, scale: float, zero_point: int = 0) -> np.ndarray:\n    \"\"\"\n     int8 tensor  U8C3 RGB \n    int8_tensor: (1,3,H,W)  (3,H,W)\n    : (H,W,3) uint8\n    \"\"\"\n    # \n    float_tensor = dequantize_int8(int8_tensor, scale, zero_point)\n    #  batch \n    if float_tensor.ndim == 4:",
        "detail": "quant.visualInt8",
        "documentation": {}
    },
    {
        "label": "save_u8c3_image",
        "kind": 2,
        "importPath": "quant.visualInt8",
        "description": "quant.visualInt8",
        "peekOfCode": "def save_u8c3_image(img: np.ndarray, path: str):\n    \"\"\"\n     U8C3 RGB  PNG\n    \"\"\"\n    Image.fromarray(img).save(path)\n    print(f\"Saved image to {path}, shape={img.shape}, dtype={img.dtype}\")\n# ================================\n# \n# ================================\ndef main():",
        "detail": "quant.visualInt8",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "quant.visualInt8",
        "description": "quant.visualInt8",
        "peekOfCode": "def main():\n    # 1.  int8 bin\n    left_int8  = load_int8_bin(LEFT_BIN_PATH)\n    right_int8 = load_int8_bin(RIGHT_BIN_PATH)\n    print(\"Loaded int8 tensors:\")\n    print(f\"  Left  : {left_int8.shape}, dtype={left_int8.dtype}\")\n    print(f\"  Right : {right_int8.shape}, dtype={right_int8.dtype}\")\n    # 2.  U8C3\n    left_u8c3  = int8_to_u8c3(left_int8,  SCALE_LEFT,  ZERO_POINT)\n    right_u8c3 = int8_to_u8c3(right_int8, SCALE_RIGHT, ZERO_POINT)",
        "detail": "quant.visualInt8",
        "documentation": {}
    },
    {
        "label": "INPUT_SHAPE",
        "kind": 5,
        "importPath": "quant.visualInt8",
        "description": "quant.visualInt8",
        "peekOfCode": "INPUT_SHAPE = (1, 3, 256, 512)  # (N,C,H,W)\nNORMALIZE_MEAN = [0.485, 0.456, 0.406]\nNORMALIZE_STD  = [0.229, 0.224, 0.225]\n# NPU int8 \nSCALE_LEFT  = 0.017425\nSCALE_RIGHT = 0.017288\nZERO_POINT  = 0\n#  bin \nLEFT_BIN_PATH  = \"left.npy\"\nRIGHT_BIN_PATH = \"right.npy\"",
        "detail": "quant.visualInt8",
        "documentation": {}
    },
    {
        "label": "NORMALIZE_MEAN",
        "kind": 5,
        "importPath": "quant.visualInt8",
        "description": "quant.visualInt8",
        "peekOfCode": "NORMALIZE_MEAN = [0.485, 0.456, 0.406]\nNORMALIZE_STD  = [0.229, 0.224, 0.225]\n# NPU int8 \nSCALE_LEFT  = 0.017425\nSCALE_RIGHT = 0.017288\nZERO_POINT  = 0\n#  bin \nLEFT_BIN_PATH  = \"left.npy\"\nRIGHT_BIN_PATH = \"right.npy\"\n#  PNG ",
        "detail": "quant.visualInt8",
        "documentation": {}
    },
    {
        "label": "SCALE_RIGHT",
        "kind": 5,
        "importPath": "quant.visualInt8",
        "description": "quant.visualInt8",
        "peekOfCode": "SCALE_RIGHT = 0.017288\nZERO_POINT  = 0\n#  bin \nLEFT_BIN_PATH  = \"left.npy\"\nRIGHT_BIN_PATH = \"right.npy\"\n#  PNG \nLEFT_PNG_PATH  = \"output/left_image_u8c3.png\"\nRIGHT_PNG_PATH = \"output/right_image_u8c3.png\"\n# ================================\n# ",
        "detail": "quant.visualInt8",
        "documentation": {}
    },
    {
        "label": "RIGHT_BIN_PATH",
        "kind": 5,
        "importPath": "quant.visualInt8",
        "description": "quant.visualInt8",
        "peekOfCode": "RIGHT_BIN_PATH = \"right.npy\"\n#  PNG \nLEFT_PNG_PATH  = \"output/left_image_u8c3.png\"\nRIGHT_PNG_PATH = \"output/right_image_u8c3.png\"\n# ================================\n# \n# ================================\ndef load_int8_bin(file_path: str, shape=INPUT_SHAPE) -> np.ndarray:\n    \"\"\"\n     bin  int8  reshape ",
        "detail": "quant.visualInt8",
        "documentation": {}
    },
    {
        "label": "RIGHT_PNG_PATH",
        "kind": 5,
        "importPath": "quant.visualInt8",
        "description": "quant.visualInt8",
        "peekOfCode": "RIGHT_PNG_PATH = \"output/right_image_u8c3.png\"\n# ================================\n# \n# ================================\ndef load_int8_bin(file_path: str, shape=INPUT_SHAPE) -> np.ndarray:\n    \"\"\"\n     bin  int8  reshape \n    \"\"\"\n    if not os.path.exists(file_path):\n        raise FileNotFoundError(f\"{file_path} \")",
        "detail": "quant.visualInt8",
        "documentation": {}
    },
    {
        "label": "readpfm",
        "kind": 2,
        "importPath": "stereo.datasets.dataset_utils.readpfm",
        "description": "stereo.datasets.dataset_utils.readpfm",
        "peekOfCode": "def readpfm(file):\n    file = open(file, 'rb')\n    color = None\n    width = None\n    height = None\n    scale = None\n    endian = None\n    header = file.readline().rstrip()\n    if (sys.version[0]) == '3':\n        header = header.decode('utf-8')",
        "detail": "stereo.datasets.dataset_utils.readpfm",
        "documentation": {}
    },
    {
        "label": "Compose",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n    def __call__(self, sample):\n        for t in self.transforms:\n            sample = t(sample)\n        return sample\nclass TransposeImage(object):\n    def __init__(self, config):\n        self.config = config",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "TransposeImage",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class TransposeImage(object):\n    def __init__(self, config):\n        self.config = config\n    def __call__(self, sample):\n        sample['left'] = sample['left'].transpose((2, 0, 1))\n        sample['right'] = sample['right'].transpose((2, 0, 1))\n        return sample\nclass ToTensor(object):\n    def __init__(self, config):\n        self.config = config",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "ToTensor",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class ToTensor(object):\n    def __init__(self, config):\n        self.config = config\n    def __call__(self, sample):\n        for k in sample.keys():\n            if isinstance(sample[k], np.ndarray):\n                if k == 'super_pixel_label':\n                    sample[k] = torch.from_numpy(sample[k].copy()).to(torch.int32)\n                elif k in ['occ_mask', 'occ_mask_2']:\n                    sample[k] = torch.from_numpy(sample[k].copy()).to(torch.bool)",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "NormalizeImage",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class NormalizeImage(object):\n    def __init__(self, config):\n        self.mean = config.MEAN\n        self.std = config.STD\n    def __call__(self, sample):\n        sample['left'] = normalize(sample['left'] / 255.0, self.mean, self.std)\n        sample['right'] = normalize(sample['right'] / 255.0, self.mean, self.std)\n        return sample\nclass RandomCrop(object):\n    def __init__(self, config):",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RandomCrop",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RandomCrop(object):\n    def __init__(self, config):\n        self.crop_size = config.SIZE\n        self.base_size = config.SIZE\n        self.y_jitter = config.get('Y_JITTER', False)\n    def __call__(self, sample):\n        crop_height, crop_width = self.crop_size\n        height, width = sample['left'].shape[:2]  # (H, W, 3)\n        # crop_height = min(height, crop_height)\n        # crop_width = min(width, crop_width)",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RandomScale",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RandomScale(object):\n    def __init__(self, config):\n        self.config = config\n        self.crop_size = config.SIZE\n        self.min_scale = config.MIN_SCALE\n        self.max_scale = config.MAX_SCALE\n        self.scale_prob = config.SCALE_PROB\n        self.stretch_prob = config.STRETCH_PROB\n        self.max_stretch = 0.2\n    def __call__(self, sample):",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RandomSparseScale",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RandomSparseScale(object):\n    def __init__(self, config):\n        self.crop_size = config.SIZE\n        self.min_scale = config.MIN_SCALE\n        self.max_scale = config.MAX_SCALE\n        self.scale_prob = config.SCALE_PROB\n    def __call__(self, sample):\n        ht, wd = sample['left'].shape[:2]\n        min_scale = np.maximum((self.crop_size[0] + 1) / float(ht), (self.crop_size[1] + 1) / float(wd))\n        scale = 2 ** np.random.uniform(self.min_scale, self.max_scale)",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RandomErase",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RandomErase(object):\n    def __init__(self, config):\n        self.eraser_aug_prob = config.PROB\n        self.max_erase_time = config.MAX_TIME\n        self.bounds = config.BOUNDS\n    def __call__(self, sample):\n        img1 = sample['left']\n        img2 = sample['right']\n        ht, wd = img1.shape[:2]\n        if 'super_pixel_label' in sample:",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "StereoColorJitter",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class StereoColorJitter(object):\n    def __init__(self, config):\n        self.brightness = list(config.BRIGHTNESS)\n        self.contrast = list(config.CONTRAST)\n        self.saturation = list(config.SATURATION)\n        if isinstance(config.HUE, float):\n            config.HUE = [-config.HUE, config.HUE]\n        self.hue = list(config.HUE)\n        self.asymmetric_color_aug_prob = config.ASYMMETRIC_PROB\n        self.color_jitter = ColorJitter(brightness=self.brightness, contrast=self.contrast,",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RightTopPad",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RightTopPad(object):\n    def __init__(self, config):\n        self.size = config.SIZE\n    def __call__(self, sample):\n        h, w = sample['left'].shape[:2]\n        th, tw = self.size\n        h = min(h, th)  # ensure h is within the bounds of the image\n        w = min(w, tw)  # ensure w is within the bounds of the image\n        pad_left = 0\n        pad_right = tw - w",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "DivisiblePad",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class DivisiblePad(object):\n    def __init__(self, config):\n        self.by = config.BY\n        self.mode = config.get('MODE', 'tr')\n    def __call__(self, sample):\n        h, w = sample['left'].shape[:2]\n        if h % self.by != 0:\n            pad_h = self.by - h % self.by\n        else:\n            pad_h = 0",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RandomFlip",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RandomFlip(object):\n    def __init__(self, config):\n        self.config = config\n        self.flip_type = config.FLIP_TYPE\n        self.prob = config.PROB\n    def __call__(self, sample):\n        img1 = sample['left']\n        img2 = sample['right']\n        disp = sample['disp']\n        disp_right = sample['disp_right']",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "RightBottomCrop",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class RightBottomCrop(object):\n    def __init__(self, config):\n        self.size = config.SIZE\n    def __call__(self, sample):\n        h, w = sample['left'].shape[:2]\n        crop_h, crop_w = self.size\n        crop_h = min(h, crop_h)\n        crop_w = min(w, crop_w)\n        for k in sample.keys():\n            sample[k] = sample[k][h - crop_h:, w - crop_w:]",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "CropOrPad",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_utils.stereo_trans",
        "description": "stereo.datasets.dataset_utils.stereo_trans",
        "peekOfCode": "class CropOrPad(object):\n    def __init__(self, config):\n        self.size = config.SIZE\n        self.crop_fn = RightBottomCrop(config)\n        self.pad_fn = RightTopPad(config)\n    def __call__(self, sample):\n        h, w = sample['left'].shape[:2]\n        th, tw = self.size\n        if th > h or tw > w:\n            sample = self.pad_fn(sample)",
        "detail": "stereo.datasets.dataset_utils.stereo_trans",
        "documentation": {}
    },
    {
        "label": "ArgoverseDataset",
        "kind": 6,
        "importPath": "stereo.datasets.argoverse_dataset",
        "description": "stereo.datasets.argoverse_dataset",
        "peekOfCode": "class ArgoverseDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "stereo.datasets.argoverse_dataset",
        "documentation": {}
    },
    {
        "label": "CarlaDataset",
        "kind": 6,
        "importPath": "stereo.datasets.carla_dataset",
        "description": "stereo.datasets.carla_dataset",
        "peekOfCode": "class CarlaDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "stereo.datasets.carla_dataset",
        "documentation": {}
    },
    {
        "label": "CREStereoDataset",
        "kind": 6,
        "importPath": "stereo.datasets.crestereo_dataset",
        "description": "stereo.datasets.crestereo_dataset",
        "peekOfCode": "class CREStereoDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "stereo.datasets.crestereo_dataset",
        "documentation": {}
    },
    {
        "label": "DatasetTemplate",
        "kind": 6,
        "importPath": "stereo.datasets.dataset_template",
        "description": "stereo.datasets.dataset_template",
        "peekOfCode": "class DatasetTemplate(torch_data.Dataset):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__()\n        self.data_info = data_info\n        self.data_cfg = data_cfg\n        self.mode = mode\n        self.root = self.data_info.DATA_PATH\n        if self.mode.upper() in self.data_info.DATA_SPLIT:\n            self.split_file = self.data_info.DATA_SPLIT[self.mode.upper()]\n            transform_config = self.data_cfg.DATA_TRANSFORM[self.mode.upper()]",
        "detail": "stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "build_transform_by_cfg",
        "kind": 2,
        "importPath": "stereo.datasets.dataset_template",
        "description": "stereo.datasets.dataset_template",
        "peekOfCode": "def build_transform_by_cfg(transform_config):\n    transform_compose = []\n    for cur_cfg in transform_config:\n        cur_augmentor = getattr(stereo_trans, cur_cfg.NAME)(config=cur_cfg)\n        transform_compose.append(cur_augmentor)\n    return stereo_trans.Compose(transform_compose)\nclass DatasetTemplate(torch_data.Dataset):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__()\n        self.data_info = data_info",
        "detail": "stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "get_size",
        "kind": 2,
        "importPath": "stereo.datasets.dataset_template",
        "description": "stereo.datasets.dataset_template",
        "peekOfCode": "def get_size(base_size, w_range, h_range, random_type):\n    if random_type == 'range':\n        w = random.randint(int(w_range[0] * base_size[1]), int(w_range[1] * base_size[1]))\n        h = random.randint(int(h_range[0] * base_size[0]), int(h_range[1] * base_size[0]))\n    elif random_type == 'choice':\n        w = random.choice(w_range) if isinstance(w_range, list) else w_range\n        h = random.choice(h_range) if isinstance(h_range, list) else h_range\n    else:\n        raise NotImplementedError\n    return int(h), int(w)",
        "detail": "stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "custom_collate",
        "kind": 2,
        "importPath": "stereo.datasets.dataset_template",
        "description": "stereo.datasets.dataset_template",
        "peekOfCode": "def custom_collate(data_list, concat_dataset, batch_uniform=False,\n                   random_type=None, h_range=None, w_range=None):\n    if batch_uniform:\n        for each_dataset in concat_dataset.datasets:\n            for cur_t in each_dataset.transform.transforms:\n                if type(cur_t).__name__ == 'RandomCrop':\n                    base_size = cur_t.base_size\n                    cur_t.crop_size = get_size(base_size, w_range, h_range, random_type)\n                    break\n    return torch_data.default_collate(data_list)",
        "detail": "stereo.datasets.dataset_template",
        "documentation": {}
    },
    {
        "label": "DrivingDataset",
        "kind": 6,
        "importPath": "stereo.datasets.driving_dataset",
        "description": "stereo.datasets.driving_dataset",
        "peekOfCode": "class DrivingDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        self.use_noc = self.data_info.get('USE_NOC', False)\n        # random.shuffle(self.data_list)\n        # self.data_list = self.data_list[:10000]\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "stereo.datasets.driving_dataset",
        "documentation": {}
    },
    {
        "label": "DynamicReplicaDataset",
        "kind": 6,
        "importPath": "stereo.datasets.dynamic_replica",
        "description": "stereo.datasets.dynamic_replica",
        "peekOfCode": "class DynamicReplicaDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "stereo.datasets.dynamic_replica",
        "documentation": {}
    },
    {
        "label": "ETH3DDataset",
        "kind": 6,
        "importPath": "stereo.datasets.eth3d_dataset",
        "description": "stereo.datasets.eth3d_dataset",
        "peekOfCode": "class ETH3DDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_POS'):\n            self.retrun_pos = self.data_info.RETURN_POS\n        else:\n            self.retrun_pos = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "stereo.datasets.eth3d_dataset",
        "documentation": {}
    },
    {
        "label": "FallingThingsDataset",
        "kind": 6,
        "importPath": "stereo.datasets.fallingthings_dataset",
        "description": "stereo.datasets.fallingthings_dataset",
        "peekOfCode": "class FallingThingsDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "stereo.datasets.fallingthings_dataset",
        "documentation": {}
    },
    {
        "label": "FoundationStereoDataset",
        "kind": 6,
        "importPath": "stereo.datasets.foundationstereo",
        "description": "stereo.datasets.foundationstereo",
        "peekOfCode": "class FoundationStereoDataset(torch_data.Dataset):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__()\n        self.data_info = data_info\n        self.data_cfg = data_cfg\n        self.mode = mode\n        self.root = self.data_info.DATA_PATH\n        self.data_list = []\n        if self.mode.upper() in self.data_info.DATA_SPLIT:\n            transform_config = self.data_cfg.DATA_TRANSFORM[self.mode.upper()]",
        "detail": "stereo.datasets.foundationstereo",
        "documentation": {}
    },
    {
        "label": "depth_uint8_decoding",
        "kind": 2,
        "importPath": "stereo.datasets.foundationstereo",
        "description": "stereo.datasets.foundationstereo",
        "peekOfCode": "def depth_uint8_decoding(depth_uint8, scale=1000):\n    depth_uint8 = depth_uint8.astype(float)\n    out = depth_uint8[...,0]*255*255 + depth_uint8[..., 1]*255 + depth_uint8[..., 2]\n    return out/float(scale)\nclass FoundationStereoDataset(torch_data.Dataset):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__()\n        self.data_info = data_info\n        self.data_cfg = data_cfg\n        self.mode = mode",
        "detail": "stereo.datasets.foundationstereo",
        "documentation": {}
    },
    {
        "label": "InStereo2KDataset",
        "kind": 6,
        "importPath": "stereo.datasets.instereo2k_dataset",
        "description": "stereo.datasets.instereo2k_dataset",
        "peekOfCode": "class InStereo2KDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "stereo.datasets.instereo2k_dataset",
        "documentation": {}
    },
    {
        "label": "KittiDataset",
        "kind": 6,
        "importPath": "stereo.datasets.kitti_dataset",
        "description": "stereo.datasets.kitti_dataset",
        "peekOfCode": "class KittiDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        self.use_noc = self.data_info.get('USE_NOC', False)\n        if hasattr(self.data_info, 'RETURN_POS'):\n            self.retrun_pos = self.data_info.RETURN_POS\n        else:\n            self.retrun_pos = False\n    def __getitem__(self, idx):",
        "detail": "stereo.datasets.kitti_dataset",
        "documentation": {}
    },
    {
        "label": "MiddleburyDataset",
        "kind": 6,
        "importPath": "stereo.datasets.middlebury_dataset",
        "description": "stereo.datasets.middlebury_dataset",
        "peekOfCode": "class MiddleburyDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_POS'):\n            self.retrun_pos = self.data_info.RETURN_POS\n        else:\n            self.retrun_pos = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "stereo.datasets.middlebury_dataset",
        "documentation": {}
    },
    {
        "label": "SceneFlowDataset",
        "kind": 6,
        "importPath": "stereo.datasets.sceneflow_dataset",
        "description": "stereo.datasets.sceneflow_dataset",
        "peekOfCode": "class SceneFlowDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_POS'):\n            self.retrun_pos = self.data_info.RETURN_POS\n        else:\n            self.retrun_pos = False\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:",
        "detail": "stereo.datasets.sceneflow_dataset",
        "documentation": {}
    },
    {
        "label": "FlyingThings3DSubsetDataset",
        "kind": 6,
        "importPath": "stereo.datasets.sceneflow_dataset",
        "description": "stereo.datasets.sceneflow_dataset",
        "peekOfCode": "class FlyingThings3DSubsetDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_occ_mask = self.data_info.RETURN_OCC_MASK\n        self.zeroing_occ = self.data_info.ZEROING_OCC\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item[0:6]]\n        left_img_path, right_img_path, disp_img_path, disp_img_right_path, occ_path, occ_right_path = full_paths\n        left_img = Image.open(left_img_path).convert('RGB')",
        "detail": "stereo.datasets.sceneflow_dataset",
        "documentation": {}
    },
    {
        "label": "SintelDataset",
        "kind": 6,
        "importPath": "stereo.datasets.sintel_dataset",
        "description": "stereo.datasets.sintel_dataset",
        "peekOfCode": "class SintelDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "stereo.datasets.sintel_dataset",
        "documentation": {}
    },
    {
        "label": "SpringDataset",
        "kind": 6,
        "importPath": "stereo.datasets.spring",
        "description": "stereo.datasets.spring",
        "peekOfCode": "class SpringDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "stereo.datasets.spring",
        "documentation": {}
    },
    {
        "label": "TartanAirDataset",
        "kind": 6,
        "importPath": "stereo.datasets.tartanair_dataset",
        "description": "stereo.datasets.tartanair_dataset",
        "peekOfCode": "class TartanAirDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]",
        "detail": "stereo.datasets.tartanair_dataset",
        "documentation": {}
    },
    {
        "label": "UnrealStereo4KDataset",
        "kind": 6,
        "importPath": "stereo.datasets.unrealstereo4k_dataset",
        "description": "stereo.datasets.unrealstereo4k_dataset",
        "peekOfCode": "class UnrealStereo4KDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "stereo.datasets.unrealstereo4k_dataset",
        "documentation": {}
    },
    {
        "label": "VirtualKitti2Dataset",
        "kind": 6,
        "importPath": "stereo.datasets.vkitti2_dataset",
        "description": "stereo.datasets.vkitti2_dataset",
        "peekOfCode": "class VirtualKitti2Dataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode):\n        super().__init__(data_info, data_cfg, mode)\n        self.return_right_disp = self.data_info.RETURN_RIGHT_DISP\n        if hasattr(self.data_info, 'RETURN_SUPER_PIXEL'):\n            self.retrun_super_pixel = self.data_info.RETURN_SUPER_PIXEL\n        else:\n            self.retrun_super_pixel = False\n    def __getitem__(self, idx):\n        item = self.data_list[idx]",
        "detail": "stereo.datasets.vkitti2_dataset",
        "documentation": {}
    },
    {
        "label": "get_disp",
        "kind": 2,
        "importPath": "stereo.datasets.vkitti2_dataset",
        "description": "stereo.datasets.vkitti2_dataset",
        "peekOfCode": "def get_disp(file_path, checkinvalid=True):\n    if '.png' in file_path:\n        depth = cv2.imread(file_path, cv2.IMREAD_ANYDEPTH | cv2.IMREAD_ANYCOLOR)\n        invalid = depth >= 65535\n        num_invalid = depth[invalid].shape[0]\n        depth = depth / 100.0\n    else:\n        raise TypeError('only support png and npy format, invalid type found: {}'.format(file_path))\n    f = 725.0087\n    b = 0.532725 # meter",
        "detail": "stereo.datasets.vkitti2_dataset",
        "documentation": {}
    },
    {
        "label": "d1_metric",
        "kind": 2,
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "peekOfCode": "def d1_metric(disp_pred, disp_gt, mask):\n    E = torch.abs(disp_gt - disp_pred)\n    err_mask = (E > 3) & (E / torch.abs(disp_gt) > 0.05)\n    err_mask = err_mask & mask\n    num_errors = err_mask.sum(dim=[1, 2])\n    num_valid_pixels = mask.sum(dim=[1, 2])\n    d1_per_image = num_errors.float() / num_valid_pixels.float() * 100\n    d1_per_image = torch.where(num_valid_pixels > 0, d1_per_image, torch.zeros_like(d1_per_image))\n    return d1_per_image\ndef threshold_metric(disp_pred, disp_gt, mask, threshold):",
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "threshold_metric",
        "kind": 2,
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "peekOfCode": "def threshold_metric(disp_pred, disp_gt, mask, threshold):\n    E = torch.abs(disp_gt - disp_pred)\n    err_mask = E > threshold\n    err_mask = err_mask & mask\n    num_errors = err_mask.sum(dim=[1, 2])\n    num_valid_pixels = mask.sum(dim=[1, 2])\n    bad_per_image = num_errors.float() / num_valid_pixels.float() * 100\n    bad_per_image = torch.where(num_valid_pixels > 0, bad_per_image, torch.zeros_like(bad_per_image))\n    return bad_per_image\ndef epe_metric(disp_pred, disp_gt, mask):",
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "epe_metric",
        "kind": 2,
        "importPath": "stereo.evaluation.metric_per_image",
        "description": "stereo.evaluation.metric_per_image",
        "peekOfCode": "def epe_metric(disp_pred, disp_gt, mask):\n    E = torch.abs(disp_gt - disp_pred)\n    E_masked = torch.where(mask, E, torch.zeros_like(E))\n    E_sum = E_masked.sum(dim=[1, 2])\n    num_valid_pixels = mask.sum(dim=[1, 2])\n    epe_per_image = E_sum / num_valid_pixels\n    epe_per_image = torch.where(num_valid_pixels > 0, epe_per_image, torch.zeros_like(epe_per_image))\n    return epe_per_image",
        "detail": "stereo.evaluation.metric_per_image",
        "documentation": {}
    },
    {
        "label": "DeformConvFunction",
        "kind": 6,
        "importPath": "stereo.libs.AANet.deform_conv.deform_conv",
        "description": "stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class DeformConvFunction(Function):\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                weight,\n                stride=1,\n                padding=0,\n                dilation=1,\n                groups=1,",
        "detail": "stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "ModulatedDeformConvFunction",
        "kind": 6,
        "importPath": "stereo.libs.AANet.deform_conv.deform_conv",
        "description": "stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class ModulatedDeformConvFunction(Function):\n    @staticmethod\n    def forward(ctx,\n                input,\n                offset,\n                mask,\n                weight,\n                bias=None,\n                stride=1,\n                padding=0,",
        "detail": "stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "DeformConv",
        "kind": 6,
        "importPath": "stereo.libs.AANet.deform_conv.deform_conv",
        "description": "stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class DeformConv(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,",
        "detail": "stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "DeformConvPack",
        "kind": 6,
        "importPath": "stereo.libs.AANet.deform_conv.deform_conv",
        "description": "stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class DeformConvPack(DeformConv):\n    \"\"\"A Deformable Conv Encapsulation that acts as normal Conv layers.\n    Args:\n        in_channels (int): Same as nn.Conv2d.\n        out_channels (int): Same as nn.Conv2d.\n        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n        stride (int or tuple[int]): Same as nn.Conv2d.\n        padding (int or tuple[int]): Same as nn.Conv2d.\n        dilation (int or tuple[int]): Same as nn.Conv2d.\n        groups (int): Same as nn.Conv2d.",
        "detail": "stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "ModulatedDeformConv",
        "kind": 6,
        "importPath": "stereo.libs.AANet.deform_conv.deform_conv",
        "description": "stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class ModulatedDeformConv(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 deformable_groups=1,",
        "detail": "stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "ModulatedDeformConvPack",
        "kind": 6,
        "importPath": "stereo.libs.AANet.deform_conv.deform_conv",
        "description": "stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "class ModulatedDeformConvPack(ModulatedDeformConv):\n    \"\"\"A ModulatedDeformable Conv Encapsulation that acts as normal Conv layers.\n    Args:\n        in_channels (int): Same as nn.Conv2d.\n        out_channels (int): Same as nn.Conv2d.\n        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n        stride (int or tuple[int]): Same as nn.Conv2d.\n        padding (int or tuple[int]): Same as nn.Conv2d.\n        dilation (int or tuple[int]): Same as nn.Conv2d.\n        groups (int): Same as nn.Conv2d.",
        "detail": "stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "deform_conv",
        "kind": 5,
        "importPath": "stereo.libs.AANet.deform_conv.deform_conv",
        "description": "stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "deform_conv = DeformConvFunction.apply\nmodulated_deform_conv = ModulatedDeformConvFunction.apply\nclass DeformConv(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,",
        "detail": "stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "modulated_deform_conv",
        "kind": 5,
        "importPath": "stereo.libs.AANet.deform_conv.deform_conv",
        "description": "stereo.libs.AANet.deform_conv.deform_conv",
        "peekOfCode": "modulated_deform_conv = ModulatedDeformConvFunction.apply\nclass DeformConv(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,",
        "detail": "stereo.libs.AANet.deform_conv.deform_conv",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "stereo.modeling.backbones.fadnet",
        "description": "stereo.modeling.backbones.fadnet",
        "peekOfCode": "class ResBlock(nn.Module):\n    def __init__(self, n_in, n_out, stride=1):\n        super(ResBlock, self).__init__()\n        self.conv1 = nn.Conv2d(n_in, n_out, kernel_size=3, stride=stride, padding=1)\n        self.bn1 = nn.BatchNorm2d(n_out)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(n_out, n_out, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(n_out)\n        if stride != 1 or n_out != n_in:\n            self.shortcut = nn.Sequential(",
        "detail": "stereo.modeling.backbones.fadnet",
        "documentation": {}
    },
    {
        "label": "FadnetBackbone",
        "kind": 6,
        "importPath": "stereo.modeling.backbones.fadnet",
        "description": "stereo.modeling.backbones.fadnet",
        "peekOfCode": "class FadnetBackbone(nn.Module):\n    def __init__(self, resblock=True, input_channel=3, encoder_ratio=16, decoder_ratio=16):\n        super(FadnetBackbone, self).__init__()\n        self.input_channel = input_channel\n        self.basicC = 2\n        self.eratio = encoder_ratio\n        self.dratio = decoder_ratio\n        self.basicE = self.basicC * self.eratio\n        self.basicD = self.basicC * self.dratio\n        # shrink and extract features",
        "detail": "stereo.modeling.backbones.fadnet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.backbones.psmnet_backbone",
        "description": "stereo.modeling.backbones.psmnet_backbone",
        "peekOfCode": "class BasicBlock(nn.Module):\n    def __init__(self, downsample, in_planes, out_planes, stride, padding):\n        super(BasicBlock, self).__init__()\n        self.conv1 = BasicConv2d(in_channels=in_planes, out_channels=out_planes,\n                                 norm_layer=nn.BatchNorm2d,\n                                 act_layer=partial(nn.ReLU, inplace=True),\n                                 kernel_size=3, stride=stride, padding=padding)\n        self.conv2 = BasicConv2d(in_channels=out_planes, out_channels=out_planes,\n                                 norm_layer=nn.BatchNorm2d, act_layer=None,\n                                 kernel_size=3, stride=1, padding=padding)",
        "detail": "stereo.modeling.backbones.psmnet_backbone",
        "documentation": {}
    },
    {
        "label": "PsmnetBackbone",
        "kind": 6,
        "importPath": "stereo.modeling.backbones.psmnet_backbone",
        "description": "stereo.modeling.backbones.psmnet_backbone",
        "peekOfCode": "class PsmnetBackbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.firstconv = nn.Sequential(\n            BasicConv2d(3, out_channels=32,\n                        norm_layer=nn.BatchNorm2d, act_layer=partial(nn.ReLU, inplace=True),\n                        kernel_size=3, stride=2, padding=1),\n            BasicConv2d(32, out_channels=32,\n                        norm_layer=nn.BatchNorm2d, act_layer=partial(nn.ReLU, inplace=True),\n                        kernel_size=3, stride=1, padding=1),",
        "detail": "stereo.modeling.backbones.psmnet_backbone",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "kind": 6,
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "peekOfCode": "class BasicConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, bias=False,\n                 norm_layer=None, act_layer=None, **kwargs):\n        super(BasicConv2d, self).__init__()\n        layers = [nn.Conv2d(in_channels, out_channels,\n                            kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, **kwargs)]\n        if norm_layer is not None:\n            layers.append(norm_layer(out_channels))\n        if act_layer is not None:\n            layers.append(act_layer())",
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv2d",
        "kind": 6,
        "importPath": "stereo.modeling.common.basic_block_2d",
        "description": "stereo.modeling.common.basic_block_2d",
        "peekOfCode": "class BasicDeconv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False,\n                 norm_layer=None, act_layer=None, **kwargs):\n        super(BasicDeconv2d, self).__init__()\n        layers = [nn.ConvTranspose2d(in_channels, out_channels,\n                                     kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, **kwargs)]\n        if norm_layer is not None:\n            layers.append(norm_layer(out_channels))\n        if act_layer is not None:\n            layers.append(act_layer())",
        "detail": "stereo.modeling.common.basic_block_2d",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "kind": 6,
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "peekOfCode": "class BasicConv3d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, bias=False,\n                 norm_layer=None, act_layer=None, **kwargs):\n        super(BasicConv3d, self).__init__()\n        layers = [nn.Conv3d(in_channels, out_channels,\n                            kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, **kwargs)]\n        if norm_layer is not None:\n            layers.append(norm_layer(out_channels))\n        if act_layer is not None:\n            layers.append(act_layer())",
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BasicDeconv3d",
        "kind": 6,
        "importPath": "stereo.modeling.common.basic_block_3d",
        "description": "stereo.modeling.common.basic_block_3d",
        "peekOfCode": "class BasicDeconv3d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False,\n                 norm_layer=None, act_layer=None, **kwargs):\n        super(BasicDeconv3d, self).__init__()\n        layers = [nn.ConvTranspose3d(in_channels, out_channels,\n                                     kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, **kwargs)]\n        if norm_layer is not None:\n            layers.append(norm_layer(out_channels))\n        if act_layer is not None:\n            layers.append(act_layer())",
        "detail": "stereo.modeling.common.basic_block_3d",
        "documentation": {}
    },
    {
        "label": "BCELoss",
        "kind": 6,
        "importPath": "stereo.modeling.common.loss",
        "description": "stereo.modeling.common.loss",
        "peekOfCode": "class BCELoss:  # nn.BCELoss()\n    def __init__(self):\n        self.reduction = 'mean'\n    def __call__(self, inputs, targets):\n        \"\"\"\n        no c dim, because each item means a prob\n        :param inputs: [bz, d1, d2, ...], after softmax or sigmod\n        :param targets: [bz, d1, d2, ...], same shape with inputs, between 0 and 1, such as 0, 0.7, 1\n        :return:\n        t = torch.rand(size=[4, 3])",
        "detail": "stereo.modeling.common.loss",
        "documentation": {}
    },
    {
        "label": "BCEWithLogitsLoss",
        "kind": 6,
        "importPath": "stereo.modeling.common.loss",
        "description": "stereo.modeling.common.loss",
        "peekOfCode": "class BCEWithLogitsLoss:  # nn.BCEWithLogitsLoss()\n    def __call__(self, inputs, targets):\n        \"\"\"\n        t = torch.rand(size=[4, 3])\n        a = torch.randn(size=[4, 3])\n        b = F.sigmoid(a)\n        nn.BCEWithLogitsLoss()(a, t) == BCEWithLogitsLoss()(a, t) == BCELoss()(b, t)\n        \"\"\"\n        inputs = F.sigmoid(inputs)\n        return BCELoss()(inputs, targets)",
        "detail": "stereo.modeling.common.loss",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "kind": 6,
        "importPath": "stereo.modeling.common.loss",
        "description": "stereo.modeling.common.loss",
        "peekOfCode": "class CrossEntropyLoss:  # nn.CrossEntropyLoss()\n    def __call__(self, inputs, targets):\n        \"\"\"\n        :param inputs: [bz, c, d1, d2, ...], before softmax\n        :param targets: [bz, d1, d2, ...], value in the range [0, c), or [bz, c, d1, d2, ...], between 0 and 1, such as 0.7\n        :return:\n        \"\"\"\n        inputs = F.softmax(inputs, dim=1)\n        inputs = torch.log(inputs)\n        if targets.shape != inputs.shape:",
        "detail": "stereo.modeling.common.loss",
        "documentation": {}
    },
    {
        "label": "KLDivLoss",
        "kind": 6,
        "importPath": "stereo.modeling.common.loss",
        "description": "stereo.modeling.common.loss",
        "peekOfCode": "class KLDivLoss:\n    def __init__(self):\n        self.reduction = 'mean'\n    def __call__(self, inputs, targets):\n        loss_pointwise = targets * (torch.log(targets) - inputs)\n        if self.reduction == \"mean\":\n            loss = loss_pointwise.mean()\n        elif self.reduction == \"batchmean\":\n            loss = loss_pointwise.sum() / inputs.shape[0]\n        else:",
        "detail": "stereo.modeling.common.loss",
        "documentation": {}
    },
    {
        "label": "Hourglass",
        "kind": 6,
        "importPath": "stereo.modeling.cost_aggregation.hourglass",
        "description": "stereo.modeling.cost_aggregation.hourglass",
        "peekOfCode": "class Hourglass(nn.Module):\n    def __init__(self, in_channels, backbone_channels=None):\n        super(Hourglass, self).__init__()\n        if backbone_channels is None:\n            backbone_channels = [48, 64, 192, 120]\n        self.conv1 = nn.Sequential(\n            BasicConv3d(in_channels, in_channels * 2,\n                        norm_layer=nn.BatchNorm3d, act_layer=nn.LeakyReLU,\n                        kernel_size=3, padding=1, stride=2, dilation=1),\n            BasicConv3d(in_channels * 2, in_channels * 2,",
        "detail": "stereo.modeling.cost_aggregation.hourglass",
        "documentation": {}
    },
    {
        "label": "CoExCostVolume",
        "kind": 6,
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "class CoExCostVolume(nn.Module):\n    def __init__(self, maxdisp, group=1):\n        super(CoExCostVolume, self).__init__()\n        self.maxdisp = maxdisp + 1\n        self.group = group\n        self.unfold = nn.Unfold((1, maxdisp + 1), 1, 0, 1)\n        self.left_pad = nn.ZeroPad2d((maxdisp, 0, 0, 0))\n    def forward(self, x, y):\n        b, c, h, w = x.shape\n        y = self.left_pad(y)",
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "InterlacedVolume",
        "kind": 6,
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "class InterlacedVolume(nn.Module):\n    def __init__(self, num_features=8):\n        super(InterlacedVolume, self).__init__()\n        self.num_features = num_features\n        self.conv3d = nn.Sequential(BasicConv3d(in_channels=1, out_channels=16,\n                                                norm_layer=nn.BatchNorm3d, act_layer=nn.ReLU,\n                                                kernel_size=(8, 3, 3), stride=(8, 1, 1), padding=(0, 1, 1)),\n                                    BasicConv3d(in_channels=16, out_channels=32,\n                                                norm_layer=nn.BatchNorm3d, act_layer=nn.ReLU,\n                                                kernel_size=(8, 3, 3), stride=(8, 1, 1), padding=(0, 1, 1)),",
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "correlation_volume",
        "kind": 2,
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def correlation_volume(left_feature, right_feature, max_disp):\n    b, c, h, w = left_feature.shape\n    padded_right = F.pad(right_feature, (max_disp, 0, 0, 0)) \n    cost_volume = torch.stack([\n        (left_feature * padded_right[:, :, :, max_disp - i : max_disp + w - i]).mean(dim=1)  # \n        for i in range(max_disp)\n    ], dim=1)  \n    return cost_volume.contiguous()\ndef compute_volume(reference_embedding, target_embedding, maxdisp, side='left'):\n    batch, channel, height, width = reference_embedding.size()",
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "compute_volume",
        "kind": 2,
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def compute_volume(reference_embedding, target_embedding, maxdisp, side='left'):\n    batch, channel, height, width = reference_embedding.size()\n    cost = torch.zeros(batch, channel, maxdisp, height, width, device='cuda').type_as(reference_embedding)\n    cost[:, :, 0, :, :] = reference_embedding - target_embedding\n    for idx in range(1, maxdisp):\n        if side == 'left':\n            cost[:, :, idx, :, idx:] = reference_embedding[:, :, :, idx:] - target_embedding[:, :, :, :-idx]\n        if side == 'right':\n            cost[:, :, idx, :, :-idx] = target_embedding[:, :, :, idx:] - reference_embedding[:, :, :, :-idx]\n    cost = cost.contiguous()",
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W], requires_grad=False)\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_corr_volume",
        "kind": 2,
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def build_corr_volume(img_left, img_right, max_disp):\n    B, C, H, W = img_left.shape\n    volume = img_left.new_zeros([B, max_disp, H, W])\n    for i in range(max_disp):\n        if (i > 0) & (i < W):\n            volume[:, i, :, i:] = (img_left[:, :, :, i:] * img_right[:, :, :, :W-i]).mean(dim=1)\n        else:\n            volume[:, i, :, :] = (img_left[:, :, :, :] * img_right[:, :, :, :]).mean(dim=1)\n    volume = volume.contiguous()\n    return volume",
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "build_sub_volume",
        "kind": 2,
        "importPath": "stereo.modeling.cost_volume.cost_volume",
        "description": "stereo.modeling.cost_volume.cost_volume",
        "peekOfCode": "def build_sub_volume(feat_l, feat_r, maxdisp):\n    cost = torch.zeros((feat_l.size()[0], maxdisp, feat_l.size()[2], feat_l.size()[3]), device='cuda')\n    for i in range(maxdisp):\n        cost[:, i, :, :i] = feat_l[:, :, :, :i].abs().sum(1)\n        if i > 0:\n            cost[:, i, :, i:] = torch.norm(feat_l[:, :, :, i:] - feat_r[:, :, :, :-i], 1, 1)\n        else:\n            cost[:, i, :, i:] = torch.norm(feat_l[:, :, :, :] - feat_r[:, :, :, :], 1, 1)\n    return cost.contiguous()\nclass InterlacedVolume(nn.Module):",
        "detail": "stereo.modeling.cost_volume.cost_volume",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "stereo.modeling.disp_pred.disp_regression",
        "description": "stereo.modeling.disp_pred.disp_regression",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=True)",
        "detail": "stereo.modeling.disp_pred.disp_regression",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.disp_refinement.disp_refinement",
        "description": "stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "class BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, padding, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = BasicConv2d(in_channels=in_planes, out_channels=out_planes,\n                                 norm_layer=nn.BatchNorm2d,\n                                 act_layer=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True),\n                                 kernel_size=3, stride=stride, padding=padding, dilation=dilation)\n        self.conv2 = BasicConv2d(in_channels=out_planes, out_channels=out_planes,\n                                 norm_layer=nn.BatchNorm2d, act_layer=None,\n                                 kernel_size=3, stride=1, padding=padding, dilation=dilation)",
        "detail": "stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "StereoNetRefinement",
        "kind": 6,
        "importPath": "stereo.modeling.disp_refinement.disp_refinement",
        "description": "stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "class StereoNetRefinement(nn.Module):\n    def __init__(self):\n        super(StereoNetRefinement, self).__init__()\n        # Original StereoNet: left, disp\n        self.conv = BasicConv2d(4, 32,\n                                norm_layer=nn.BatchNorm2d,\n                                act_layer=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True),\n                                kernel_size=3, stride=1, padding=1)\n        self.dilation_list = [1, 2, 4, 8, 1, 1]\n        self.dilated_blocks = nn.ModuleList()",
        "detail": "stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "StereoDRNetRefinement",
        "kind": 6,
        "importPath": "stereo.modeling.disp_refinement.disp_refinement",
        "description": "stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "class StereoDRNetRefinement(nn.Module):\n    def __init__(self):\n        super(StereoDRNetRefinement, self).__init__()\n        # Left and warped error\n        in_channels = 6\n        self.conv1 = BasicConv2d(in_channels, 16,\n                                norm_layer=nn.BatchNorm2d,\n                                act_layer=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True),\n                                kernel_size=3, stride=1, padding=1)\n        self.conv2 = BasicConv2d(1, 16,",
        "detail": "stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "meshgrid",
        "kind": 2,
        "importPath": "stereo.modeling.disp_refinement.disp_refinement",
        "description": "stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "def meshgrid(img, homogeneous=False):\n    \"\"\"Generate meshgrid in image scale\n    Args:\n        img: [B, _, H, W]\n        homogeneous: whether to return homogeneous coordinates\n    Return:\n        grid: [B, 2, H, W]\n    \"\"\"\n    b, _, h, w = img.size()\n    x_range = torch.arange(0, w).view(1, 1, w).expand(1, h, w).type_as(img)  # [1, H, W]",
        "detail": "stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "normalize_coords",
        "kind": 2,
        "importPath": "stereo.modeling.disp_refinement.disp_refinement",
        "description": "stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "def normalize_coords(grid):\n    \"\"\"Normalize coordinates of image scale to [-1, 1]\n    Args:\n        grid: [B, 2, H, W]\n    \"\"\"\n    assert grid.size(1) == 2\n    h, w = grid.size()[2:]\n    grid[:, 0, :, :] = 2 * (grid[:, 0, :, :].clone() / (w - 1)) - 1  # x: [-1, 1]\n    grid[:, 1, :, :] = 2 * (grid[:, 1, :, :].clone() / (h - 1)) - 1  # y: [-1, 1]\n    grid = grid.permute((0, 2, 3, 1))  # [B, H, W, 2]",
        "detail": "stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "disp_warp",
        "kind": 2,
        "importPath": "stereo.modeling.disp_refinement.disp_refinement",
        "description": "stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "def disp_warp(img, disp, padding_mode='border'):\n    \"\"\"Warping by disparity\n    Args:\n        img: [B, 3, H, W]\n        disp: [B, 1, H, W], positive\n        padding_mode: 'zeros' or 'border'\n    Returns:\n        warped_img: [B, 3, H, W]\n        valid_mask: [B, 3, H, W]\n    \"\"\"",
        "detail": "stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "context_upsample",
        "kind": 2,
        "importPath": "stereo.modeling.disp_refinement.disp_refinement",
        "description": "stereo.modeling.disp_refinement.disp_refinement",
        "peekOfCode": "def context_upsample(disp_low, up_weights, scale_factor=4):\n    # disp_low [b,1,h,w]\n    # up_weights [b,9,4*h,4*w]\n    b, c, h, w = disp_low.shape\n    disp_unfold = F.unfold(disp_low, kernel_size=3, dilation=1, padding=1)  # [bz, 3x3, hxw]\n    disp_unfold = disp_unfold.reshape(b, -1, h, w)  # [bz, 3x3, h, w]\n    disp_unfold = F.interpolate(disp_unfold, (h * scale_factor, w * scale_factor), mode='nearest')  # [bz, 3x3, 4h, 4w]\n    disp = (disp_unfold * up_weights).sum(1)  # # [bz, 4h, 4w]\n    return disp",
        "detail": "stereo.modeling.disp_refinement.disp_refinement",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "stereo.modeling.disp_refinement.gru_blocks",
        "description": "stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)",
        "detail": "stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "MultiBasicEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.disp_refinement.gru_blocks",
        "description": "stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class MultiBasicEncoder(nn.Module):\n    def __init__(self, output_dim=None, norm_fn='batch', downsample=3):\n        super(MultiBasicEncoder, self).__init__()\n        if output_dim is None:\n            output_dim = [128]\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1 + (downsample > 2), padding=3)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.norm_fn = norm_fn\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)",
        "detail": "stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "CombinedGeoEncodingVolume",
        "kind": 6,
        "importPath": "stereo.modeling.disp_refinement.gru_blocks",
        "description": "stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class CombinedGeoEncodingVolume:\n    def __init__(self, init_fmap1, init_fmap2, geo_volume, num_levels=2, radius=4):\n        self.num_levels = num_levels\n        self.radius = radius\n        self.geo_volume_pyramid = []\n        self.init_corr_pyramid = []\n        # all pairs correlation\n        init_corr = self.corr(init_fmap1, init_fmap2)  # [bz, H/4, W/4, 1, W/4]\n        b, h, w, _, w2 = init_corr.shape\n        b, c, d, h, w = geo_volume.shape  # [bz, channel, maxdisp/4, H/4, W/4]",
        "detail": "stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "BasicMotionEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.disp_refinement.gru_blocks",
        "description": "stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class BasicMotionEncoder(nn.Module):\n    def __init__(self, corr_levels, corr_radius, volume_channel):\n        super(BasicMotionEncoder, self).__init__()\n        cor_planes = corr_levels * (2 * corr_radius + 1) * (volume_channel + 1)\n        self.convc1 = nn.Conv2d(cor_planes, 64, 1, padding=0)\n        self.convc2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.convd1 = nn.Conv2d(1, 64, 7, padding=3)\n        self.convd2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.conv = nn.Conv2d(64 + 64, 128 - 1, 3, padding=1)\n    def forward(self, disp, corr):",
        "detail": "stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "ConvGRU",
        "kind": 6,
        "importPath": "stereo.modeling.disp_refinement.gru_blocks",
        "description": "stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class ConvGRU(nn.Module):\n    def __init__(self, hidden_dim, input_dim, kernel_size=3):\n        super(ConvGRU, self).__init__()\n        self.convz = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convq = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n    def forward(self, h, cz, cr, cq, *x_list):\n        x = torch.cat(x_list, dim=1)\n        hx = torch.cat([h, x], dim=1)\n        z = torch.sigmoid(self.convz(hx) + cz)",
        "detail": "stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "DispHead",
        "kind": 6,
        "importPath": "stereo.modeling.disp_refinement.gru_blocks",
        "description": "stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):\n        super(DispHead, self).__init__()\n        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 3, padding=1)\n        self.conv2 = nn.Conv2d(hidden_dim, output_dim, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\ndef pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)",
        "detail": "stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "BasicMultiUpdateBlock",
        "kind": 6,
        "importPath": "stereo.modeling.disp_refinement.gru_blocks",
        "description": "stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "class BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []\n        self.n_gru_layers = n_gru_layers\n        self.encoder = BasicMotionEncoder(corr_levels, corr_radius, volume_channel)\n        encoder_output_dim = 128\n        self.gru04 = ConvGRU(hidden_dims[2], encoder_output_dim + hidden_dims[1] * (self.n_gru_layers > 1))\n        self.gru08 = ConvGRU(hidden_dims[1], hidden_dims[0] * (self.n_gru_layers == 3) + hidden_dims[2])",
        "detail": "stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "kind": 2,
        "importPath": "stereo.modeling.disp_refinement.gru_blocks",
        "description": "stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "def bilinear_sampler(img, coords, mask=False):\n    \"\"\" Wrapper for grid_sample, uses pixel coordinates \"\"\"\n    H, W = img.shape[-2:]\n    xgrid, ygrid = coords.split([1, 1], dim=-1)\n    xgrid = 2 * xgrid / (W - 1) - 1\n    assert torch.unique(ygrid).numel() == 1 and H == 1  # This is a stereo problem\n    grid = torch.cat([xgrid, ygrid], dim=-1)\n    img = F.grid_sample(img, grid, align_corners=True)\n    if mask:\n        mask = (xgrid > -1) & (ygrid > -1) & (xgrid < 1) & (ygrid < 1)",
        "detail": "stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "pool2x",
        "kind": 2,
        "importPath": "stereo.modeling.disp_refinement.gru_blocks",
        "description": "stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "def pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []",
        "detail": "stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "interp",
        "kind": 2,
        "importPath": "stereo.modeling.disp_refinement.gru_blocks",
        "description": "stereo.modeling.disp_refinement.gru_blocks",
        "peekOfCode": "def interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []\n        self.n_gru_layers = n_gru_layers\n        self.encoder = BasicMotionEncoder(corr_levels, corr_radius, volume_channel)",
        "detail": "stereo.modeling.disp_refinement.gru_blocks",
        "documentation": {}
    },
    {
        "label": "aanet",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.aanet",
        "description": "stereo.modeling.models.aanet.aanet",
        "peekOfCode": "class aanet(nn.Module):\n    def __init__(self, model_cfg):\n        super(aanet, self).__init__()\n        self.model_cfg = model_cfg\n        self.max_disp = model_cfg.MAX_DISP\n        self.refinement_type = model_cfg.REFINEMENT_TYPE\n        self.num_downsample = model_cfg.NUM_DOWNSAMPLE\n        self.aggregation_type = model_cfg.AGGREGATION_TYPE\n        self.num_scales = model_cfg.NUM_SCALES\n        self.no_feature_mdconv = model_cfg.NO_FEATURE_MDCONV",
        "detail": "stereo.modeling.models.aanet.aanet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, leaky_relu=True):\n        \"\"\"StereoNet uses leaky relu (alpha = 0.2)\"\"\"\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride=stride, dilation=dilation)",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "Bottleneck",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class Bottleneck(nn.Module):\n    expansion = 4\n    __constants__ = ['downsample']\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "AANetFeature",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class AANetFeature(nn.Module):\n    def __init__(self, in_channels=32,\n                 zero_init_residual=True,\n                 groups=1,\n                 width_per_group=64,\n                 feature_mdconv=True,\n                 norm_layer=None):\n        super(AANetFeature, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "DeformConv2d",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class DeformConv2d(nn.Module):\n    \"\"\"A single (modulated) deformable conv layer\"\"\"\n    def __init__(self, in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 stride=1,\n                 dilation=2,\n                 groups=1,\n                 deformable_groups=2,\n                 modulation=True,",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "DeformBottleneck",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class DeformBottleneck(nn.Module):\n    expansion = 4\n    __constants__ = ['downsample']\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(DeformBottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "SimpleBottleneck",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class SimpleBottleneck(nn.Module):\n    \"\"\"Simple bottleneck block without channel expansion\"\"\"\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(SimpleBottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "DeformSimpleBottleneck",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class DeformSimpleBottleneck(nn.Module):\n    \"\"\"Used for cost aggregation\"\"\"\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, norm_layer=None,\n                 mdconv_dilation=2,\n                 deformable_groups=2,\n                 modulation=True,\n                 double_mask=True,\n                 ):\n        super(DeformSimpleBottleneck, self).__init__()",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "FeaturePyramidNetwork",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class FeaturePyramidNetwork(nn.Module):\n    def __init__(self, in_channels, out_channels=128,\n                 num_levels=3):\n        # FPN paper uses 256 out channels by default\n        super(FeaturePyramidNetwork, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n        for i in range(num_levels):",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "CostVolume",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class CostVolume(nn.Module):\n    def __init__(self, max_disp, feature_similarity='correlation'):\n        \"\"\"Construct cost volume based on different\n        similarity measures\n        Args:\n            max_disp: max disparity candidate\n            feature_similarity: type of similarity measure\n        \"\"\"\n        super(CostVolume, self).__init__()\n        self.max_disp = max_disp",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "CostVolumePyramid",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class CostVolumePyramid(nn.Module):\n    def __init__(self, max_disp, feature_similarity='correlation'):\n        super(CostVolumePyramid, self).__init__()\n        self.max_disp = max_disp\n        self.feature_similarity = feature_similarity\n    def forward(self, left_feature_pyramid, right_feature_pyramid):\n        num_scales = len(left_feature_pyramid)\n        cost_volume_pyramid = []\n        for s in range(num_scales):\n            max_disp = self.max_disp // (2 ** s)",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "AdaptiveAggregationModule",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class AdaptiveAggregationModule(nn.Module):\n    def __init__(self, num_scales, num_output_branches, max_disp,\n                 num_blocks=1,\n                 simple_bottleneck=False,\n                 deformable_groups=2,\n                 mdconv_dilation=2):\n        super(AdaptiveAggregationModule, self).__init__()\n        self.num_scales = num_scales\n        self.num_output_branches = num_output_branches\n        self.max_disp = max_disp",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "AdaptiveAggregation",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class AdaptiveAggregation(nn.Module):\n    def __init__(self, max_disp, num_scales=3, num_fusions=6,\n                 num_stage_blocks=1,\n                 num_deform_blocks=2,\n                 intermediate_supervision=True,\n                 deformable_groups=2,\n                 mdconv_dilation=2):\n        super(AdaptiveAggregation, self).__init__()\n        self.max_disp = max_disp\n        self.num_scales = num_scales",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "DisparityEstimation",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class DisparityEstimation(nn.Module):\n    def __init__(self, max_disp, match_similarity=True):\n        super(DisparityEstimation, self).__init__()\n        self.max_disp = max_disp\n        self.match_similarity = match_similarity\n    def forward(self, cost_volume):\n        assert cost_volume.dim() == 4\n        # Matching similarity or matching cost\n        cost_volume = cost_volume if self.match_similarity else -cost_volume\n        prob_volume = F.softmax(cost_volume, dim=1)  # [B, D, H, W]",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "StereoDRNetRefinement",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "class StereoDRNetRefinement(nn.Module):\n    def __init__(self):\n        super(StereoDRNetRefinement, self).__init__()\n        # Left and warped error\n        in_channels = 6\n        self.conv1 = conv2d(in_channels, 16)\n        self.conv2 = conv2d(1, 16)  # on low disparity\n        self.dilation_list = [1, 2, 4, 8, 1, 1]\n        self.dilated_blocks = nn.ModuleList()\n        for dilation in self.dilation_list:",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1, with_bn_relu=False, leaky_relu=False):\n    \"\"\"3x3 convolution with padding\"\"\"\n    conv = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n    if with_bn_relu:\n        relu = nn.LeakyReLU(0.2, inplace=True) if leaky_relu else nn.ReLU(inplace=True)\n        conv = nn.Sequential(conv,\n                             nn.BatchNorm2d(out_planes),\n                             relu)\n    return conv",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def conv1x1(in_planes, out_planes, stride=1):\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\ndef conv2d(in_channels, out_channels, kernel_size=3, stride=1, dilation=1, groups=1):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                                   stride=stride, padding=dilation, dilation=dilation,\n                                   bias=False, groups=groups),\n                         nn.BatchNorm2d(out_channels),\n                         nn.LeakyReLU(0.2, inplace=True))\nclass BasicBlock(nn.Module):",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "conv2d",
        "kind": 2,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def conv2d(in_channels, out_channels, kernel_size=3, stride=1, dilation=1, groups=1):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n                                   stride=stride, padding=dilation, dilation=dilation,\n                                   bias=False, groups=groups),\n                         nn.BatchNorm2d(out_channels),\n                         nn.LeakyReLU(0.2, inplace=True))\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None, leaky_relu=True):",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "normalize_coords",
        "kind": 2,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def normalize_coords(grid):\n    \"\"\"Normalize coordinates of image scale to [-1, 1]\n    Args:\n        grid: [B, 2, H, W]\n    \"\"\"\n    assert grid.size(1) == 2\n    h, w = grid.size()[2:]\n    grid[:, 0, :, :] = 2 * (grid[:, 0, :, :].clone() / (w - 1)) - 1  # x: [-1, 1]\n    grid[:, 1, :, :] = 2 * (grid[:, 1, :, :].clone() / (h - 1)) - 1  # y: [-1, 1]\n    grid = grid.permute((0, 2, 3, 1))  # [B, H, W, 2]",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "meshgrid",
        "kind": 2,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def meshgrid(img, homogeneous=False):\n    \"\"\"Generate meshgrid in image scale\n    Args:\n        img: [B, _, H, W]\n        homogeneous: whether to return homogeneous coordinates\n    Return:\n        grid: [B, 2, H, W]\n    \"\"\"\n    b, _, h, w = img.size()\n    x_range = torch.arange(0, w).view(1, 1, w).expand(1, h, w).type_as(img)  # [1, H, W]",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "disp_warp",
        "kind": 2,
        "importPath": "stereo.modeling.models.aanet.submodule",
        "description": "stereo.modeling.models.aanet.submodule",
        "peekOfCode": "def disp_warp(img, disp, padding_mode='border'):\n    \"\"\"Warping by disparity\n    Args:\n        img: [B, 3, H, W]\n        disp: [B, 1, H, W], positive\n        padding_mode: 'zeros' or 'border'\n    Returns:\n        warped_img: [B, 3, H, W]\n        valid_mask: [B, 3, H, W]\n    \"\"\"",
        "detail": "stereo.modeling.models.aanet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.aanet.trainer",
        "description": "stereo.modeling.models.aanet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.aanet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.aanet.trainer",
        "description": "stereo.modeling.models.aanet.trainer",
        "peekOfCode": "__all__ = {\n    'AANet': aanet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.aanet.trainer",
        "documentation": {}
    },
    {
        "label": "hourglass",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.cas_gwc",
        "description": "stereo.modeling.models.casnet.cas_gwc",
        "peekOfCode": "class hourglass(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass, self).__init__()\n        self.conv1 = nn.Sequential(convbn_3d(in_channels, in_channels * 2, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 2, 3, 1, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv3 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 4, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv4 = nn.Sequential(convbn_3d(in_channels * 4, in_channels * 4, 3, 1, 1),",
        "detail": "stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.cas_gwc",
        "description": "stereo.modeling.models.casnet.cas_gwc",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self, arch_mode=\"nospp\", num_stage=None, concat_feature_channel=12):\n        super(feature_extraction, self).__init__()\n        assert arch_mode in [\"spp\", \"nospp\"]\n        self.arch_mode = arch_mode\n        self.num_stage = num_stage\n        self.inplanes = 32\n        self.concat_feature_channel = concat_feature_channel\n        # strid 1\n        self.firstconv_a = nn.Sequential(convbn(3, 32, 3, 1, 1, 1),",
        "detail": "stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "CostAggregation",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.cas_gwc",
        "description": "stereo.modeling.models.casnet.cas_gwc",
        "peekOfCode": "class CostAggregation(nn.Module):\n    def __init__(self, in_channels, base_channels=32):\n        super(CostAggregation, self).__init__()\n        self.dres0 = nn.Sequential(convbn_3d(in_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(base_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True))\n        self.dres1 = nn.Sequential(convbn_3d(base_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(base_channels, base_channels, 3, 1, 1))",
        "detail": "stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "GetCostVolume",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.cas_gwc",
        "description": "stereo.modeling.models.casnet.cas_gwc",
        "peekOfCode": "class GetCostVolume(nn.Module):\n    def __init__(self):\n        super(GetCostVolume, self).__init__()\n    def get_warped_feats(self, x, y, disp_range_samples, ndisp):\n        bs, channels, height, width = y.size()\n        mh, mw = torch.meshgrid([torch.arange(0, height, dtype=x.dtype, device=x.device),\n                                 torch.arange(0, width, dtype=x.dtype, device=x.device)])  # (H *W)\n        mh = mh.reshape(1, 1, height, width).repeat(bs, ndisp, 1, 1)\n        mw = mw.reshape(1, 1, height, width).repeat(bs, ndisp, 1, 1)  # (B, D, H, W)\n        cur_disp_coords_y = mh",
        "detail": "stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "GwcNet",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.cas_gwc",
        "description": "stereo.modeling.models.casnet.cas_gwc",
        "peekOfCode": "class GwcNet(nn.Module):\n    def __init__(self, cfgs):\n        super(GwcNet, self).__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.ndisps = cfgs.NDISPS\n        self.disp_interval_pixel = cfgs.DISP_INTERVAL_PIXEL\n        self.num_stage = len(cfgs.NDISPS)\n        self.cr_base_chs = cfgs.CR_BASE_CHS\n        self.grad_method = cfgs.GRAD_METHOD\n        self.ns_size = cfgs.NS_SIZE",
        "detail": "stereo.modeling.models.casnet.cas_gwc",
        "documentation": {}
    },
    {
        "label": "hourglass",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.cas_psm",
        "description": "stereo.modeling.models.casnet.cas_psm",
        "peekOfCode": "class hourglass(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass, self).__init__()\n        self.conv1 = nn.Sequential(convbn_3d(in_channels, in_channels * 2, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 2, 3, 1, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv3 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 4, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv4 = nn.Sequential(convbn_3d(in_channels * 4, in_channels * 4, 3, 1, 1),",
        "detail": "stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.cas_psm",
        "description": "stereo.modeling.models.casnet.cas_psm",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self, arch_mode=\"fpn\", num_stage=None):\n        super(feature_extraction, self).__init__()\n        assert arch_mode in [\"fpn\"]\n        self.inplanes = 32\n        self.arch_mode = arch_mode\n        self.num_stage = num_stage\n        # TODO: split modifiy\n        self.firstconv_a = nn.Sequential(convbn(3, 32, 3, 1, 1, 1),\n                                         nn.ReLU(inplace=True),",
        "detail": "stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "CostAggregation",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.cas_psm",
        "description": "stereo.modeling.models.casnet.cas_psm",
        "peekOfCode": "class CostAggregation(nn.Module):\n    def __init__(self, in_channels, base_channels=32):\n        super(CostAggregation, self).__init__()\n        self.dres0 = nn.Sequential(convbn_3d(in_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(base_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True))\n        self.dres1 = nn.Sequential(convbn_3d(base_channels, base_channels, 3, 1, 1),\n                                   nn.ReLU(inplace=True),\n                                   convbn_3d(base_channels, base_channels, 3, 1, 1))",
        "detail": "stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "GetCostVolume",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.cas_psm",
        "description": "stereo.modeling.models.casnet.cas_psm",
        "peekOfCode": "class GetCostVolume(nn.Module):\n    def __init__(self):\n        super(GetCostVolume, self).__init__()\n    def forward(self, x, y, disp_range_samples, ndisp):\n        assert (x.is_contiguous() == True)\n        bs, channels, height, width = x.size()\n        cost = x.new().resize_(bs, channels * 2, ndisp, height, width).zero_()\n        # cost = y.unsqueeze(2).repeat(1, 2, ndisp, 1, 1) #(B, 2C, D, H, W)\n        mh, mw = torch.meshgrid([torch.arange(0, height, dtype=x.dtype, device=x.device),\n                                 torch.arange(0, width, dtype=x.dtype, device=x.device)])  # (H *W)",
        "detail": "stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "PSMNet",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.cas_psm",
        "description": "stereo.modeling.models.casnet.cas_psm",
        "peekOfCode": "class PSMNet(nn.Module):\n    def __init__(self, cfgs):\n        super(PSMNet, self).__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.ndisps = cfgs.NDISPS\n        self.disp_interval_pixel = cfgs.DISP_INTERVAL_PIXEL\n        self.num_stage = len(cfgs.NDISPS)\n        self.cr_base_chs = cfgs.CR_BASE_CHS\n        self.grad_method = cfgs.GRAD_METHOD\n        self.ns_size = cfgs.NS_SIZE",
        "detail": "stereo.modeling.models.casnet.cas_psm",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_channels))\ndef convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))\ndef disparity_regression(x, disp_values):\n    assert len(x.shape) == 4",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))\ndef disparity_regression(x, disp_values):\n    assert len(x.shape) == 4\n    return torch.sum(x * disp_values, 1, keepdim=False)\ndef build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def disparity_regression(x, disp_values):\n    assert len(x.shape) == 4\n    return torch.sum(x * disp_values, 1, keepdim=False)\ndef build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "get_cur_disp_range_samples",
        "kind": 2,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def get_cur_disp_range_samples(cur_disp, ndisp, disp_inteval_pixel, shape, ns_size, using_ns=False, max_disp=192.0):\n    # shape, (B, H, W)\n    # cur_disp: (B, H, W)\n    # return disp_range_samples: (B, D, H, W)\n    if not using_ns:\n        cur_disp_min = (cur_disp - ndisp / 2 * disp_inteval_pixel)  # (B, H, W)\n        cur_disp_max = (cur_disp + ndisp / 2 * disp_inteval_pixel)\n        # cur_disp_min = (cur_disp - ndisp / 2 * disp_inteval_pixel).clamp(min=0.0)   #(B, H, W)\n        # cur_disp_max = (cur_disp_min + (ndisp - 1) * disp_inteval_pixel).clamp(max=max_disp)\n        assert cur_disp.shape == torch.Size(shape), \"cur_disp:{}, input shape:{}\".format(cur_disp.shape, shape)",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "get_disp_range_samples",
        "kind": 2,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "def get_disp_range_samples(cur_disp, ndisp, disp_inteval_pixel, device, dtype, shape, using_ns, ns_size,\n                           max_disp=192.0):\n    # shape, (B, H, W)\n    # cur_disp: (B, H, W) or float\n    # return disp_range_values: (B, D, H, W)\n    # with torch.no_grad():\n    if cur_disp is None:\n        cur_disp = torch.tensor(0, device=device, dtype=dtype, requires_grad=False).reshape(1, 1, 1).repeat(*shape)\n        cur_disp_min = (cur_disp - ndisp / 2 * disp_inteval_pixel).clamp(min=0.0)  # (B, H, W)\n        cur_disp_max = (cur_disp_min + (ndisp - 1) * disp_inteval_pixel).clamp(max=max_disp)",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "Align_Corners",
        "kind": 5,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "Align_Corners = False\nAlign_Corners_Range = False\ndef convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_channels))\ndef convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "Align_Corners_Range",
        "kind": 5,
        "importPath": "stereo.modeling.models.casnet.submodule",
        "description": "stereo.modeling.models.casnet.submodule",
        "peekOfCode": "Align_Corners_Range = False\ndef convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_channels))\ndef convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))\ndef disparity_regression(x, disp_values):",
        "detail": "stereo.modeling.models.casnet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.casnet.trainer",
        "description": "stereo.modeling.models.casnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.casnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.casnet.trainer",
        "description": "stereo.modeling.models.casnet.trainer",
        "peekOfCode": "__all__ = {\n    'CasGwcNet': CasGwcNet,\n    'CasPSMNet': CasPSMNet\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.casnet.trainer",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.cfnet",
        "description": "stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self, concat_feature=False, concat_feature_channel=12):\n        super(feature_extraction, self).__init__()\n        self.concat_feature = concat_feature\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn(3, 32, 3, 2, 1, 1),\n                                       Mish(),\n                                       convbn(32, 32, 3, 1, 1, 1),\n                                       Mish(),\n                                       convbn(32, 32, 3, 1, 1, 1),",
        "detail": "stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "hourglassup",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.cfnet",
        "description": "stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "class hourglassup(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglassup, self).__init__()\n        self.conv1 = nn.Conv3d(in_channels, in_channels * 2, kernel_size=3, stride=2,\n                               padding=1, bias=False)\n        self.conv2 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 2, 3, 1, 1),\n                                   Mish())\n        self.conv3 = nn.Conv3d(in_channels * 2, in_channels * 4, kernel_size=3, stride=2,\n                               padding=1, bias=False)\n        self.conv4 = nn.Sequential(convbn_3d(in_channels * 4, in_channels * 4, 3, 1, 1),",
        "detail": "stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "hourglass",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.cfnet",
        "description": "stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "class hourglass(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass, self).__init__()\n        self.conv1 = nn.Sequential(convbn_3d(in_channels, in_channels * 2, 3, 2, 1),\n                                   Mish())\n        self.conv2 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 2, 3, 1, 1),\n                                   Mish())\n        self.conv3 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 4, 3, 2, 1),\n                                   Mish())\n        self.conv4 = nn.Sequential(convbn_3d(in_channels * 4, in_channels * 4, 3, 1, 1),",
        "detail": "stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "cfnet",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.cfnet",
        "description": "stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "class cfnet(nn.Module):\n    def __init__(self, cfgs, use_concat_volume=False):\n        super(cfnet, self).__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.use_concat_volume = True\n        self.v_scale_s1 = 1\n        self.v_scale_s2 = 2\n        self.v_scale_s3 = 3\n        self.sample_count_s1 = 6\n        self.sample_count_s2 = 10",
        "detail": "stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "CFNet",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.cfnet",
        "description": "stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "def CFNet(d, replace_mish=False):\n    net = cfnet(d, use_concat_volume=True)\n    if replace_mish:\n        replace_layers(net, Mish, nn.ReLU(inplace=True))\n        print('replacing', Mish(), '->', nn.ReLU())\n    return net\ndef replace_layers(model, old, new):\n    for n, module in model.named_children():\n        if len(list(module.children())) > 0:\n            ## compound module, go inside it",
        "detail": "stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "replace_layers",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.cfnet",
        "description": "stereo.modeling.models.cfnet.cfnet",
        "peekOfCode": "def replace_layers(model, old, new):\n    for n, module in model.named_children():\n        if len(list(module.children())) > 0:\n            ## compound module, go inside it\n            replace_layers(module, old, new)\n        if isinstance(module, old):\n            ## simple module\n            setattr(model, n, new)",
        "detail": "stereo.modeling.models.cfnet.cfnet",
        "documentation": {}
    },
    {
        "label": "pyramidPooling",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class pyramidPooling(nn.Module):\n    def __init__(self, in_channels, pool_sizes, model_name='pspnet', fusion_mode='cat', with_bn=True):\n        super(pyramidPooling, self).__init__()\n        bias = not with_bn\n        self.paths = []\n        if pool_sizes is None:\n            for i in range(4):\n                self.paths.append(conv2DBatchNormRelu(in_channels, in_channels, 1, 1, 0, bias=bias, with_bn=with_bn))\n        else:\n            for i in range(len(pool_sizes)):",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv2DBatchNormRelu",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class conv2DBatchNormRelu(nn.Module):\n    def __init__(self, in_channels, n_filters, k_size, stride, padding, bias=True, dilation=1, with_bn=True):\n        super(conv2DBatchNormRelu, self).__init__()\n        if dilation > 1:\n            conv_mod = nn.Conv2d(int(in_channels), int(n_filters), kernel_size=k_size,\n                                 padding=padding, stride=stride, bias=bias, dilation=dilation)\n        else:\n            conv_mod = nn.Conv2d(int(in_channels), int(n_filters), kernel_size=k_size,\n                                 padding=padding, stride=stride, bias=bias, dilation=1)\n        if with_bn:",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "Mish",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # print(\"Mish activation loaded...\")\n    def forward(self, x):\n        # save 1 second per epoch with no x= x*() and then return x...just inline it.\n        return x * (torch.tanh(F.softplus(x)))\ndef convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   Mish())\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n        self.downsample = downsample\n        self.stride = stride\n        # self.gc = ContextBlock2d(planes, planes // 8, 'att', ['channel_add'])",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "UniformSampler",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class UniformSampler(nn.Module):\n    def __init__(self):\n        super(UniformSampler, self).__init__()\n    def forward(self, min_disparity, max_disparity, number_of_samples=10):\n        \"\"\"\n        Args:\n            :min_disparity: lower bound of disparity search range\n            :max_disparity: upper bound of disparity range predictor\n            :number_of_samples (default:10): number of samples to be genearted.\n        Returns:",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "SpatialTransformer",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "class SpatialTransformer(nn.Module):\n    def __init__(self):\n        super(SpatialTransformer, self).__init__()\n    def forward(self, left_input, right_input, disparity_samples):\n        \"\"\"\n        Disparity Sample Cost Evaluator\n        Description:\n                Given the left image features, right iamge features and the disparity samples, generates:\n                    - Warped right image features\n        Args:",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_channels))\ndef convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))\ndef disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=pad, bias=False),\n                         nn.BatchNorm3d(out_channels))\ndef disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=False)\ndef disparity_variance(x, maxdisp, disparity):",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=False)\ndef disparity_variance(x, maxdisp, disparity):\n    # the shape of disparity should be B,1,H,W, return is the variance of the cost volume [B,1,H,W]\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_variance",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def disparity_variance(x, maxdisp, disparity):\n    # the shape of disparity should be B,1,H,W, return is the variance of the cost volume [B,1,H,W]\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    disp_values = (disp_values - disparity) ** 2\n    return torch.sum(x * disp_values, 1, keepdim=True)\ndef disparity_variance_confidence(x, disparity_samples, disparity):\n    # the shape of disparity should be B,1,H,W, return is the uncertainty estimation\n    assert len(x.shape) == 4",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_variance_confidence",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def disparity_variance_confidence(x, disparity_samples, disparity):\n    # the shape of disparity should be B,1,H,W, return is the uncertainty estimation\n    assert len(x.shape) == 4\n    disp_values = (disparity - disparity_samples) ** 2\n    return torch.sum(x * disp_values, 1, keepdim=True)\ndef build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef groupwise_correlation_4D(fea1, fea2, num_groups):\n    B, C, D, H, W = fea1.shape\n    assert C % num_groups == 0",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation_4D",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def groupwise_correlation_4D(fea1, fea2, num_groups):\n    B, C, D, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, D, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, D, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_corrleation_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def build_corrleation_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, 2 * maxdisp + 1, H, W])\n    for i in range(-maxdisp, maxdisp + 1):\n        if i > 0:\n            volume[:, :, i + maxdisp, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:],\n                                                                     targetimg_fea[:, :, :, :-i],\n                                                                     num_groups)\n        elif i < 0:\n            volume[:, :, i + maxdisp, :, :-i] = groupwise_correlation(refimg_fea[:, :, :, :-i],",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "warp",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def warp(x, disp):\n    \"\"\"\n    warp an image/tensor (imright) back to imleft, according to the disp\n    x: [B, C, H, W] (imright)\n    disp: [B, 1, H, W] disp\n    \"\"\"\n    B, C, H, W = x.size()\n    # mesh grid\n    xx = torch.arange(0, W).view(1, -1).repeat(H, 1)\n    yy = torch.arange(0, H).view(-1, 1).repeat(1, W)",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "FMish",
        "kind": 2,
        "importPath": "stereo.modeling.models.cfnet.submodule",
        "description": "stereo.modeling.models.cfnet.submodule",
        "peekOfCode": "def FMish(x):\n    '''\n    Applies the mish function element-wise:\n    mish(x) = x * tanh(softplus(x)) = x * tanh(ln(1 + exp(x)))\n    See additional documentation for mish class.\n    '''\n    return x * torch.tanh(F.softplus(x))\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):",
        "detail": "stereo.modeling.models.cfnet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.cfnet.trainer",
        "description": "stereo.modeling.models.cfnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.cfnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.cfnet.trainer",
        "description": "stereo.modeling.models.cfnet.trainer",
        "peekOfCode": "__all__ = {\n    'CFNet': cfnet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.cfnet.trainer",
        "documentation": {}
    },
    {
        "label": "CoEx",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex",
        "description": "stereo.modeling.models.coex.coex",
        "peekOfCode": "class CoEx(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.max_disp = cfgs.MAX_DISP\n        spixel_branch_channels = cfgs.SPIXEL_BRANCH_CHANNELS\n        chans = cfgs.CHANS\n        matching_weighted = cfgs.MATCHING_WEIGHTED\n        matching_head = cfgs.MATCHING_HEAD\n        gce = cfgs.GCE\n        aggregation_disp_strides = cfgs.AGGREGATION_DISP_STRIDES",
        "detail": "stereo.modeling.models.coex.coex",
        "documentation": {}
    },
    {
        "label": "FeatUp",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex_backbone",
        "description": "stereo.modeling.models.coex.coex_backbone",
        "peekOfCode": "class FeatUp(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # self.cfg = cfg['backbone']\n        chans = [16, 24, 32, 96, 160]\n        self.deconv32_16 = Conv2x(chans[4], chans[3], deconv=True, concat=True)\n        self.deconv16_8 = Conv2x(chans[3] * 2, chans[2], deconv=True, concat=True)\n        self.deconv8_4 = Conv2x(chans[2] * 2, chans[1], deconv=True, concat=True)\n        self.conv4 = BasicConv(chans[1] * 2, chans[1] * 2, kernel_size=3, stride=1, padding=1)\n        self.weight_init()",
        "detail": "stereo.modeling.models.coex.coex_backbone",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex_backbone",
        "description": "stereo.modeling.models.coex.coex_backbone",
        "peekOfCode": "class Feature(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.type = 'mobilenetv2_100'\n        layers = [1, 2, 3, 5, 6]\n        self.pre_trained = True\n        model = timm.create_model(self.type, pretrained=self.pre_trained, features_only=True)\n        self.conv_stem = model.conv_stem\n        self.bn1 = model.bn1\n        self.block0 = torch.nn.Sequential(*model.blocks[0:layers[0]])",
        "detail": "stereo.modeling.models.coex.coex_backbone",
        "documentation": {}
    },
    {
        "label": "CoExBackbone",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex_backbone",
        "description": "stereo.modeling.models.coex.coex_backbone",
        "peekOfCode": "class CoExBackbone(nn.Module):\n    def __init__(self, spixel_branch_channels=[32, 48]):\n        super().__init__()\n        self.feat = Feature()\n        self.up = FeatUp()\n        self.spixel_branch_channels = spixel_branch_channels\n        self.stem_2 = nn.Sequential(\n            BasicConv(3, self.spixel_branch_channels[0], kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(self.spixel_branch_channels[0], self.spixel_branch_channels[0], 3, 1, 1, bias=False),\n            nn.BatchNorm2d(self.spixel_branch_channels[0]),",
        "detail": "stereo.modeling.models.coex.coex_backbone",
        "documentation": {}
    },
    {
        "label": "CostVolume",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex_cost_processor",
        "description": "stereo.modeling.models.coex.coex_cost_processor",
        "peekOfCode": "class CostVolume(nn.Module):\n    def __init__(self, maxdisp, glue=False, group=1):\n        super(CostVolume, self).__init__()\n        self.maxdisp = maxdisp + 1\n        self.glue = glue\n        self.group = group\n        self.unfold = nn.Unfold((1, maxdisp + 1), 1, 0, 1)\n        self.left_pad = nn.ZeroPad2d((maxdisp, 0, 0, 0))\n    def forward(self, x, y, v=None):\n        b, c, h, w = x.shape",
        "detail": "stereo.modeling.models.coex.coex_cost_processor",
        "documentation": {}
    },
    {
        "label": "AttentionCostVolume",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex_cost_processor",
        "description": "stereo.modeling.models.coex.coex_cost_processor",
        "peekOfCode": "class AttentionCostVolume(nn.Module):\n    def __init__(self, max_disparity, in_chan, hidden_chan, head=1, weighted=False):\n        super(AttentionCostVolume, self).__init__()\n        self.costVolume = CostVolume(int(max_disparity // 4), False, head)\n        self.conv = BasicConv(in_chan, hidden_chan, kernel_size=3, padding=1, stride=1)\n        self.desc = nn.Conv2d(hidden_chan, hidden_chan, kernel_size=1, padding=0, stride=1)\n        self.head = head\n        self.weighted = weighted\n        if weighted:\n            self.weights = nn.Parameter(",
        "detail": "stereo.modeling.models.coex.coex_cost_processor",
        "documentation": {}
    },
    {
        "label": "channelAtt",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex_cost_processor",
        "description": "stereo.modeling.models.coex.coex_cost_processor",
        "peekOfCode": "class channelAtt(nn.Module):\n    def __init__(self, cv_chan, im_chan, D):\n        super(channelAtt, self).__init__()\n        self.im_att = nn.Sequential(\n            BasicConv(im_chan, im_chan // 2, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(im_chan // 2, cv_chan, 1))\n        self.weight_init()\n    def forward(self, cv, im):\n        channel_att = self.im_att(im).unsqueeze(2)\n        cv = torch.sigmoid(channel_att) * cv",
        "detail": "stereo.modeling.models.coex.coex_cost_processor",
        "documentation": {}
    },
    {
        "label": "Aggregation",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex_cost_processor",
        "description": "stereo.modeling.models.coex.coex_cost_processor",
        "peekOfCode": "class Aggregation(nn.Module):\n    def __init__(\n            self,\n            max_disparity=192,\n            matching_head=1,\n            gce=True,\n            disp_strides=2,\n            channels=[16, 32, 48],\n            blocks_num=[2, 2, 2],\n            spixel_branch_channels=[32, 48]",
        "detail": "stereo.modeling.models.coex.coex_cost_processor",
        "documentation": {}
    },
    {
        "label": "CoExCostProcessor",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex_cost_processor",
        "description": "stereo.modeling.models.coex.coex_cost_processor",
        "peekOfCode": "class CoExCostProcessor(nn.Module):\n    def __init__(\n            self,\n            max_disp=192,\n            gce=True,\n            matching_weighted=False,\n            spixel_branch_channels=[32, 48],\n            matching_head=1,\n            aggregation_disp_strides=2,\n            aggregation_channels=[16, 32, 48],",
        "detail": "stereo.modeling.models.coex.coex_cost_processor",
        "documentation": {}
    },
    {
        "label": "Regression",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex_disp_processor",
        "description": "stereo.modeling.models.coex.coex_disp_processor",
        "peekOfCode": "class Regression(nn.Module):\n    def __init__(self, max_disparity=192, top_k=2):\n        super(Regression, self).__init__()\n        self.D = int(max_disparity // 4)\n        self.top_k = top_k\n        self.ind_init = False\n    def forward(self, cost, spg):\n        b, _, h, w = spg.shape\n        corr, disp = self.topkpool(cost, self.top_k)\n        corr = F.softmax(corr, 2)",
        "detail": "stereo.modeling.models.coex.coex_disp_processor",
        "documentation": {}
    },
    {
        "label": "CoExDispProcessor",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.coex_disp_processor",
        "description": "stereo.modeling.models.coex.coex_disp_processor",
        "peekOfCode": "class CoExDispProcessor(nn.Module):\n    def __init__(self, max_disp=192, regression_topk=2, chans =[16, 24, 32, 96, 160]):\n        super().__init__()\n        self.max_disp = max_disp\n        self.regression_topk = regression_topk\n        self.chans = chans\n        self.spixel_branch_channels = [32, 48]\n        self.spx = nn.Sequential(nn.ConvTranspose2d(2 * 32, 9, kernel_size=4, stride=2, padding=1), )\n        self.spx_2 = Conv2x(self.chans[1], 32, True)\n        self.spx_4 = nn.Sequential(",
        "detail": "stereo.modeling.models.coex.coex_disp_processor",
        "documentation": {}
    },
    {
        "label": "upfeat",
        "kind": 2,
        "importPath": "stereo.modeling.models.coex.coex_disp_processor",
        "description": "stereo.modeling.models.coex.coex_disp_processor",
        "peekOfCode": "def upfeat(input, prob, up_h=2, up_w=2):\n    b, c, h, w = input.shape\n    feat = F.unfold(input, 3, 1, 1).reshape(b, -1, h, w)\n    feat = F.interpolate(\n        feat, (h * up_h, w * up_w), mode='nearest').reshape(\n        b, -1, 9, h * up_h, w * up_w)\n    feat_sum = (feat * prob.unsqueeze(1)).sum(2)\n    return feat_sum\nclass Regression(nn.Module):\n    def __init__(self, max_disparity=192, top_k=2):",
        "detail": "stereo.modeling.models.coex.coex_disp_processor",
        "documentation": {}
    },
    {
        "label": "PSMBasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "class PSMBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(PSMBasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "class BasicConv(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, bn=True, relu=True, **kwargs):\n        super(BasicConv, self).__init__()\n        self.relu = relu\n        self.use_bn = bn\n        if is_3d:\n            if deconv:\n                self.conv = nn.ConvTranspose3d(in_channels, out_channels, bias=False, **kwargs)\n            else:\n                self.conv = nn.Conv3d(in_channels, out_channels, bias=False, **kwargs)",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "Conv2x",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "class Conv2x(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, concat=True, keep_concat=True, bn=True,\n                 relu=True, keep_dispc=False):\n        super(Conv2x, self).__init__()\n        self.concat = concat\n        self.is_3d = is_3d\n        if deconv and is_3d:\n            kernel = (4, 4, 4)\n        elif deconv:\n            kernel = 4",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(\n            self,\n            inplanes: int,\n            planes: int,\n            stride: int = 1,\n            downsample: Optional[nn.Module] = None,\n            groups: int = 1,\n            base_width: int = 64,",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "ConvBNReLU3d",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "class ConvBNReLU3d(nn.Sequential):\n    def __init__(\n            self,\n            in_planes: int,\n            out_planes: int,\n            kernel_size: int = 3,\n            stride: int = 1,\n            groups: int = 1,\n            norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "InvertedResidual3d",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "class InvertedResidual3d(nn.Module):\n    def __init__(\n            self,\n            inp: int,\n            oup: int,\n            stride: int,\n            expand_ratio: int,\n            norm_layer: Optional[Callable[..., nn.Module]] = None\n    ) -> None:\n        super(InvertedResidual3d, self).__init__()",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "AtrousBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "class AtrousBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, bn=True, relu=True):\n        super(AtrousBlock, self).__init__()\n        dilations = [2, 4, 6]\n        self.conv_1 = BasicConv(in_channels, out_channels // 4, is_3d=True, kernel_size=3, stride=stride, padding=1,\n                                dilation=1)\n        self.conv_2 = BasicConv(in_channels, out_channels // 4, is_3d=True, kernel_size=3, stride=stride,\n                                padding=(1, dilations[0], dilations[0]), dilation=(1, dilations[0], dilations[0]))\n        self.conv_3 = BasicConv(in_channels, out_channels // 4, is_3d=True, kernel_size=3, stride=stride,\n                                padding=(1, dilations[1], dilations[1]), dilation=(1, dilations[1], dilations[1]))",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "def convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n    return nn.Sequential(\n        nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                  padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n        nn.BatchNorm2d(out_planes)\n    )\nclass PSMBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(PSMBasicBlock, self).__init__()",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "kind": 2,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "def BasicConv2d(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                  padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(inplace=True, negative_slope=0.2),\n    )\ndef BasicTransposeConv2d(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    output_pad = stride + 2 * pad - kernel_size * dilation + dilation - 1\n    return nn.Sequential(",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicTransposeConv2d",
        "kind": 2,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "def BasicTransposeConv2d(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    output_pad = stride + 2 * pad - kernel_size * dilation + dilation - 1\n    return nn.Sequential(\n        nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, pad, output_pad, dilation, bias=False),\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(inplace=True, negative_slope=0.2),\n    )\ndef BasicConv3d(in_channels, out_channels, kernel_size, stride, pad, dilation=1):\n    return nn.Sequential(\n        nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "kind": 2,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "def BasicConv3d(in_channels, out_channels, kernel_size, stride, pad, dilation=1):\n    return nn.Sequential(\n        nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                  padding=pad, dilation=dilation, bias=False),\n        nn.BatchNorm3d(out_channels),\n        nn.LeakyReLU(inplace=True, negative_slope=0.2),\n    )\ndef BasicTransposeConv3d(in_channels, out_channels, kernel_size, stride, pad, output_pad=0, dilation=1):\n    # output_pad = stride + 2 * pad - kernel_size * dilation + dilation - 1\n    return nn.Sequential(",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "BasicTransposeConv3d",
        "kind": 2,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "def BasicTransposeConv3d(in_channels, out_channels, kernel_size, stride, pad, output_pad=0, dilation=1):\n    # output_pad = stride + 2 * pad - kernel_size * dilation + dilation - 1\n    return nn.Sequential(\n        nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride,\n                           pad, output_pad, dilation, bias=False),\n        nn.BatchNorm3d(out_channels),\n        nn.LeakyReLU(inplace=True, negative_slope=0.2),\n    )\ndef conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\nclass BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "stereo.modeling.models.coex.submodule",
        "description": "stereo.modeling.models.coex.submodule",
        "peekOfCode": "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\nclass BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(\n            self,\n            inplanes: int,\n            planes: int,\n            stride: int = 1,",
        "detail": "stereo.modeling.models.coex.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.coex.trainer",
        "description": "stereo.modeling.models.coex.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.coex.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.coex.trainer",
        "description": "stereo.modeling.models.coex.trainer",
        "peekOfCode": "__all__ = {\n    'CoExNet': CoExNet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.coex.trainer",
        "documentation": {}
    },
    {
        "label": "FADNet",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.fadnet",
        "description": "stereo.modeling.models.fadnet.fadnet",
        "peekOfCode": "class FADNet(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        resBlock = cfgs.RESBLOCK\n        input_channel = cfgs.INPUT_CHANNEL\n        encoder_ratio = cfgs.ENCODER_RATIO\n        decoder_ratio = cfgs.DECODER_RATIO\n        in_planes = cfgs.IN_PLANES\n        self.backbone = FadnetBackbone(resBlock=resBlock, maxdisp=self.maxdisp, input_channel=input_channel,",
        "detail": "stereo.modeling.models.fadnet.fadnet",
        "documentation": {}
    },
    {
        "label": "FadnetBackbone",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.fadnet_backbone",
        "description": "stereo.modeling.models.fadnet.fadnet_backbone",
        "peekOfCode": "class FadnetBackbone(nn.Module):\n    def __init__(self, resBlock=True, maxdisp=192, input_channel=3, encoder_ratio=16, decoder_ratio=16):\n        super(FadnetBackbone, self).__init__()\n        self.input_channel = input_channel\n        self.maxdisp = maxdisp\n        self.relu = nn.ReLU(inplace=False)\n        self.basicC = 2\n        self.eratio = encoder_ratio\n        self.dratio = decoder_ratio\n        self.basicE = self.basicC*self.eratio",
        "detail": "stereo.modeling.models.fadnet.fadnet_backbone",
        "documentation": {}
    },
    {
        "label": "FADAggregator",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.fadnet_cost_processor",
        "description": "stereo.modeling.models.fadnet.fadnet_cost_processor",
        "peekOfCode": "class FADAggregator(nn.Module):\n    def __init__(self, resBlock=True, maxdisp=192, input_channel=3, encoder_ratio=16, decoder_ratio=16):\n        super(FADAggregator, self).__init__()\n        self.input_channel = input_channel\n        self.maxdisp = maxdisp\n        self.relu = nn.ReLU(inplace=False)\n        self.basicC = 2\n        self.eratio = encoder_ratio\n        self.dratio = decoder_ratio\n        self.basicE = self.basicC*self.eratio",
        "detail": "stereo.modeling.models.fadnet.fadnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "DispNetRes",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.fadnet_disp_predictor",
        "description": "stereo.modeling.models.fadnet.fadnet_disp_predictor",
        "peekOfCode": "class DispNetRes(nn.Module):\n    def __init__(self, in_planes, resBlock=True, input_channel=3, encoder_ratio=16, decoder_ratio=16):\n        super(DispNetRes, self).__init__()\n        self.input_channel = input_channel\n        self.basicC = 2\n        self.eratio = encoder_ratio\n        self.dratio = decoder_ratio\n        self.basicE = self.basicC*self.eratio\n        self.basicD = self.basicC*self.dratio\n        self.resBlock = resBlock",
        "detail": "stereo.modeling.models.fadnet.fadnet_disp_predictor",
        "documentation": {}
    },
    {
        "label": "HorizontalPoolingPyramid",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class HorizontalPoolingPyramid():\n    \"\"\"\n        Horizontal Pyramid Matching for Person Re-identification\n        Arxiv: https://arxiv.org/abs/1804.05275\n        Github: https://github.com/SHI-Labs/Horizontal-Pyramid-Matching\n    \"\"\"\n    def __init__(self, bin_num=None):\n        if bin_num is None:\n            bin_num = [16, 8, 4, 2, 1]\n        self.bin_num = bin_num",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "SetBlockWrapper",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class SetBlockWrapper(nn.Module):\n    def __init__(self, forward_block):\n        super(SetBlockWrapper, self).__init__()\n        self.forward_block = forward_block\n    def forward(self, x, *args, **kwargs):\n        \"\"\"\n            In  x: [n, c_in, s, h_in, w_in]\n            Out x: [n, c_out, s, h_out, w_out]\n        \"\"\"\n        n, c, s, h, w = x.size()",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "PackSequenceWrapper",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class PackSequenceWrapper(nn.Module):\n    def __init__(self, pooling_func):\n        super(PackSequenceWrapper, self).__init__()\n        self.pooling_func = pooling_func\n    def forward(self, seqs, seqL, dim=2, options={}):\n        \"\"\"\n            In  seqs: [n, c, s, ...]\n            Out rets: [n, ...]\n        \"\"\"\n        if seqL is None:",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv2d",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class BasicConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n                              stride=stride, padding=padding, bias=False, **kwargs)\n    def forward(self, x):\n        x = self.conv(x)\n        return x\nclass SeparateFCs(nn.Module):\n    def __init__(self, parts_num, in_channels, out_channels, norm=False):",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "SeparateFCs",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class SeparateFCs(nn.Module):\n    def __init__(self, parts_num, in_channels, out_channels, norm=False):\n        super(SeparateFCs, self).__init__()\n        self.p = parts_num\n        self.fc_bin = nn.Parameter(\n            nn.init.xavier_uniform_(\n                torch.zeros(parts_num, in_channels, out_channels)))\n        self.norm = norm\n    def forward(self, x):\n        \"\"\"",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "SeparateBNNecks",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class SeparateBNNecks(nn.Module):\n    \"\"\"\n        GaitSet: Bag of Tricks and a Strong Baseline for Deep Person Re-Identification\n        CVPR Workshop:  https://openaccess.thecvf.com/content_CVPRW_2019/papers/TRMTMCT/Luo_Bag_of_Tricks_and_a_Strong_Baseline_for_Deep_Person_CVPRW_2019_paper.pdf\n        Github: https://github.com/michuanhaohao/reid-strong-baseline\n    \"\"\"\n    def __init__(self, parts_num, in_channels, class_num, norm=True, parallel_BN1d=True):\n        super(SeparateBNNecks, self).__init__()\n        self.p = parts_num\n        self.class_num = class_num",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "FocalConv2d",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class FocalConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, halving, **kwargs):\n        super(FocalConv2d, self).__init__()\n        self.halving = halving\n        self.conv = nn.Conv2d(in_channels, out_channels,\n                              kernel_size, bias=False, **kwargs)\n    def forward(self, x):\n        if self.halving == 0:\n            z = self.conv(x)\n        else:",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv3d",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class BasicConv3d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False, **kwargs):\n        super(BasicConv3d, self).__init__()\n        self.conv3d = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size,\n                                stride=stride, padding=padding, bias=bias, **kwargs)\n    def forward(self, ipts):\n        '''\n            ipts: [n, c, s, h, w]\n            outs: [n, c, s, h, w]\n        '''",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "GaitAlign",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class GaitAlign(nn.Module):\n    def __init__(self, H=64, W=44, eps=1, **kwargs):\n        super(GaitAlign, self).__init__()\n        self.H, self.W, self.eps = H, W, eps\n        self.Pad = nn.ZeroPad2d((int(self.W / 2), int(self.W / 2), 0, 0))\n        self.RoiPool = RoIAlign((self.H, self.W), 1, sampling_ratio=-1)\n    def forward(self, feature_map, binary_mask, w_h_ratio):\n        \"\"\"\n           In  sils:         [n, c, h, w]\n               w_h_ratio:    [n, 1]",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "DynamicConv2d",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class DynamicConv2d(nn.Module):\n    def __init__(self, max_in_channels, max_out_channels, kernel_size=1, stride=1, dilation=1):\n        super(DynamicConv2d, self).__init__()\n        self.max_in_channels = max_in_channels\n        self.max_out_channels = max_out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n        self.conv = nn.Conv2d(\n            self.max_in_channels, self.max_out_channels, self.kernel_size, stride=self.stride, bias=False,",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "DyRes",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class DyRes(nn.Module):\n    def __init__(self, max_in=98, max_out=128, stride = 1):\n        super(DyRes, self).__init__()\n        self.conv1 = DynamicConv2d(max_in, max_out, kernel_size = 3, stride = stride)\n        self.bn1 = nn.BatchNorm2d(max_out)\n        self.relu = nn.ReLU(inplace = True)\n        self.conv2 = nn.Conv2d(max_out, max_out, kernel_size = 3, padding = 1)\n        self.bn2 = nn.BatchNorm2d(max_out)\n        self.stride = stride\n        self.max_out = max_out",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class ResBlock(nn.Module):\n    def __init__(self, n_in, n_out, stride = 1):\n        super(ResBlock, self).__init__()\n        self.conv1 = nn.Conv2d(n_in, n_out, kernel_size = 3, stride = stride, padding = 1)\n        self.bn1 = nn.BatchNorm2d(n_out)\n        self.relu = nn.ReLU(inplace = True)\n        self.conv2 = nn.Conv2d(n_out, n_out, kernel_size = 3, padding = 1)\n        self.bn2 = nn.BatchNorm2d(n_out)\n        if stride != 1 or n_out != n_in:\n            self.shortcut = nn.Sequential(",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "matchshifted",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class matchshifted(nn.Module):\n    def __init__(self):\n        super(matchshifted, self).__init__()\n    def forward(self, left, right, shift):\n        batch, filters, height, width = left.size()\n        shifted_left  = F.pad(torch.index_select(left,  3, Variable(torch.LongTensor([i for i in range(shift,width)])).cuda()),(shift,0,0,0))\n        shifted_right = F.pad(torch.index_select(right, 3, Variable(torch.LongTensor([i for i in range(width-shift)])).cuda()),(shift,0,0,0))\n        out = torch.cat((shifted_left,shifted_right),1).view(batch,filters*2,1,height,width)\n        return out\nclass disparityregression(nn.Module):",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparityregression",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class disparityregression(nn.Module):\n    def __init__(self, maxdisp):\n        super(disparityregression, self).__init__()\n        self.disp = Variable(torch.Tensor(np.reshape(np.array(range(maxdisp)),[1,maxdisp,1,1])).cuda(), requires_grad=False)\n    def forward(self, x):\n        disp = self.disp.repeat(x.size()[0],1,x.size()[2],x.size()[3])\n        out = torch.sum(x*disp,1)\n        return out\ndef disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self):\n        super(feature_extraction, self).__init__()\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn(3, 32, 3, 2, 1, 1),\n                                       nn.ReLU(inplace=True),\n                                       convbn(32, 32, 3, 1, 1, 1),\n                                       nn.ReLU(inplace=True),\n                                       convbn(32, 32, 3, 1, 1, 1),\n                                       nn.ReLU(inplace=True))",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "tofp16",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class tofp16(nn.Module):\n    def __init__(self):\n        super(tofp16, self).__init__()\n    def forward(self, input):\n        return input.half()\nclass tofp32(nn.Module):\n    def __init__(self):\n        super(tofp32, self).__init__()\n    def forward(self, input):\n        return input.float()",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "tofp32",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "class tofp32(nn.Module):\n    def __init__(self):\n        super(tofp32, self).__init__()\n    def forward(self, input):\n        return input.float()\ndef init_deconv_bilinear(weight):\n    f_shape = weight.size()\n    heigh, width = f_shape[-2], f_shape[-1]\n    f = np.ceil(width/2.0)\n    c = (2 * f - 1 - f % 2) / (2.0 * f)",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "clones",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def clones(module, N):\n    \"Produce N identical layers.\"\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\ndef is_list_or_tuple(x):\n    return isinstance(x, (list, tuple))\nclass HorizontalPoolingPyramid():\n    \"\"\"\n        Horizontal Pyramid Matching for Person Re-identification\n        Arxiv: https://arxiv.org/abs/1804.05275\n        Github: https://github.com/SHI-Labs/Horizontal-Pyramid-Matching",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "is_list_or_tuple",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def is_list_or_tuple(x):\n    return isinstance(x, (list, tuple))\nclass HorizontalPoolingPyramid():\n    \"\"\"\n        Horizontal Pyramid Matching for Person Re-identification\n        Arxiv: https://arxiv.org/abs/1804.05275\n        Github: https://github.com/SHI-Labs/Horizontal-Pyramid-Matching\n    \"\"\"\n    def __init__(self, bin_num=None):\n        if bin_num is None:",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "RmBN2dAffine",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def RmBN2dAffine(model):\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.weight.requires_grad = False\n            m.bias.requires_grad = False\n# freda (todo) : \nclass DynamicConv2d(nn.Module):\n    def __init__(self, max_in_channels, max_out_channels, kernel_size=1, stride=1, dilation=1):\n        super(DynamicConv2d, self).__init__()\n        self.max_in_channels = max_in_channels",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def conv(in_planes, out_planes, kernel_size=3, stride=1, batchNorm=False):\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.LeakyReLU(0.1,inplace=True)\n        )\n    else:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "i_conv",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def i_conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, bias = True):\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=bias),\n            nn.BatchNorm2d(out_planes),\n        )\n    else:\n        return nn.Sequential(\n            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=bias),\n        )",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=dilation if dilation > 1 else pad, dilation = dilation, bias=False),\n                         nn.BatchNorm2d(out_planes))\ndef convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),\n                         nn.BatchNorm3d(out_planes))\ndef predict_flow(in_planes, out_planes = 1):\n    return nn.Conv2d(in_planes,out_planes,kernel_size=3,stride=1,padding=1,bias=False)\n#def predict_flow(in_planes):\n#    return nn.Conv2d(in_planes,1,kernel_size=1,stride=1,padding=0,bias=False)",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),\n                         nn.BatchNorm3d(out_planes))\ndef predict_flow(in_planes, out_planes = 1):\n    return nn.Conv2d(in_planes,out_planes,kernel_size=3,stride=1,padding=1,bias=False)\n#def predict_flow(in_planes):\n#    return nn.Conv2d(in_planes,1,kernel_size=1,stride=1,padding=0,bias=False)\n#def corr(in_planes, max_disp=40):\n#    return Correlation1d(pad_size=max_disp, kernel_size=1, max_displacement=max_disp, stride1=1, stride2=2, corr_multiply=1)\ndef build_corr(img_left, img_right, max_disp=40, zero_volume=None):",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "predict_flow",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def predict_flow(in_planes, out_planes = 1):\n    return nn.Conv2d(in_planes,out_planes,kernel_size=3,stride=1,padding=1,bias=False)\n#def predict_flow(in_planes):\n#    return nn.Conv2d(in_planes,1,kernel_size=1,stride=1,padding=0,bias=False)\n#def corr(in_planes, max_disp=40):\n#    return Correlation1d(pad_size=max_disp, kernel_size=1, max_displacement=max_disp, stride1=1, stride2=2, corr_multiply=1)\ndef build_corr(img_left, img_right, max_disp=40, zero_volume=None):\n    B, C, H, W = img_left.shape\n    if zero_volume is not None:\n        tmp_zero_volume = zero_volume #* 0.0",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_corr",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def build_corr(img_left, img_right, max_disp=40, zero_volume=None):\n    B, C, H, W = img_left.shape\n    if zero_volume is not None:\n        tmp_zero_volume = zero_volume #* 0.0\n        #print('tmp_zero_volume: ', mean)\n        volume = tmp_zero_volume\n    else:\n        volume = img_left.new_zeros([B, max_disp, H, W])\n    for i in range(max_disp):\n        if (i > 0) & (i < W):",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, i:] = refimg_fea[:, :, :, i:]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "deconv",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def deconv(in_planes, out_planes):\n    return nn.Sequential(\n        nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4, stride=2, padding=1, bias=False),\n        nn.LeakyReLU(0.1,inplace=True)\n    )\ndef convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=dilation if dilation > 1 else pad, dilation = dilation, bias=False),\n                         nn.BatchNorm2d(out_planes))\ndef convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def convbn(in_planes, out_planes, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=dilation if dilation > 1 else pad, dilation = dilation, bias=False),\n                         nn.BatchNorm2d(out_planes))\ndef convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),\n                         nn.BatchNorm3d(out_planes))\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def convbn_3d(in_planes, out_planes, kernel_size, stride, pad):\n    return nn.Sequential(nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, padding=pad, stride=stride,bias=False),\n                         nn.BatchNorm3d(out_planes))\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=False)\nclass feature_extraction(nn.Module):\n    def __init__(self):\n        super(feature_extraction, self).__init__()\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn(3, 32, 3, 2, 1, 1),",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "init_deconv_bilinear",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def init_deconv_bilinear(weight):\n    f_shape = weight.size()\n    heigh, width = f_shape[-2], f_shape[-1]\n    f = np.ceil(width/2.0)\n    c = (2 * f - 1 - f % 2) / (2.0 * f)\n    bilinear = np.zeros([heigh, width])\n    for x in range(width):\n        for y in range(heigh):\n            value = (1 - abs(x / f - c)) * (1 - abs(y / f - c))\n            bilinear[x, y] = value",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "save_grad",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def save_grad(grads, name):\n    def hook(grad):\n        grads[name] = grad\n    return hook\ndef disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=True)\ndef channel_normalize(x):",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=True)\ndef channel_normalize(x):\n    return x / (torch.norm(x, 2, dim=1, keepdim=True) + 1e-8)\ndef channel_length(x):\n    return torch.sqrt(torch.sum(torch.pow(x, 2), dim=1, keepdim=True) + 1e-8)\ndef warp_right_to_left(x, disp, warp_grid=None):",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "channel_normalize",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def channel_normalize(x):\n    return x / (torch.norm(x, 2, dim=1, keepdim=True) + 1e-8)\ndef channel_length(x):\n    return torch.sqrt(torch.sum(torch.pow(x, 2), dim=1, keepdim=True) + 1e-8)\ndef warp_right_to_left(x, disp, warp_grid=None):\n    #print('size: ', x.size())\n    B, C, H, W = x.size()\n    # mesh grid\n    if warp_grid is not None:\n        xx0, yy = warp_grid",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "channel_length",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def channel_length(x):\n    return torch.sqrt(torch.sum(torch.pow(x, 2), dim=1, keepdim=True) + 1e-8)\ndef warp_right_to_left(x, disp, warp_grid=None):\n    #print('size: ', x.size())\n    B, C, H, W = x.size()\n    # mesh grid\n    if warp_grid is not None:\n        xx0, yy = warp_grid\n        xx = xx0 + disp\n        xx = 2.0*xx / max(W-1,1)-1.0",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "warp_right_to_left",
        "kind": 2,
        "importPath": "stereo.modeling.models.fadnet.submodule",
        "description": "stereo.modeling.models.fadnet.submodule",
        "peekOfCode": "def warp_right_to_left(x, disp, warp_grid=None):\n    #print('size: ', x.size())\n    B, C, H, W = x.size()\n    # mesh grid\n    if warp_grid is not None:\n        xx0, yy = warp_grid\n        xx = xx0 + disp\n        xx = 2.0*xx / max(W-1,1)-1.0\n    else:\n        #xx = torch.arange(0, W, device=disp.device).float()",
        "detail": "stereo.modeling.models.fadnet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.fadnet.trainer",
        "description": "stereo.modeling.models.fadnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.fadnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.fadnet.trainer",
        "description": "stereo.modeling.models.fadnet.trainer",
        "peekOfCode": "__all__ = {\n    'FADNet': FADNet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.fadnet.trainer",
        "documentation": {}
    },
    {
        "label": "InputPadder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "peekOfCode": "class InputPadder:\n    \"\"\" Pads images such that dimensions are divisible by 8 \"\"\"\n    def __init__(self, dims, mode='sintel', divis_by=8, force_square=False):\n        self.ht, self.wd = dims[-2:]\n        if force_square:\n          max_side = max(self.ht, self.wd)\n          pad_ht = ((max_side // divis_by) + 1) * divis_by - self.ht\n          pad_wd = ((max_side // divis_by) + 1) * divis_by - self.wd\n        else:\n          pad_ht = (((self.ht // divis_by) + 1) * divis_by - self.ht) % divis_by",
        "detail": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "peekOfCode": "def bilinear_sampler(img, coords, mode='bilinear', mask=False, low_memory=False):\n    \"\"\" Wrapper for grid_sample, uses pixel coordinates \"\"\"\n    H, W = img.shape[-2:]\n    xgrid, ygrid = coords.split([1,1], dim=-1)\n    xgrid = 2*xgrid/(W-1) - 1   # Normalize to [-1,1]\n    assert torch.unique(ygrid).numel() == 1 and H == 1 # This is a stereo problem\n    grid = torch.cat([xgrid, ygrid], dim=-1).to(img.dtype)\n    img = F.grid_sample(img, grid, align_corners=True)\n    if mask:\n        mask = (xgrid > -1) & (ygrid > -1) & (xgrid < 1) & (ygrid < 1)",
        "detail": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "documentation": {}
    },
    {
        "label": "coords_grid",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "peekOfCode": "def coords_grid(batch, ht, wd):\n    coords = torch.meshgrid(torch.arange(ht), torch.arange(wd))\n    coords = torch.stack(coords[::-1], dim=0).float()\n    return coords[None].repeat(batch, 1, 1, 1)",
        "detail": "stereo.modeling.models.foundationstereo.core.utils.utils",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.extractor",
        "description": "stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)",
        "detail": "stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "MultiBasicEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.extractor",
        "description": "stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "class MultiBasicEncoder(nn.Module):\n    def __init__(self, output_dim=[128], norm_fn='batch', dropout=0.0, downsample=3):\n        super(MultiBasicEncoder, self).__init__()\n        self.norm_fn = norm_fn\n        self.downsample = downsample\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n        elif self.norm_fn == 'batch':\n            self.norm1 = nn.BatchNorm2d(64)\n        elif self.norm_fn == 'instance':",
        "detail": "stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "ContextNetDino",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.extractor",
        "description": "stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "class ContextNetDino(MultiBasicEncoder):\n    def __init__(self, args, output_dim=[128], norm_fn='batch', downsample=3):\n        nn.Module.__init__(self)\n        self.args = args\n        self.patch_size = 14\n        self.image_size = 518\n        self.vit_feat_dim = 384\n        code_dir = os.path.dirname(os.path.realpath(__file__))\n        self.out_dims = output_dim\n        self.norm_fn = norm_fn",
        "detail": "stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "DepthAnythingFeature",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.extractor",
        "description": "stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "class DepthAnythingFeature(nn.Module):\n    model_configs = {\n        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]}\n    }\n    def __init__(self, encoder='vits'):\n        super().__init__()\n        from depth_anything.dpt import DepthAnything\n        self.encoder = encoder",
        "detail": "stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.extractor",
        "description": "stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "class Feature(nn.Module):\n    def __init__(self, args):\n        super(Feature, self).__init__()\n        self.args = args\n        model = timm.create_model('edgenext_small', pretrained=True, features_only=False)\n        self.stem = model.stem\n        self.stages = model.stages\n        chans = [48, 96, 160, 304]\n        self.chans = chans\n        self.dino = DepthAnythingFeature(encoder=self.args.vit_size)",
        "detail": "stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.core.extractor",
        "description": "stereo.modeling.models.foundationstereo.core.extractor",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom core.submodule import *\nfrom Utils import *\nimport timm\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)",
        "detail": "stereo.modeling.models.foundationstereo.core.extractor",
        "documentation": {}
    },
    {
        "label": "hourglass",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "description": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "peekOfCode": "class hourglass(nn.Module):\n    def __init__(self, cfg, in_channels, feat_dims=None):\n        super().__init__()\n        self.cfg = cfg\n        self.conv1 = nn.Sequential(BasicConv(in_channels, in_channels*2, is_3d=True, bn=True, relu=True, kernel_size=3,\n                                             padding=1, stride=2, dilation=1),\n                                   Conv3dNormActReduced(in_channels*2, in_channels*2, kernel_size=3, kernel_disp=17))\n        self.conv2 = nn.Sequential(BasicConv(in_channels*2, in_channels*4, is_3d=True, bn=True, relu=True, kernel_size=3,\n                                             padding=1, stride=2, dilation=1),\n                                   Conv3dNormActReduced(in_channels*4, in_channels*4, kernel_size=3, kernel_disp=17))",
        "detail": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "documentation": {}
    },
    {
        "label": "FoundationStereo",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "description": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "peekOfCode": "class FoundationStereo(nn.Module, huggingface_hub.PyTorchModelHubMixin):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        context_dims = args.hidden_dims\n        self.cv_group = 8\n        volume_dim = 28\n        self.max_disp = args.max_disp\n        self.cnet = ContextNetDino(args, output_dim=[args.hidden_dims, context_dims], downsample=args.n_downsample)\n        self.update_block = BasicSelectiveMultiUpdateBlock(self.args, self.args.hidden_dims[0], volume_dim=volume_dim)",
        "detail": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "documentation": {}
    },
    {
        "label": "normalize_image",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "description": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "peekOfCode": "def normalize_image(img):\n    '''\n    @img: (B,C,H,W) in range 0-255, RGB order\n    '''\n    tf = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False)\n    return tf(img/255.0).contiguous()\nclass hourglass(nn.Module):\n    def __init__(self, cfg, in_channels, feat_dims=None):\n        super().__init__()\n        self.cfg = cfg",
        "detail": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "description": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom stereo.modeling.models.foundationstereo.core.update import *\nfrom stereo.modeling.models.foundationstereo.core.extractor import *\nfrom stereo.modeling.models.foundationstereo.core.geometry import Combined_Geo_Encoding_Volume\nfrom stereo.modeling.models.foundationstereo.core.submodule import *\nfrom stereo.modeling.models.foundationstereo.core.utils.utils import *\nfrom stereo.modeling.models.foundationstereo.Utils import *\nimport time,huggingface_hub\ntry:",
        "detail": "stereo.modeling.models.foundationstereo.core.foundation_stereo",
        "documentation": {}
    },
    {
        "label": "Combined_Geo_Encoding_Volume",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.geometry",
        "description": "stereo.modeling.models.foundationstereo.core.geometry",
        "peekOfCode": "class Combined_Geo_Encoding_Volume:\n    def __init__(self, init_fmap1, init_fmap2, geo_volume, num_levels=2, dx=None):\n        self.num_levels = num_levels\n        self.geo_volume_pyramid = []\n        self.init_corr_pyramid = []\n        self.dx = dx\n        # all pairs correlation\n        init_corr = Combined_Geo_Encoding_Volume.corr(init_fmap1, init_fmap2)\n        b, h, w, _, w2 = init_corr.shape\n        b, c, d, h, w = geo_volume.shape",
        "detail": "stereo.modeling.models.foundationstereo.core.geometry",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.core.geometry",
        "description": "stereo.modeling.models.foundationstereo.core.geometry",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom Utils import *\nclass Combined_Geo_Encoding_Volume:\n    def __init__(self, init_fmap1, init_fmap2, geo_volume, num_levels=2, dx=None):\n        self.num_levels = num_levels\n        self.geo_volume_pyramid = []\n        self.init_corr_pyramid = []\n        self.dx = dx\n        # all pairs correlation",
        "detail": "stereo.modeling.models.foundationstereo.core.geometry",
        "documentation": {}
    },
    {
        "label": "LayerNorm2d",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class LayerNorm2d(nn.LayerNorm):\n    r\"\"\" https://huggingface.co/spaces/Roll20/pet_score/blob/b258ef28152ab0d5b377d9142a23346f863c1526/lib/timm/models/convnext.py#L85\n    LayerNorm for channels_first tensors with 2d spatial dimensions (ie N, C, H, W).\n    \"\"\"\n    def __init__(self, normalized_shape, eps=1e-6):\n        super().__init__(normalized_shape, eps=eps)\n    def forward(self, x) -> torch.Tensor:\n        \"\"\"\n        @x: (B,C,H,W)\n        \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class BasicConv(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, bn=True, relu=True, norm='batch', **kwargs):\n        super(BasicConv, self).__init__()\n        self.relu = relu\n        self.use_bn = bn\n        self.bn = nn.Identity()\n        if is_3d:\n            if deconv:\n                self.conv = nn.ConvTranspose3d(in_channels, out_channels, bias=False, **kwargs)\n            else:",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "Conv3dNormActReduced",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class Conv3dNormActReduced(nn.Module):\n    def __init__(self, C_in, C_out, hidden=None, kernel_size=3, kernel_disp=None, stride=1, norm=nn.BatchNorm3d):\n        super().__init__()\n        if kernel_disp is None:\n          kernel_disp = kernel_size\n        if hidden is None:\n            hidden = C_out\n        self.conv1 = nn.Sequential(\n            nn.Conv3d(C_in, hidden, kernel_size=(1,kernel_size,kernel_size), padding=(0, kernel_size//2, kernel_size//2), stride=(1, stride, stride)),\n            norm(hidden),",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "ResnetBasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class ResnetBasicBlock(nn.Module):\n  def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=nn.BatchNorm2d, bias=False):\n    super().__init__()\n    self.norm_layer = norm_layer\n    if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n    if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n    # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n    self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=kernel_size, stride=stride, bias=bias, padding=padding)",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "ResnetBasicBlock3D",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class ResnetBasicBlock3D(nn.Module):\n  def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, downsample=None, groups=1, base_width=64, dilation=1, norm_layer=nn.BatchNorm3d, bias=False):\n    super().__init__()\n    self.norm_layer = norm_layer\n    if groups != 1 or base_width != 64:\n            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n    if dilation > 1:\n            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n    self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=kernel_size, stride=stride, bias=bias, padding=padding)\n    if self.norm_layer is not None:",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "FlashMultiheadAttention",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class FlashMultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "FlashAttentionTransformerEncoderLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class FlashAttentionTransformerEncoderLayer(nn.Module):\n    def __init__(self, embed_dim, num_heads, dim_feedforward, dropout=0.1, act=nn.GELU, norm=nn.LayerNorm):\n        super().__init__()\n        self.self_attn = FlashMultiheadAttention(embed_dim, num_heads)\n        self.act = act()\n        self.linear1 = nn.Linear(embed_dim, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, embed_dim)\n        self.norm1 = norm(embed_dim)\n        self.norm2 = norm(embed_dim)",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "UpsampleConv",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class UpsampleConv(nn.Module):\n    def __init__(self, C_in, C_out, is_3d=False, kernel_size=3, bias=True, stride=1, padding=1):\n        super().__init__()\n        self.is_3d = is_3d\n        if is_3d:\n          self.conv = nn.Conv3d(C_in, C_out, kernel_size=kernel_size, stride=1, padding=kernel_size//2, bias=bias)\n        else:\n          self.conv = nn.Conv2d(C_in, C_out, kernel_size=kernel_size, stride=1, padding=kernel_size//2, bias=bias)\n    def forward(self, x):\n        if self.is_3d:",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "Conv2x",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class Conv2x(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, concat=True, keep_concat=True, bn=True, relu=True, keep_dispc=False):\n        super(Conv2x, self).__init__()\n        self.concat = concat\n        self.is_3d = is_3d\n        if deconv and is_3d:\n            kernel = (4, 4, 4)\n        elif deconv:\n            kernel = 4\n        else:",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv_IN",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class BasicConv_IN(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, IN=True, relu=True, **kwargs):\n        super(BasicConv_IN, self).__init__()\n        self.relu = relu\n        self.use_in = IN\n        if is_3d:\n            if deconv:\n                self.conv = nn.ConvTranspose3d(in_channels, out_channels, bias=False, **kwargs)\n            else:\n                self.conv = nn.Conv3d(in_channels, out_channels, bias=False, **kwargs)",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "Conv2x_IN",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class Conv2x_IN(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, concat=True, keep_concat=True, IN=True, relu=True, keep_dispc=False):\n        super(Conv2x_IN, self).__init__()\n        self.concat = concat\n        self.is_3d = is_3d\n        if deconv and is_3d:\n            kernel = (4, 4, 4)\n        elif deconv:\n            kernel = 4\n        else:",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "FeatureAtt",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class FeatureAtt(nn.Module):\n    def __init__(self, cv_chan, feat_chan):\n        super(FeatureAtt, self).__init__()\n        self.feat_att = nn.Sequential(\n            BasicConv(feat_chan, feat_chan//2, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(feat_chan//2, cv_chan, 1)\n            )\n    def forward(self, cv, feat):\n        '''\n        @cv: cost volume (B,C,D,H,W)",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "PositionalEmbedding",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class PositionalEmbedding(nn.Module):\n  def __init__(self, d_model, max_len=512):\n    super().__init__()\n    # Compute the positional encodings once in log space.\n    pe = torch.zeros(max_len, d_model).float()\n    pe.require_grad = False\n    position = torch.arange(0, max_len).float().unsqueeze(1)  #(N,1)\n    div_term = (torch.arange(0, d_model, 2).float() * -(np.log(10000.0) / d_model)).exp()[None]\n    pe[:, 0::2] = torch.sin(position * div_term)  #(N, d_model/2)\n    pe[:, 1::2] = torch.cos(position * div_term)",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "CostVolumeDisparityAttention",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class CostVolumeDisparityAttention(nn.Module):\n  def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1, act=nn.GELU, norm_first=False, num_transformer=6, max_len=512, resize_embed=False):\n    super().__init__()\n    self.resize_embed = resize_embed\n    self.sa = nn.ModuleList([])\n    for _ in range(num_transformer):\n      self.sa.append(FlashAttentionTransformerEncoderLayer(embed_dim=d_model, num_heads=nhead, dim_feedforward=dim_feedforward, act=act, dropout=dropout))\n    self.pos_embed0 = PositionalEmbedding(d_model, max_len=max_len)\n  def forward(self, cv, window_size=(-1,-1)):\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "ChannelAttentionEnhancement",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class ChannelAttentionEnhancement(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttentionEnhancement, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n                               nn.ReLU(),\n                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "SpatialAttentionExtractor",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class SpatialAttentionExtractor(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttentionExtractor, self).__init__()\n        self.samconv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.samconv(x)",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "EdgeNextConvEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "class EdgeNextConvEncoder(nn.Module):\n    def __init__(self, dim, layer_scale_init_value=1e-6, expan_ratio=4, kernel_size=7, norm='layer'):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, kernel_size=kernel_size, padding=kernel_size // 2, groups=dim)\n        if norm=='layer':\n          self.norm = LayerNorm2d(dim, eps=1e-6)\n        else:\n          self.norm = nn.Identity()\n        self.pwconv1 = nn.Linear(dim, expan_ratio * dim)\n        self.act = nn.GELU()",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0, f\"C:{C}, num_groups:{num_groups}\"\n    channels_per_group = C // num_groups\n    fea1 = fea1.reshape(B, num_groups, channels_per_group, H, W)\n    fea2 = fea2.reshape(B, num_groups, channels_per_group, H, W)\n    with torch.cuda.amp.autocast(enabled=False):\n      cost = (F.normalize(fea1.float(), dim=2) * F.normalize(fea2.float(), dim=2)).sum(dim=2)  #!NOTE Divide first for numerical stability\n    assert cost.shape == (B, num_groups, H, W)\n    return cost",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups, stride=1):\n    \"\"\"\n    @refimg_fea: left image feature\n    @targetimg_fea: right image feature\n    \"\"\"\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i], num_groups)",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, :] = refimg_fea[:, :, :, :]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.reshape(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=True)\nclass FeatureAtt(nn.Module):\n    def __init__(self, cv_chan, feat_chan):\n        super(FeatureAtt, self).__init__()\n        self.feat_att = nn.Sequential(\n            BasicConv(feat_chan, feat_chan//2, kernel_size=1, stride=1, padding=0),",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "context_upsample",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "def context_upsample(disp_low, up_weights):\n    \"\"\"\n    @disp_low: (b,1,h,w)  1/4 resolution\n    @up_weights: (b,9,4*h,4*w)  Image resolution\n    \"\"\"\n    b, c, h, w = disp_low.shape\n    disp_unfold = F.unfold(disp_low.reshape(b,c,h,w),3,1,1).reshape(b,-1,h,w)\n    disp_unfold = F.interpolate(disp_unfold,(h*4,w*4),mode='nearest').reshape(b,9,h*4,w*4)\n    disp = (disp_unfold*up_weights).sum(1)\n    return disp",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.core.submodule",
        "description": "stereo.modeling.models.foundationstereo.core.submodule",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom Utils import *\nfrom flash_attn import flash_attn_qkvpacked_func, flash_attn_func\ndef _is_contiguous(tensor: torch.Tensor) -> bool:\n    if torch.jit.is_scripting():\n        return tensor.is_contiguous()\n    else:\n        return tensor.is_contiguous(memory_format=torch.contiguous_format)\nclass LayerNorm2d(nn.LayerNorm):",
        "detail": "stereo.modeling.models.foundationstereo.core.submodule",
        "documentation": {}
    },
    {
        "label": "DispHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):\n        super(DispHead, self).__init__()\n        self.conv = nn.Sequential(\n          nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1),\n          nn.ReLU(),\n          EdgeNextConvEncoder(input_dim, expan_ratio=4, kernel_size=7, norm=None),\n          EdgeNextConvEncoder(input_dim, expan_ratio=4, kernel_size=7, norm=None),\n          nn.Conv2d(input_dim, output_dim, 3, padding=1),\n        )",
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "ConvGRU",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class ConvGRU(nn.Module):\n    def __init__(self, hidden_dim, input_dim, kernel_size=3):\n        super(ConvGRU, self).__init__()\n        self.convz = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size//2)\n        self.convr = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size//2)\n        self.convq = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size//2)\n    def forward(self, h, cz, cr, cq, *x_list):\n        x = torch.cat(x_list, dim=1)\n        hx = torch.cat([h, x], dim=1)\n        z = torch.sigmoid(self.convz(hx) + cz)",
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "BasicMotionEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class BasicMotionEncoder(nn.Module):\n    def __init__(self, args, ngroup=8):\n        super(BasicMotionEncoder, self).__init__()\n        self.args = args\n        cor_planes = args.corr_levels * (2*args.corr_radius + 1) * (ngroup+1)\n        self.convc1 = nn.Conv2d(cor_planes, 256, 1, padding=0)\n        self.convc2 = nn.Conv2d(256, 256, 3, padding=1)\n        self.convd1 = nn.Conv2d(1, 64, 7, padding=3)\n        self.convd2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.conv = nn.Conv2d(64+256, 128-1, 3, padding=1)",
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "RaftConvGRU",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class RaftConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=256, kernel_size=3):\n        super().__init__()\n        self.convz = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convq = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n    def forward(self, h, x, hx):\n        z = torch.sigmoid(self.convz(hx))\n        r = torch.sigmoid(self.convr(hx))\n        q = torch.tanh(self.convq(torch.cat([r*h, x], dim=1)))",
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "SelectiveConvGRU",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class SelectiveConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=256, small_kernel_size=1, large_kernel_size=3, patch_size=None):\n        super(SelectiveConvGRU, self).__init__()\n        self.conv0 = nn.Sequential(\n            nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1),\n            nn.ReLU(),\n        )\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(input_dim+hidden_dim, input_dim+hidden_dim, kernel_size=3, padding=1),\n            nn.ReLU(),",
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "BasicSelectiveMultiUpdateBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "class BasicSelectiveMultiUpdateBlock(nn.Module):\n    def __init__(self, args, hidden_dim=128, volume_dim=8):\n        super().__init__()\n        self.args = args\n        self.encoder = BasicMotionEncoder(args, volume_dim)\n        if args.n_gru_layers == 3:\n            self.gru16 = SelectiveConvGRU(hidden_dim, hidden_dim * 2)\n        if args.n_gru_layers >= 2:\n            self.gru08 = SelectiveConvGRU(hidden_dim, hidden_dim * (args.n_gru_layers == 3) + hidden_dim * 2)\n        self.gru04 = SelectiveConvGRU(hidden_dim, hidden_dim * (args.n_gru_layers > 1) + hidden_dim * 2)",
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "pool2x",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "def pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)\ndef pool4x(x):\n    return F.avg_pool2d(x, 5, stride=4, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass RaftConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=256, kernel_size=3):\n        super().__init__()",
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "pool4x",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "def pool4x(x):\n    return F.avg_pool2d(x, 5, stride=4, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass RaftConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=256, kernel_size=3):\n        super().__init__()\n        self.convz = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)",
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "interp",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "def interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass RaftConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=256, kernel_size=3):\n        super().__init__()\n        self.convz = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convq = nn.Conv2d(hidden_dim+input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n    def forward(self, h, x, hx):",
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.core.update",
        "description": "stereo.modeling.models.foundationstereo.core.update",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom core.submodule import *\nfrom core.extractor import *\nclass DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):\n        super(DispHead, self).__init__()\n        self.conv = nn.Sequential(\n          nn.Conv2d(input_dim, input_dim, kernel_size=3, padding=1),\n          nn.ReLU(),",
        "detail": "stereo.modeling.models.foundationstereo.core.update",
        "documentation": {}
    },
    {
        "label": "Resize",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "description": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "peekOfCode": "class Resize(object):\n    \"\"\"Resize sample to given size (width, height).\n    \"\"\"\n    def __init__(\n        self,\n        width,\n        height,\n        resize_target=True,\n        keep_aspect_ratio=False,\n        ensure_multiple_of=1,",
        "detail": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "documentation": {}
    },
    {
        "label": "NormalizeImage",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "description": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "peekOfCode": "class NormalizeImage(object):\n    \"\"\"Normlize image by given mean and std.\n    \"\"\"\n    def __init__(self, mean, std):\n        self.__mean = mean\n        self.__std = std\n    def __call__(self, sample):\n        sample[\"image\"] = (sample[\"image\"] - self.__mean) / self.__std\n        return sample\nclass PrepareForNet(object):",
        "detail": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "documentation": {}
    },
    {
        "label": "PrepareForNet",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "description": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "peekOfCode": "class PrepareForNet(object):\n    \"\"\"Prepare sample for usage as network input.\n    \"\"\"\n    def __init__(self):\n        pass\n    def __call__(self, sample):\n        image = np.transpose(sample[\"image\"], (2, 0, 1))\n        sample[\"image\"] = np.ascontiguousarray(image).astype(np.float32)\n        if \"mask\" in sample:\n            sample[\"mask\"] = sample[\"mask\"].astype(np.float32)",
        "detail": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "documentation": {}
    },
    {
        "label": "apply_min_size",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "description": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "peekOfCode": "def apply_min_size(sample, size, image_interpolation_method=cv2.INTER_AREA):\n    \"\"\"Rezise the sample to ensure the given size. Keeps aspect ratio.\n    Args:\n        sample (dict): sample\n        size (tuple): image size\n    Returns:\n        tuple: new size\n    \"\"\"\n    shape = list(sample[\"disparity\"].shape)\n    if shape[0] >= size[0] and shape[1] >= size[1]:",
        "detail": "stereo.modeling.models.foundationstereo.depth_anything.util.transform",
        "documentation": {}
    },
    {
        "label": "ResidualConvUnit",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "description": "stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "peekOfCode": "class ResidualConvUnit(nn.Module):\n    \"\"\"Residual convolution module.\n    \"\"\"\n    def __init__(self, features, activation, bn):\n        \"\"\"Init.\n        Args:\n            features (int): number of features\n        \"\"\"\n        super().__init__()\n        self.bn = bn",
        "detail": "stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "description": "stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "peekOfCode": "class FeatureFusionBlock(nn.Module):\n    \"\"\"Feature fusion block.\n    \"\"\"\n    def __init__(self, features, activation, deconv=False, bn=False, expand=False, align_corners=True, size=None):\n        \"\"\"Init.\n        Args:\n            features (int): number of features\n        \"\"\"\n        super(FeatureFusionBlock, self).__init__()\n        self.deconv = deconv",
        "detail": "stereo.modeling.models.foundationstereo.depth_anything.blocks",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "description": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "peekOfCode": "class DPTHead(nn.Module):\n    def __init__(self, nclass, in_channels, features=256, use_bn=False, out_channels=[256, 512, 1024, 1024], use_clstoken=False):\n        super(DPTHead, self).__init__()\n        self.nclass = nclass\n        self.use_clstoken = use_clstoken\n        self.projects = nn.ModuleList([\n            nn.Conv2d(\n                in_channels=in_channels,\n                out_channels=out_channel,\n                kernel_size=1,",
        "detail": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "documentation": {}
    },
    {
        "label": "DPT_DINOv2",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "description": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "peekOfCode": "class DPT_DINOv2(nn.Module):\n    def __init__(self, encoder='vitl', features=256, out_channels=[256, 512, 1024, 1024], use_bn=False, use_clstoken=False, pretrained_dino=False):\n        super(DPT_DINOv2, self).__init__()\n        assert encoder in ['vits', 'vitb', 'vitl']\n        # in case the Internet connection is not stable, please load the DINOv2 locally\n        # if localhub:\n        #     self.pretrained = torch.hub.load('torchhub/facebookresearch_dinov2_main', 'dinov2_{:}14'.format(encoder), source='local', pretrained=False)\n        # else:\n        # self.pretrained = torch.hub.load('facebookresearch/dinov2', 'dinov2_{:}14'.format(encoder), pretrained=pretrained_dino, skip_validation=True)\n        self.pretrained = torch.hub.load('/file_system/vepfs/algorithm/chenming.zhang/.cache/torch/hub/facebookresearch_dinov2_main', 'dinov2_{:}14'.format(encoder),",
        "detail": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "documentation": {}
    },
    {
        "label": "DepthAnything",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "description": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "peekOfCode": "class DepthAnything(DPT_DINOv2):\n    def __init__(self, config):\n        super().__init__(**config)\n    def forward(self, x):\n        h, w = x.shape[-2:]\n        features = self.pretrained.get_intermediate_layers(x, 4, return_class_token=True)\n        patch_size = self.pretrained.patch_size\n        patch_h, patch_w = h // patch_size, w // patch_size\n        depth = self.depth_head(features, patch_h, patch_w, patch_size=patch_size)\n        depth = F.interpolate(depth, size=(h, w), mode=\"bilinear\", align_corners=True)",
        "detail": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "description": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(f'{code_dir}/../')\nfrom dinov2.models.vision_transformer import vit_small,vit_base,vit_large\nfrom depth_anything.blocks import FeatureFusionBlock, _make_scratch\ndef _make_fusion_block(features, use_bn, size = None):\n    return FeatureFusionBlock(\n        features,\n        nn.ReLU(False),\n        deconv=False,\n        bn=use_bn,",
        "detail": "stereo.modeling.models.foundationstereo.depth_anything.dpt",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.decoders",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.decoders",
        "peekOfCode": "class Decoder:\n    def decode(self) -> Any:\n        raise NotImplementedError\nclass ImageDataDecoder(Decoder):\n    def __init__(self, image_data: bytes) -> None:\n        self._image_data = image_data\n    def decode(self) -> Image:\n        f = BytesIO(self._image_data)\n        return Image.open(f).convert(mode=\"RGB\")\nclass TargetDecoder(Decoder):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.decoders",
        "documentation": {}
    },
    {
        "label": "ImageDataDecoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.decoders",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.decoders",
        "peekOfCode": "class ImageDataDecoder(Decoder):\n    def __init__(self, image_data: bytes) -> None:\n        self._image_data = image_data\n    def decode(self) -> Image:\n        f = BytesIO(self._image_data)\n        return Image.open(f).convert(mode=\"RGB\")\nclass TargetDecoder(Decoder):\n    def __init__(self, target: Any):\n        self._target = target\n    def decode(self) -> Any:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.decoders",
        "documentation": {}
    },
    {
        "label": "TargetDecoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.decoders",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.decoders",
        "peekOfCode": "class TargetDecoder(Decoder):\n    def __init__(self, target: Any):\n        self._target = target\n    def decode(self) -> Any:\n        return self._target",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.decoders",
        "documentation": {}
    },
    {
        "label": "ExtendedVisionDataset",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.extended",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.extended",
        "peekOfCode": "class ExtendedVisionDataset(VisionDataset):\n    def __init__(self, *args, **kwargs) -> None:\n        super().__init__(*args, **kwargs)  # type: ignore\n    def get_image_data(self, index: int) -> bytes:\n        raise NotImplementedError\n    def get_target(self, index: int) -> Any:\n        raise NotImplementedError\n    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n        try:\n            image_data = self.get_image_data(index)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.extended",
        "documentation": {}
    },
    {
        "label": "_Split",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "peekOfCode": "class _Split(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"  # NOTE: torchvision does not support the test split\n    @property\n    def length(self) -> int:\n        split_lengths = {\n            _Split.TRAIN: 1_281_167,\n            _Split.VAL: 50_000,\n            _Split.TEST: 100_000,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "documentation": {}
    },
    {
        "label": "ImageNet",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "peekOfCode": "class ImageNet(ExtendedVisionDataset):\n    Target = Union[_Target]\n    Split = Union[_Split]\n    def __init__(\n        self,\n        *,\n        split: \"ImageNet.Split\",\n        root: str,\n        extra: str,\n        transforms: Optional[Callable] = None,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\n_Target = int\nclass _Split(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"  # NOTE: torchvision does not support the test split\n    @property\n    def length(self) -> int:\n        split_lengths = {\n            _Split.TRAIN: 1_281_167,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "documentation": {}
    },
    {
        "label": "_Target",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "peekOfCode": "_Target = int\nclass _Split(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    TEST = \"test\"  # NOTE: torchvision does not support the test split\n    @property\n    def length(self) -> int:\n        split_lengths = {\n            _Split.TRAIN: 1_281_167,\n            _Split.VAL: 50_000,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net",
        "documentation": {}
    },
    {
        "label": "_ClassEntry",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "peekOfCode": "class _ClassEntry:\n    block_offset: int\n    maybe_filename: Optional[str] = None\n@dataclass\nclass _Entry:\n    class_index: int  # noqa: E701\n    start_offset: int\n    end_offset: int\n    filename: str\nclass _Split(Enum):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "documentation": {}
    },
    {
        "label": "_Entry",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "peekOfCode": "class _Entry:\n    class_index: int  # noqa: E701\n    start_offset: int\n    end_offset: int\n    filename: str\nclass _Split(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    @property\n    def length(self) -> int:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "documentation": {}
    },
    {
        "label": "_Split",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "peekOfCode": "class _Split(Enum):\n    TRAIN = \"train\"\n    VAL = \"val\"\n    @property\n    def length(self) -> int:\n        return {\n            _Split.TRAIN: 11_797_647,\n            _Split.VAL: 561_050,\n        }[self]\n    def entries_path(self):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "documentation": {}
    },
    {
        "label": "ImageNet22k",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "peekOfCode": "class ImageNet22k(ExtendedVisionDataset):\n    _GZIPPED_INDICES: Set[int] = {\n        841_545,\n        1_304_131,\n        2_437_921,\n        2_672_079,\n        2_795_676,\n        2_969_786,\n        6_902_965,\n        6_903_550,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "documentation": {}
    },
    {
        "label": "_Labels",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "peekOfCode": "_Labels = int\n_DEFAULT_MMAP_CACHE_SIZE = 16  # Warning: This can exhaust file descriptors\n@dataclass\nclass _ClassEntry:\n    block_offset: int\n    maybe_filename: Optional[str] = None\n@dataclass\nclass _Entry:\n    class_index: int  # noqa: E701\n    start_offset: int",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "documentation": {}
    },
    {
        "label": "_DEFAULT_MMAP_CACHE_SIZE",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "peekOfCode": "_DEFAULT_MMAP_CACHE_SIZE = 16  # Warning: This can exhaust file descriptors\n@dataclass\nclass _ClassEntry:\n    block_offset: int\n    maybe_filename: Optional[str] = None\n@dataclass\nclass _Entry:\n    class_index: int  # noqa: E701\n    start_offset: int\n    end_offset: int",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.datasets.image_net_22k",
        "documentation": {}
    },
    {
        "label": "DatasetWithEnumeratedTargets",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.adapters",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.adapters",
        "peekOfCode": "class DatasetWithEnumeratedTargets(Dataset):\n    def __init__(self, dataset):\n        self._dataset = dataset\n    def get_image_data(self, index: int) -> bytes:\n        return self._dataset.get_image_data(index)\n    def get_target(self, index: int) -> Tuple[Any, int]:\n        target = self._dataset.get_target(index)\n        return (index, target)\n    def __getitem__(self, index: int) -> Tuple[Any, Tuple[Any, int]]:\n        image, target = self._dataset[index]",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.adapters",
        "documentation": {}
    },
    {
        "label": "DataAugmentationDINO",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.augmentations",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.augmentations",
        "peekOfCode": "class DataAugmentationDINO(object):\n    def __init__(\n        self,\n        global_crops_scale,\n        local_crops_scale,\n        local_crops_number,\n        global_crops_size=224,\n        local_crops_size=96,\n    ):\n        self.global_crops_scale = global_crops_scale",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.augmentations",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.augmentations",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.augmentations",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass DataAugmentationDINO(object):\n    def __init__(\n        self,\n        global_crops_scale,\n        local_crops_scale,\n        local_crops_number,\n        global_crops_size=224,\n        local_crops_size=96,\n    ):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.augmentations",
        "documentation": {}
    },
    {
        "label": "collate_data_and_cast",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.collate",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.collate",
        "peekOfCode": "def collate_data_and_cast(samples_list, mask_ratio_tuple, mask_probability, dtype, n_tokens=None, mask_generator=None):\n    # dtype = torch.half  # TODO: Remove\n    n_global_crops = len(samples_list[0][0][\"global_crops\"])\n    n_local_crops = len(samples_list[0][0][\"local_crops\"])\n    collated_global_crops = torch.stack([s[0][\"global_crops\"][i] for i in range(n_global_crops) for s in samples_list])\n    collated_local_crops = torch.stack([s[0][\"local_crops\"][i] for i in range(n_local_crops) for s in samples_list])\n    B = len(collated_global_crops)\n    N = n_tokens\n    n_samples_masked = int(B * mask_probability)\n    probs = torch.linspace(*mask_ratio_tuple, n_samples_masked + 1)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.collate",
        "documentation": {}
    },
    {
        "label": "SamplerType",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "peekOfCode": "class SamplerType(Enum):\n    DISTRIBUTED = 0\n    EPOCH = 1\n    INFINITE = 2\n    SHARDED_INFINITE = 3\n    SHARDED_INFINITE_NEW = 4\ndef _make_bool_str(b: bool) -> str:\n    return \"yes\" if b else \"no\"\ndef _make_sample_transform(image_transform: Optional[Callable] = None, target_transform: Optional[Callable] = None):\n    def transform(sample):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "documentation": {}
    },
    {
        "label": "make_dataset",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "peekOfCode": "def make_dataset(\n    *,\n    dataset_str: str,\n    transform: Optional[Callable] = None,\n    target_transform: Optional[Callable] = None,\n):\n    \"\"\"\n    Creates a dataset with the specified parameters.\n    Args:\n        dataset_str: A dataset string description (e.g. ImageNet:split=TRAIN).",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "documentation": {}
    },
    {
        "label": "make_data_loader",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "peekOfCode": "def make_data_loader(\n    *,\n    dataset,\n    batch_size: int,\n    num_workers: int,\n    shuffle: bool = True,\n    seed: int = 0,\n    sampler_type: Optional[SamplerType] = SamplerType.INFINITE,\n    sampler_size: int = -1,\n    sampler_advance: int = 0,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass SamplerType(Enum):\n    DISTRIBUTED = 0\n    EPOCH = 1\n    INFINITE = 2\n    SHARDED_INFINITE = 3\n    SHARDED_INFINITE_NEW = 4\ndef _make_bool_str(b: bool) -> str:\n    return \"yes\" if b else \"no\"\ndef _make_sample_transform(image_transform: Optional[Callable] = None, target_transform: Optional[Callable] = None):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "peekOfCode": "T = TypeVar(\"T\")\ndef make_data_loader(\n    *,\n    dataset,\n    batch_size: int,\n    num_workers: int,\n    shuffle: bool = True,\n    seed: int = 0,\n    sampler_type: Optional[SamplerType] = SamplerType.INFINITE,\n    sampler_size: int = -1,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.loaders",
        "documentation": {}
    },
    {
        "label": "MaskingGenerator",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.masking",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.masking",
        "peekOfCode": "class MaskingGenerator:\n    def __init__(\n        self,\n        input_size,\n        num_masking_patches=None,\n        min_num_patches=4,\n        max_num_patches=None,\n        min_aspect=0.3,\n        max_aspect=None,\n    ):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.masking",
        "documentation": {}
    },
    {
        "label": "EpochSampler",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.samplers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.samplers",
        "peekOfCode": "class EpochSampler(Sampler):\n    def __init__(\n        self,\n        *,\n        size: int,\n        sample_count: int,\n        shuffle: bool = False,\n        seed: int = 0,\n        start: Optional[int] = None,\n        step: Optional[int] = None,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.samplers",
        "documentation": {}
    },
    {
        "label": "InfiniteSampler",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.samplers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.samplers",
        "peekOfCode": "class InfiniteSampler(Sampler):\n    def __init__(\n        self,\n        *,\n        sample_count: int,\n        shuffle: bool = False,\n        seed: int = 0,\n        start: Optional[int] = None,\n        step: Optional[int] = None,\n        advance: int = 0,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.samplers",
        "documentation": {}
    },
    {
        "label": "ShardedInfiniteSampler",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.samplers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.samplers",
        "peekOfCode": "class ShardedInfiniteSampler(Sampler):\n    def __init__(\n        self,\n        *,\n        sample_count: int,\n        shuffle: bool = False,\n        seed: int = 0,\n        start: Optional[int] = None,\n        step: Optional[int] = None,\n        advance: int = 0,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.samplers",
        "documentation": {}
    },
    {
        "label": "GaussianBlur",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "peekOfCode": "class GaussianBlur(transforms.RandomApply):\n    \"\"\"\n    Apply Gaussian Blur to the PIL image.\n    \"\"\"\n    def __init__(self, *, p: float = 0.5, radius_min: float = 0.1, radius_max: float = 2.0):\n        # NOTE: torchvision is applying 1 - probability to return the original image\n        keep_p = 1 - p\n        transform = transforms.GaussianBlur(kernel_size=9, sigma=(radius_min, radius_max))\n        super().__init__(transforms=[transform], p=keep_p)\nclass MaybeToTensor(transforms.ToTensor):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "MaybeToTensor",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "peekOfCode": "class MaybeToTensor(transforms.ToTensor):\n    \"\"\"\n    Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor, or keep as is if already a tensor.\n    \"\"\"\n    def __call__(self, pic):\n        \"\"\"\n        Args:\n            pic (PIL Image, numpy.ndarray or torch.tensor): Image to be converted to tensor.\n        Returns:\n            Tensor: Converted image.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "make_normalize_transform",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "peekOfCode": "def make_normalize_transform(\n    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n) -> transforms.Normalize:\n    return transforms.Normalize(mean=mean, std=std)\n# This roughly matches torchvision's preset for classification training:\n#   https://github.com/pytorch/vision/blob/main/references/classification/presets.py#L6-L44\ndef make_classification_train_transform(\n    *,\n    crop_size: int = 224,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "make_classification_train_transform",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "peekOfCode": "def make_classification_train_transform(\n    *,\n    crop_size: int = 224,\n    interpolation=transforms.InterpolationMode.BICUBIC,\n    hflip_prob: float = 0.5,\n    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n):\n    transforms_list = [transforms.RandomResizedCrop(crop_size, interpolation=interpolation)]\n    if hflip_prob > 0.0:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "make_classification_eval_transform",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "peekOfCode": "def make_classification_eval_transform(\n    *,\n    resize_size: int = 256,\n    interpolation=transforms.InterpolationMode.BICUBIC,\n    crop_size: int = 224,\n    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n) -> transforms.Compose:\n    transforms_list = [\n        transforms.Resize(resize_size, interpolation=interpolation),",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "IMAGENET_DEFAULT_MEAN",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "peekOfCode": "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\ndef make_normalize_transform(\n    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n) -> transforms.Normalize:\n    return transforms.Normalize(mean=mean, std=std)\n# This roughly matches torchvision's preset for classification training:\n#   https://github.com/pytorch/vision/blob/main/references/classification/presets.py#L6-L44\ndef make_classification_train_transform(",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "IMAGENET_DEFAULT_STD",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "description": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "peekOfCode": "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\ndef make_normalize_transform(\n    mean: Sequence[float] = IMAGENET_DEFAULT_MEAN,\n    std: Sequence[float] = IMAGENET_DEFAULT_STD,\n) -> transforms.Normalize:\n    return transforms.Normalize(mean=mean, std=std)\n# This roughly matches torchvision's preset for classification training:\n#   https://github.com/pytorch/vision/blob/main/references/classification/presets.py#L6-L44\ndef make_classification_train_transform(\n    *,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.data.transforms",
        "documentation": {}
    },
    {
        "label": "DinoVisionTransformer",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.backbones.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.backbones.vision_transformer",
        "peekOfCode": "class DinoVisionTransformer(BaseModule):\n    \"\"\"Vision Transformer.\"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.backbones.vision_transformer",
        "documentation": {}
    },
    {
        "label": "DepthBaseDecodeHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.decode_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.decode_head",
        "peekOfCode": "class DepthBaseDecodeHead(BaseModule, metaclass=ABCMeta):\n    \"\"\"Base class for BaseDecodeHead.\n    Args:\n        in_channels (List): Input channels.\n        channels (int): Channels after modules, before conv_depth.\n        conv_cfg (dict|None): Config of conv layers. Default: None.\n        act_cfg (dict): Config of activation layers.\n            Default: dict(type='ReLU')\n        loss_decode (dict): Config of decode loss.\n            Default: dict(type='SigLoss').",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.decode_head",
        "documentation": {}
    },
    {
        "label": "Interpolate",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class Interpolate(nn.Module):\n    def __init__(self, scale_factor, mode, align_corners=False):\n        super(Interpolate, self).__init__()\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.align_corners = align_corners\n    def forward(self, x):\n        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n        return x",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "HeadDepth",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class HeadDepth(nn.Module):\n    def __init__(self, features):\n        super(HeadDepth, self).__init__()\n        self.head = nn.Sequential(\n            nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(features // 2, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n        )",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "ReassembleBlocks",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class ReassembleBlocks(BaseModule):\n    \"\"\"ViTPostProcessBlock, process cls_token in ViT backbone output and\n    rearrange the feature vector to feature map.\n    Args:\n        in_channels (int): ViT feature channels. Default: 768.\n        out_channels (List): output channels of each stage.\n            Default: [96, 192, 384, 768].\n        readout_type (str): Type of readout operation. Default: 'ignore'.\n        patch_size (int): The patch size. Default: 16.\n        init_cfg (dict, optional): Initialization config dict. Default: None.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "PreActResidualConvUnit",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class PreActResidualConvUnit(BaseModule):\n    \"\"\"ResidualConvUnit, pre-activate residual unit.\n    Args:\n        in_channels (int): number of channels in the input feature map.\n        act_cfg (dict): dictionary to construct and config activation layer.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        stride (int): stride of the first block. Default: 1\n        dilation (int): dilation rate for convs layers. Default: 1.\n        init_cfg (dict, optional): Initialization config dict. Default: None.\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class FeatureFusionBlock(BaseModule):\n    \"\"\"FeatureFusionBlock, merge feature map from different stages.\n    Args:\n        in_channels (int): Input channels.\n        act_cfg (dict): The activation config for ResidualConvUnit.\n        norm_cfg (dict): Config dict for normalization layer.\n        expand (bool): Whether expand the channels in post process block.\n            Default: False.\n        align_corners (bool): align_corner setting for bilinear upsample.\n            Default: True.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "peekOfCode": "class DPTHead(DepthBaseDecodeHead):\n    \"\"\"Vision Transformers for Dense Prediction.\n    This head is implemented of `DPT <https://arxiv.org/abs/2103.13413>`_.\n    Args:\n        embed_dims (int): The embed dimension of the ViT backbone.\n            Default: 768.\n        post_process_channels (List): Out channels of post process conv\n            layers. Default: [96, 192, 384, 768].\n        readout_type (str): Type of readout operation. Default: 'ignore'.\n        patch_size (int): The patch size. Default: 16.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.dpt_head",
        "documentation": {}
    },
    {
        "label": "BNHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.linear_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.linear_head",
        "peekOfCode": "class BNHead(DepthBaseDecodeHead):\n    \"\"\"Just a batchnorm.\"\"\"\n    def __init__(self, input_transform=\"resize_concat\", in_index=(0, 1, 2, 3), upsample=1, **kwargs):\n        super().__init__(**kwargs)\n        self.input_transform = input_transform\n        self.in_index = in_index\n        self.upsample = upsample\n        # self.bn = nn.SyncBatchNorm(self.in_channels)\n        if self.classify:\n            self.conv_depth = nn.Conv2d(self.channels, self.n_bins, kernel_size=1, padding=0, stride=1)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.decode_heads.linear_head",
        "documentation": {}
    },
    {
        "label": "BaseDepther",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.base",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.base",
        "peekOfCode": "class BaseDepther(BaseModule, metaclass=ABCMeta):\n    \"\"\"Base class for depther.\"\"\"\n    def __init__(self, init_cfg=None):\n        super(BaseDepther, self).__init__(init_cfg)\n        self.fp16_enabled = False\n    @property\n    def with_neck(self):\n        \"\"\"bool: whether the depther has neck\"\"\"\n        return hasattr(self, \"neck\") and self.neck is not None\n    @property",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.base",
        "documentation": {}
    },
    {
        "label": "DepthEncoderDecoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "peekOfCode": "class DepthEncoderDecoder(BaseDepther):\n    \"\"\"Encoder Decoder depther.\n    EncoderDecoder typically consists of backbone, (neck) and decode_head.\n    \"\"\"\n    def __init__(self, backbone, decode_head, neck=None, train_cfg=None, test_cfg=None, pretrained=None, init_cfg=None):\n        super(DepthEncoderDecoder, self).__init__(init_cfg)\n        if pretrained is not None:\n            assert backbone.get(\"pretrained\") is None, \"both backbone and depther set pretrained weight\"\n            backbone.pretrained = pretrained\n        self.backbone = builder.build_backbone(backbone)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "documentation": {}
    },
    {
        "label": "add_prefix",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "peekOfCode": "def add_prefix(inputs, prefix):\n    \"\"\"Add prefix for dict.\n    Args:\n        inputs (dict): The input dict with str keys.\n        prefix (str): The prefix to add.\n    Returns:\n        dict: The dict with keys updated with ``prefix``.\n    \"\"\"\n    outputs = dict()\n    for name, value in inputs.items():",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.depther.encoder_decoder",
        "documentation": {}
    },
    {
        "label": "GradientLoss",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.gradientloss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.gradientloss",
        "peekOfCode": "class GradientLoss(nn.Module):\n    \"\"\"GradientLoss.\n    Adapted from https://www.cs.cornell.edu/projects/megadepth/\n    Args:\n        valid_mask (bool): Whether filter invalid gt (gt > 0). Default: True.\n        loss_weight (float): Weight of the loss. Default: 1.0.\n        max_depth (int): When filtering invalid gt, set a max threshold. Default: None.\n    \"\"\"\n    def __init__(self, valid_mask=True, loss_weight=1.0, max_depth=None, loss_name=\"loss_grad\"):\n        super(GradientLoss, self).__init__()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.gradientloss",
        "documentation": {}
    },
    {
        "label": "SigLoss",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.sigloss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.sigloss",
        "peekOfCode": "class SigLoss(nn.Module):\n    \"\"\"SigLoss.\n        This follows `AdaBins <https://arxiv.org/abs/2011.14141>`_.\n    Args:\n        valid_mask (bool): Whether filter invalid gt (gt > 0). Default: True.\n        loss_weight (float): Weight of the loss. Default: 1.0.\n        max_depth (int): When filtering invalid gt, set a max threshold. Default: None.\n        warm_up (bool): A simple warm up stage to help convergence. Default: False.\n        warm_iter (int): The number of warm up stage. Default: 100.\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.losses.sigloss",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "def build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return HEADS.build(cfg)\ndef build_loss(cfg):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "build_neck",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "def build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return HEADS.build(cfg)\ndef build_loss(cfg):\n    \"\"\"Build loss.\"\"\"\n    return LOSSES.build(cfg)\ndef build_depther(cfg, train_cfg=None, test_cfg=None):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "build_head",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "def build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return HEADS.build(cfg)\ndef build_loss(cfg):\n    \"\"\"Build loss.\"\"\"\n    return LOSSES.build(cfg)\ndef build_depther(cfg, train_cfg=None, test_cfg=None):\n    \"\"\"Build depther.\"\"\"\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\"train_cfg and test_cfg is deprecated, \" \"please specify them in model\", UserWarning)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "build_loss",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "def build_loss(cfg):\n    \"\"\"Build loss.\"\"\"\n    return LOSSES.build(cfg)\ndef build_depther(cfg, train_cfg=None, test_cfg=None):\n    \"\"\"Build depther.\"\"\"\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\"train_cfg and test_cfg is deprecated, \" \"please specify them in model\", UserWarning)\n    assert cfg.get(\"train_cfg\") is None or train_cfg is None, \"train_cfg specified in both outer field and model field \"\n    assert cfg.get(\"test_cfg\") is None or test_cfg is None, \"test_cfg specified in both outer field and model field \"\n    return DEPTHER.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "build_depther",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "def build_depther(cfg, train_cfg=None, test_cfg=None):\n    \"\"\"Build depther.\"\"\"\n    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\"train_cfg and test_cfg is deprecated, \" \"please specify them in model\", UserWarning)\n    assert cfg.get(\"train_cfg\") is None or train_cfg is None, \"train_cfg specified in both outer field and model field \"\n    assert cfg.get(\"test_cfg\") is None or test_cfg is None, \"test_cfg specified in both outer field and model field \"\n    return DEPTHER.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "MODELS",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "MODELS = Registry(\"models\", parent=MMCV_MODELS)\nATTENTION = Registry(\"attention\", parent=MMCV_ATTENTION)\nBACKBONES = MODELS\nNECKS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "ATTENTION",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "ATTENTION = Registry(\"attention\", parent=MMCV_ATTENTION)\nBACKBONES = MODELS\nNECKS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "BACKBONES",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "BACKBONES = MODELS\nNECKS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "NECKS",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "NECKS = MODELS\nHEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "HEADS",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "HEADS = MODELS\nLOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\ndef build_head(cfg):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "LOSSES",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "LOSSES = MODELS\nDEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "DEPTHER",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "peekOfCode": "DEPTHER = MODELS\ndef build_backbone(cfg):\n    \"\"\"Build backbone.\"\"\"\n    return BACKBONES.build(cfg)\ndef build_neck(cfg):\n    \"\"\"Build neck.\"\"\"\n    return NECKS.build(cfg)\ndef build_head(cfg):\n    \"\"\"Build head.\"\"\"\n    return HEADS.build(cfg)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.models.builder",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.ops.wrappers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.ops.wrappers",
        "peekOfCode": "def resize(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None, warning=False):\n    if warning:\n        if size is not None and align_corners:\n            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n            output_h, output_w = tuple(int(x) for x in size)\n            if output_h > input_h or output_w > output_h:\n                if (\n                    (output_h > 1 and output_w > 1 and input_h > 1 and input_w > 1)\n                    and (output_h - 1) % (input_h - 1)\n                    and (output_w - 1) % (input_w - 1)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.depth.ops.wrappers",
        "documentation": {}
    },
    {
        "label": "DistOptimizerHook",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.hooks.optimizer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.hooks.optimizer",
        "peekOfCode": "class DistOptimizerHook(OptimizerHook):\n    \"\"\"Optimizer hook for distributed training.\"\"\"\n    def __init__(self, update_interval=1, grad_clip=None, coalesce=True, bucket_size_mb=-1, use_fp16=False):\n        self.grad_clip = grad_clip\n        self.coalesce = coalesce\n        self.bucket_size_mb = bucket_size_mb\n        self.update_interval = update_interval\n        self.use_fp16 = use_fp16\n    def before_run(self, runner):\n        runner.optimizer.zero_grad()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.hooks.optimizer",
        "documentation": {}
    },
    {
        "label": "DinoVisionTransformer",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.backbones.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.backbones.vision_transformer",
        "peekOfCode": "class DinoVisionTransformer(BaseModule):\n    \"\"\"Vision Transformer.\"\"\"\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        super().__init__()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.backbones.vision_transformer",
        "documentation": {}
    },
    {
        "label": "BNHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.decode_heads.linear_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.decode_heads.linear_head",
        "peekOfCode": "class BNHead(BaseDecodeHead):\n    \"\"\"Just a batchnorm.\"\"\"\n    def __init__(self, resize_factors=None, **kwargs):\n        super().__init__(**kwargs)\n        assert self.in_channels == self.channels\n        self.bn = nn.SyncBatchNorm(self.in_channels)\n        self.resize_factors = resize_factors\n    def _forward_feature(self, inputs):\n        \"\"\"Forward function for feature maps before classifying each pixel with\n        ``self.cls_seg`` fc.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.models.decode_heads.linear_head",
        "documentation": {}
    },
    {
        "label": "ADE20K_COLORMAP",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "peekOfCode": "ADE20K_COLORMAP = [\n    (0, 0, 0),\n    (120, 120, 120),\n    (180, 120, 120),\n    (6, 230, 230),\n    (80, 50, 50),\n    (4, 200, 3),\n    (120, 120, 80),\n    (140, 140, 140),\n    (204, 5, 255),",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "documentation": {}
    },
    {
        "label": "ADE20K_CLASS_NAMES",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "peekOfCode": "ADE20K_CLASS_NAMES = [\n    \"\",\n    \"wall\",\n    \"building;edifice\",\n    \"sky\",\n    \"floor;flooring\",\n    \"tree\",\n    \"ceiling\",\n    \"road;route\",\n    \"bed\",",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "documentation": {}
    },
    {
        "label": "VOC2012_COLORMAP",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "peekOfCode": "VOC2012_COLORMAP = [\n    (0, 0, 0),\n    (128, 0, 0),\n    (0, 128, 0),\n    (128, 128, 0),\n    (0, 0, 128),\n    (128, 0, 128),\n    (0, 128, 128),\n    (128, 128, 128),\n    (64, 0, 0),",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "documentation": {}
    },
    {
        "label": "VOC2012_CLASS_NAMES",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "peekOfCode": "VOC2012_CLASS_NAMES = [\n    \"\",\n    \"aeroplane\",\n    \"bicycle\",\n    \"bird\",\n    \"boat\",\n    \"bottle\",\n    \"bus\",\n    \"car\",\n    \"cat\",",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation.utils.colormaps",
        "documentation": {}
    },
    {
        "label": "build_prior_generator",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "peekOfCode": "def build_prior_generator(cfg, default_args=None):\n    return build_from_cfg(cfg, PRIOR_GENERATORS, default_args)\ndef build_anchor_generator(cfg, default_args=None):\n    warnings.warn(\"``build_anchor_generator`` would be deprecated soon, please use \" \"``build_prior_generator`` \")\n    return build_prior_generator(cfg, default_args=default_args)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "documentation": {}
    },
    {
        "label": "build_anchor_generator",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "peekOfCode": "def build_anchor_generator(cfg, default_args=None):\n    warnings.warn(\"``build_anchor_generator`` would be deprecated soon, please use \" \"``build_prior_generator`` \")\n    return build_prior_generator(cfg, default_args=default_args)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "documentation": {}
    },
    {
        "label": "PRIOR_GENERATORS",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "peekOfCode": "PRIOR_GENERATORS = Registry(\"Generator for anchors and points\")\nANCHOR_GENERATORS = PRIOR_GENERATORS\ndef build_prior_generator(cfg, default_args=None):\n    return build_from_cfg(cfg, PRIOR_GENERATORS, default_args)\ndef build_anchor_generator(cfg, default_args=None):\n    warnings.warn(\"``build_anchor_generator`` would be deprecated soon, please use \" \"``build_prior_generator`` \")\n    return build_prior_generator(cfg, default_args=default_args)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "documentation": {}
    },
    {
        "label": "ANCHOR_GENERATORS",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "peekOfCode": "ANCHOR_GENERATORS = PRIOR_GENERATORS\ndef build_prior_generator(cfg, default_args=None):\n    return build_from_cfg(cfg, PRIOR_GENERATORS, default_args)\ndef build_anchor_generator(cfg, default_args=None):\n    warnings.warn(\"``build_anchor_generator`` would be deprecated soon, please use \" \"``build_prior_generator`` \")\n    return build_prior_generator(cfg, default_args=default_args)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.builder",
        "documentation": {}
    },
    {
        "label": "MlvlPointGenerator",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.point_generator",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.point_generator",
        "peekOfCode": "class MlvlPointGenerator:\n    \"\"\"Standard points generator for multi-level (Mlvl) feature maps in 2D\n    points-based detectors.\n    Args:\n        strides (list[int] | list[tuple[int, int]]): Strides of anchors\n            in multiple feature levels in order (w, h).\n        offset (float): The offset of points, the value is normalized with\n            corresponding stride. Defaults to 0.5.\n    \"\"\"\n    def __init__(self, strides, offset=0.5):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.anchor.point_generator",
        "documentation": {}
    },
    {
        "label": "BaseSampler",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.base_sampler",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.base_sampler",
        "peekOfCode": "class BaseSampler(metaclass=ABCMeta):\n    \"\"\"Base class of samplers.\"\"\"\n    def __init__(self, num, pos_fraction, neg_pos_ub=-1, add_gt_as_proposals=True, **kwargs):\n        self.num = num\n        self.pos_fraction = pos_fraction\n        self.neg_pos_ub = neg_pos_ub\n        self.add_gt_as_proposals = add_gt_as_proposals\n        self.pos_sampler = self\n        self.neg_sampler = self\n    @abstractmethod",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.base_sampler",
        "documentation": {}
    },
    {
        "label": "MaskPseudoSampler",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_pseudo_sampler",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_pseudo_sampler",
        "peekOfCode": "class MaskPseudoSampler(BaseSampler):\n    \"\"\"A pseudo sampler that does not do sampling actually.\"\"\"\n    def __init__(self, **kwargs):\n        pass\n    def _sample_pos(self, **kwargs):\n        \"\"\"Sample positive samples.\"\"\"\n        raise NotImplementedError\n    def _sample_neg(self, **kwargs):\n        \"\"\"Sample negative samples.\"\"\"\n        raise NotImplementedError",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_pseudo_sampler",
        "documentation": {}
    },
    {
        "label": "MaskSamplingResult",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_sampling_result",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_sampling_result",
        "peekOfCode": "class MaskSamplingResult(SamplingResult):\n    \"\"\"Mask sampling result.\"\"\"\n    def __init__(self, pos_inds, neg_inds, masks, gt_masks, assign_result, gt_flags):\n        self.pos_inds = pos_inds\n        self.neg_inds = neg_inds\n        self.pos_masks = masks[pos_inds]\n        self.neg_masks = masks[neg_inds]\n        self.pos_is_gt = gt_flags[pos_inds]\n        self.num_gts = gt_masks.shape[0]\n        self.pos_assigned_gt_inds = assign_result.gt_inds[pos_inds] - 1",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.mask_sampling_result",
        "documentation": {}
    },
    {
        "label": "SamplingResult",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.sampling_result",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.sampling_result",
        "peekOfCode": "class SamplingResult:\n    \"\"\"Bbox sampling result.\n    Example:\n        >>> # xdoctest: +IGNORE_WANT\n        >>> from mmdet.core.bbox.samplers.sampling_result import *  # NOQA\n        >>> self = SamplingResult.random(rng=10)\n        >>> print(f'self = {self}')\n        self = <SamplingResult({\n            'neg_bboxes': torch.Size([12, 4]),\n            'neg_inds': tensor([ 0,  1,  2,  4,  5,  6,  7,  8,  9, 10, 11, 12]),",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.samplers.sampling_result",
        "documentation": {}
    },
    {
        "label": "build_sampler",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "peekOfCode": "def build_sampler(cfg, **default_args):\n    \"\"\"Builder of box sampler.\"\"\"\n    return build_from_cfg(cfg, BBOX_SAMPLERS, default_args)\ndef build_bbox_coder(cfg, **default_args):\n    \"\"\"Builder of box coder.\"\"\"\n    return build_from_cfg(cfg, BBOX_CODERS, default_args)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "documentation": {}
    },
    {
        "label": "build_bbox_coder",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "peekOfCode": "def build_bbox_coder(cfg, **default_args):\n    \"\"\"Builder of box coder.\"\"\"\n    return build_from_cfg(cfg, BBOX_CODERS, default_args)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "documentation": {}
    },
    {
        "label": "BBOX_SAMPLERS",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "peekOfCode": "BBOX_SAMPLERS = Registry(\"bbox_sampler\")\nBBOX_CODERS = Registry(\"bbox_coder\")\ndef build_sampler(cfg, **default_args):\n    \"\"\"Builder of box sampler.\"\"\"\n    return build_from_cfg(cfg, BBOX_SAMPLERS, default_args)\ndef build_bbox_coder(cfg, **default_args):\n    \"\"\"Builder of box coder.\"\"\"\n    return build_from_cfg(cfg, BBOX_CODERS, default_args)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "documentation": {}
    },
    {
        "label": "BBOX_CODERS",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "peekOfCode": "BBOX_CODERS = Registry(\"bbox_coder\")\ndef build_sampler(cfg, **default_args):\n    \"\"\"Builder of box sampler.\"\"\"\n    return build_from_cfg(cfg, BBOX_SAMPLERS, default_args)\ndef build_bbox_coder(cfg, **default_args):\n    \"\"\"Builder of box coder.\"\"\"\n    return build_from_cfg(cfg, BBOX_CODERS, default_args)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.box.builder",
        "documentation": {}
    },
    {
        "label": "reduce_mean",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.dist_utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.dist_utils",
        "peekOfCode": "def reduce_mean(tensor):\n    \"\"\" \"Obtain the mean of tensor on different GPUs.\"\"\"\n    if not (dist.is_available() and dist.is_initialized()):\n        return tensor\n    tensor = tensor.clone()\n    dist.all_reduce(tensor.div_(dist.get_world_size()), op=dist.ReduceOp.SUM)\n    return tensor",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "multi_apply",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "peekOfCode": "def multi_apply(func, *args, **kwargs):\n    \"\"\"Apply function to a list of arguments.\n    Note:\n        This function applies the ``func`` to multiple inputs and\n        map the multiple outputs of the ``func`` into different\n        list. Each list contains the same type of outputs corresponding\n        to different inputs.\n    Args:\n        func (Function): A function that will be applied to a list of\n            arguments",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "documentation": {}
    },
    {
        "label": "add_prefix",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "peekOfCode": "def add_prefix(inputs, prefix):\n    \"\"\"Add prefix for dict.\n    Args:\n        inputs (dict): The input dict with str keys.\n        prefix (str): The prefix to add.\n    Returns:\n        dict: The dict with keys updated with ``prefix``.\n    \"\"\"\n    outputs = dict()\n    for name, value in inputs.items():",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.core.utils.misc",
        "documentation": {}
    },
    {
        "label": "ConvFFN",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class ConvFFN(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "DWConv",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class DWConv(nn.Module):\n    def __init__(self, dim=768):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        n = N // 21\n        x1 = x[:, 0 : 16 * n, :].transpose(1, 2).view(B, C, H * 2, W * 2).contiguous()\n        x2 = x[:, 16 * n : 20 * n, :].transpose(1, 2).view(B, C, H, W).contiguous()\n        x3 = x[:, 20 * n :, :].transpose(1, 2).view(B, C, H // 2, W // 2).contiguous()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "Extractor",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class Extractor(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=6,\n        n_points=4,\n        n_levels=1,\n        deform_ratio=1.0,\n        with_cffn=True,\n        cffn_ratio=0.25,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "Injector",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class Injector(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=6,\n        n_points=4,\n        n_levels=1,\n        deform_ratio=1.0,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        init_values=0.0,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "InteractionBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class InteractionBlock(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=6,\n        n_points=4,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        drop=0.0,\n        drop_path=0.0,\n        with_cffn=True,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "InteractionBlockWithCls",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class InteractionBlockWithCls(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads=6,\n        n_points=4,\n        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n        drop=0.0,\n        drop_path=0.0,\n        with_cffn=True,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "SpatialPriorModule",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "class SpatialPriorModule(nn.Module):\n    def __init__(self, inplanes=64, embed_dim=384, with_cp=False):\n        super().__init__()\n        self.with_cp = with_cp\n        self.stem = nn.Sequential(\n            *[\n                nn.Conv2d(3, inplanes, kernel_size=3, stride=2, padding=1, bias=False),\n                nn.SyncBatchNorm(inplanes),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(inplanes, inplanes, kernel_size=3, stride=1, padding=1, bias=False),",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "get_reference_points",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "def get_reference_points(spatial_shapes, device):\n    reference_points_list = []\n    for lvl, (H_, W_) in enumerate(spatial_shapes):\n        ref_y, ref_x = torch.meshgrid(\n            torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n            torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device),\n        )\n        ref_y = ref_y.reshape(-1)[None] / H_\n        ref_x = ref_x.reshape(-1)[None] / W_\n        ref = torch.stack((ref_x, ref_y), -1)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "deform_inputs",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "peekOfCode": "def deform_inputs(x, patch_size):\n    bs, c, h, w = x.shape\n    spatial_shapes = torch.as_tensor(\n        [(h // 8, w // 8), (h // 16, w // 16), (h // 32, w // 32)], dtype=torch.long, device=x.device\n    )\n    level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))\n    reference_points = get_reference_points([(h // patch_size, w // patch_size)], x.device)\n    deform_inputs1 = [reference_points, spatial_shapes, level_start_index]\n    spatial_shapes = torch.as_tensor([(h // patch_size, w // patch_size)], dtype=torch.long, device=x.device)\n    level_start_index = torch.cat((spatial_shapes.new_zeros((1,)), spatial_shapes.prod(1).cumsum(0)[:-1]))",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.adapter_modules",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n    def __init__(self, drop_prob: float = 0.0):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0:\n        random_tensor.div_(keep_prob)\n    return x * random_tensor\nclass DropPath(nn.Module):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.drop_path",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFN",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class SwiGLUFFN(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = None,\n        drop: float = 0.0,\n    ) -> None:\n        super().__init__()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"2D Image to Patch Embedding.\"\"\"\n    def __init__(\n        self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True, bias=True\n    ):\n        super().__init__()\n        img_size = to_2tuple(img_size)\n        patch_size = to_2tuple(patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0.0, proj_drop=0.0):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "MemEffAttention",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class MemEffAttention(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -> None:\n        super().__init__()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "WindowedAttention",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class WindowedAttention(nn.Module):\n    def __init__(\n        self, dim, num_heads=8, qkv_bias=False, attn_drop=0.0, proj_drop=0.0, window_size=14, pad_mode=\"constant\"\n    ):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(\n        self,\n        dim,\n        num_heads,\n        mlp_ratio=4.0,\n        qkv_bias=False,\n        drop=0.0,\n        attn_drop=0.0,\n        drop_path=0.0,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "TIMMVisionTransformer",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "class TIMMVisionTransformer(BaseModule):\n    \"\"\"Vision Transformer.\n    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`\n        - https://arxiv.org/abs/2010.11929\n    Includes distillation token & head support for `DeiT: Data-efficient Image Transformers`\n        - https://arxiv.org/abs/2012.12877\n    \"\"\"\n    def __init__(\n        self,\n        img_size=224,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "to_2tuple",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "def to_2tuple(x):\n    return tuple(repeat(x, 2))\nclass Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        drop: float = 0.0,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "peekOfCode": "def window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit",
        "documentation": {}
    },
    {
        "label": "ViTAdapter",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit_adapter",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit_adapter",
        "peekOfCode": "class ViTAdapter(TIMMVisionTransformer):\n    def __init__(\n        self,\n        pretrain_size=224,\n        num_heads=12,\n        conv_inplane=64,\n        n_points=4,\n        deform_num_heads=6,\n        init_values=0.0,\n        interaction_indexes=None,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.backbones.vit_adapter",
        "documentation": {}
    },
    {
        "label": "Mask2FormerHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.decode_heads.mask2former_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.decode_heads.mask2former_head",
        "peekOfCode": "class Mask2FormerHead(BaseDecodeHead):\n    \"\"\"Implements the Mask2Former head.\n    See `Masked-attention Mask Transformer for Universal Image\n    Segmentation <https://arxiv.org/pdf/2112.01527>`_ for details.\n    Args:\n        in_channels (list[int]): Number of channels in the input feature map.\n        feat_channels (int): Number of channels for features.\n        out_channels (int): Number of channels for output.\n        num_things_classes (int): Number of things.\n        num_stuff_classes (int): Number of stuff.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.decode_heads.mask2former_head",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLoss",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "peekOfCode": "class CrossEntropyLoss(nn.Module):\n    \"\"\"CrossEntropyLoss.\n    Args:\n        use_sigmoid (bool, optional): Whether the prediction uses sigmoid\n            of softmax. Defaults to False.\n        use_mask (bool, optional): Whether to use mask cross entropy loss.\n            Defaults to False.\n        reduction (str, optional): . Defaults to 'mean'.\n            Options are \"none\", \"mean\" and \"sum\".\n        class_weight (list[float] | str, optional): Weight of each class. If in",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "documentation": {}
    },
    {
        "label": "cross_entropy",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "peekOfCode": "def cross_entropy(\n    pred,\n    label,\n    weight=None,\n    class_weight=None,\n    reduction=\"mean\",\n    avg_factor=None,\n    ignore_index=-100,\n    avg_non_ignore=False,\n):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "documentation": {}
    },
    {
        "label": "binary_cross_entropy",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "peekOfCode": "def binary_cross_entropy(\n    pred,\n    label,\n    weight=None,\n    reduction=\"mean\",\n    avg_factor=None,\n    class_weight=None,\n    ignore_index=-100,\n    avg_non_ignore=False,\n    **kwargs,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "documentation": {}
    },
    {
        "label": "mask_cross_entropy",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "peekOfCode": "def mask_cross_entropy(\n    pred, target, label, reduction=\"mean\", avg_factor=None, class_weight=None, ignore_index=None, **kwargs\n):\n    \"\"\"Calculate the CrossEntropy loss for masks.\n    Args:\n        pred (torch.Tensor): The prediction with shape (N, C), C is the number\n            of classes.\n        target (torch.Tensor): The learning label of the prediction.\n        label (torch.Tensor): ``label`` indicates the class label of the mask'\n            corresponding object. This will be used to select the mask in the",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.cross_entropy_loss",
        "documentation": {}
    },
    {
        "label": "DiceLoss",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "peekOfCode": "class DiceLoss(nn.Module):\n    def __init__(self, use_sigmoid=True, activate=True, reduction=\"mean\", naive_dice=False, loss_weight=1.0, eps=1e-3):\n        \"\"\"Dice Loss, there are two forms of dice loss is supported:\n            - the one proposed in `V-Net: Fully Convolutional Neural\n                Networks for Volumetric Medical Image Segmentation\n                <https://arxiv.org/abs/1606.04797>`_.\n            - the dice loss in which the power of the number in the\n                denominator is the first power instead of the second\n                power.\n        Args:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "documentation": {}
    },
    {
        "label": "dice_loss",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "peekOfCode": "def dice_loss(pred, target, weight=None, eps=1e-3, reduction=\"mean\", avg_factor=None):\n    \"\"\"Calculate dice loss, which is proposed in\n    `V-Net: Fully Convolutional Neural Networks for Volumetric\n    Medical Image Segmentation <https://arxiv.org/abs/1606.04797>`_.\n    Args:\n        pred (torch.Tensor): The prediction, has a shape (n, *)\n        target (torch.Tensor): The learning label of the prediction,\n            shape (n, *), same shape of pred.\n        weight (torch.Tensor, optional): The weight of loss for each\n            prediction, has a shape (n,). Defaults to None.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "documentation": {}
    },
    {
        "label": "naive_dice_loss",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "peekOfCode": "def naive_dice_loss(pred, target, weight=None, eps=1e-3, reduction=\"mean\", avg_factor=None):\n    \"\"\"Calculate naive dice loss, the coefficient in the denominator is the\n    first power instead of the second power.\n    Args:\n        pred (torch.Tensor): The prediction, has a shape (n, *)\n        target (torch.Tensor): The learning label of the prediction,\n            shape (n, *), same shape of pred.\n        weight (torch.Tensor, optional): The weight of loss for each\n            prediction, has a shape (n,). Defaults to None.\n        eps (float): Avoid dividing by zero. Default: 1e-3.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.dice_loss",
        "documentation": {}
    },
    {
        "label": "ClassificationCost",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "peekOfCode": "class ClassificationCost:\n    \"\"\"ClsSoftmaxCost.Borrow from\n    mmdet.core.bbox.match_costs.match_cost.ClassificationCost.\n     Args:\n         weight (int | float, optional): loss_weight\n     Examples:\n         >>> import torch\n         >>> self = ClassificationCost()\n         >>> cls_pred = torch.rand(4, 3)\n         >>> gt_labels = torch.tensor([0, 1, 2])",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "documentation": {}
    },
    {
        "label": "DiceCost",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "peekOfCode": "class DiceCost:\n    \"\"\"Cost of mask assignments based on dice losses.\n    Args:\n        weight (int | float, optional): loss_weight. Defaults to 1.\n        pred_act (bool, optional): Whether to apply sigmoid to mask_pred.\n            Defaults to False.\n        eps (float, optional): default 1e-12.\n    \"\"\"\n    def __init__(self, weight=1.0, pred_act=False, eps=1e-3):\n        self.weight = weight",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "documentation": {}
    },
    {
        "label": "CrossEntropyLossCost",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "peekOfCode": "class CrossEntropyLossCost:\n    \"\"\"CrossEntropyLossCost.\n    Args:\n        weight (int | float, optional): loss weight. Defaults to 1.\n        use_sigmoid (bool, optional): Whether the prediction uses sigmoid\n                of softmax. Defaults to True.\n    \"\"\"\n    def __init__(self, weight=1.0, use_sigmoid=True):\n        assert use_sigmoid, \"use_sigmoid = False is not supported yet.\"\n        self.weight = weight",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.losses.match_costs",
        "documentation": {}
    },
    {
        "label": "MSDeformAttnPixelDecoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.plugins.msdeformattn_pixel_decoder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.plugins.msdeformattn_pixel_decoder",
        "peekOfCode": "class MSDeformAttnPixelDecoder(BaseModule):\n    \"\"\"Pixel decoder with multi-scale deformable attention.\n    Args:\n        in_channels (list[int] | tuple[int]): Number of channels in the\n            input feature maps.\n        strides (list[int] | tuple[int]): Output strides of feature from\n            backbone.\n        feat_channels (int): Number of channels for feature.\n        out_channels (int): Number of channels for output.\n        num_outs (int): Number of output scales.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.plugins.msdeformattn_pixel_decoder",
        "documentation": {}
    },
    {
        "label": "EncoderDecoderMask2Former",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.segmentors.encoder_decoder_mask2former",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.segmentors.encoder_decoder_mask2former",
        "peekOfCode": "class EncoderDecoderMask2Former(BaseSegmentor):\n    \"\"\"Encoder Decoder segmentors.\n    EncoderDecoder typically consists of backbone, decode_head, auxiliary_head.\n    Note that auxiliary_head is only used for deep supervision during training,\n    which could be dumped during inference.\n    \"\"\"\n    def __init__(\n        self,\n        backbone,\n        decode_head,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.segmentors.encoder_decoder_mask2former",
        "documentation": {}
    },
    {
        "label": "AssignResult",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "peekOfCode": "class AssignResult(metaclass=ABCMeta):\n    \"\"\"Collection of assign results.\"\"\"\n    def __init__(self, num_gts, gt_inds, labels):\n        self.num_gts = num_gts\n        self.gt_inds = gt_inds\n        self.labels = labels\n    @property\n    def info(self):\n        info = {\n            \"num_gts\": self.num_gts,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "documentation": {}
    },
    {
        "label": "BaseAssigner",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "peekOfCode": "class BaseAssigner(metaclass=ABCMeta):\n    \"\"\"Base assigner that assigns boxes to ground truth boxes.\"\"\"\n    @abstractmethod\n    def assign(self, masks, gt_masks, gt_masks_ignore=None, gt_labels=None):\n        \"\"\"Assign boxes to either a ground truth boxes or a negative boxes.\"\"\"\n        pass\n@MASK_ASSIGNERS.register_module()\nclass MaskHungarianAssigner(BaseAssigner):\n    \"\"\"Computes one-to-one matching between predictions and ground truth for\n    mask.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "documentation": {}
    },
    {
        "label": "MaskHungarianAssigner",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "peekOfCode": "class MaskHungarianAssigner(BaseAssigner):\n    \"\"\"Computes one-to-one matching between predictions and ground truth for\n    mask.\n    This class computes an assignment between the targets and the predictions\n    based on the costs. The costs are weighted sum of three components:\n    classification cost, regression L1 cost and regression iou cost. The\n    targets don't include the no_object, so generally there are more\n    predictions than targets. After the one-to-one matching, the un-matched\n    are treated as backgrounds. Thus each query prediction will be assigned\n    with `0` or a positive integer indicating the ground truth index:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.assigner",
        "documentation": {}
    },
    {
        "label": "get_uncertainty",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "peekOfCode": "def get_uncertainty(mask_pred, labels):\n    \"\"\"Estimate uncertainty based on pred logits.\n    We estimate uncertainty as L1 distance between 0.0 and the logits\n    prediction in 'mask_pred' for the foreground class in `classes`.\n    Args:\n        mask_pred (Tensor): mask predication logits, shape (num_rois,\n            num_classes, mask_height, mask_width).\n        labels (list[Tensor]): Either predicted or ground truth label for\n            each predicted mask, of length num_rois.\n    Returns:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "documentation": {}
    },
    {
        "label": "get_uncertain_point_coords_with_randomness",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "peekOfCode": "def get_uncertain_point_coords_with_randomness(\n    mask_pred, labels, num_points, oversample_ratio, importance_sample_ratio\n):\n    \"\"\"Get ``num_points`` most uncertain points with random points during\n    train.\n    Sample points in [0, 1] x [0, 1] coordinate space based on their\n    uncertainty. The uncertainties are calculated for each point using\n    'get_uncertainty()' function that takes point's logit prediction as\n    input.\n    Args:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.point_sample",
        "documentation": {}
    },
    {
        "label": "SinePositionalEncoding",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "peekOfCode": "class SinePositionalEncoding(BaseModule):\n    \"\"\"Position encoding with sine and cosine functions.\n    See `End-to-End Object Detection with Transformers\n    <https://arxiv.org/pdf/2005.12872>`_ for details.\n    Args:\n        num_feats (int): The feature dimension for each position\n            along x-axis or y-axis. Note the final returned dimension\n            for each position is 2 times of this value.\n        temperature (int, optional): The temperature used for scaling\n            the position embedding. Defaults to 10000.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "documentation": {}
    },
    {
        "label": "LearnedPositionalEncoding",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "peekOfCode": "class LearnedPositionalEncoding(BaseModule):\n    \"\"\"Position embedding with learnable embedding weights.\n    Args:\n        num_feats (int): The feature dimension for each position\n            along x-axis or y-axis. The final returned dimension for\n            each position is 2 times of this value.\n        row_num_embed (int, optional): The dictionary size of row embeddings.\n            Default 50.\n        col_num_embed (int, optional): The dictionary size of col embeddings.\n            Default 50.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.positional_encoding",
        "documentation": {}
    },
    {
        "label": "AdaptivePadding",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class AdaptivePadding(nn.Module):\n    \"\"\"Applies padding to input (if needed) so that input can get fully covered\n    by filter you specified. It support two modes \"same\" and \"corner\". The\n    \"same\" mode is same with \"SAME\" padding mode in TensorFlow, pad zero around\n    input. The \"corner\"  mode would pad zero to bottom right.\n    Args:\n        kernel_size (int | tuple): Size of the kernel:\n        stride (int | tuple): Stride of the filter. Default: 1:\n        dilation (int | tuple): Spacing between kernel elements.\n            Default: 1",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class PatchMerging(BaseModule):\n    \"\"\"Merge patch feature map.\n    This layer groups feature map by kernel_size, and applies norm and linear\n    layers to the grouped feature map. Our implementation uses `nn.Unfold` to\n    merge patch, which is about 25% faster than original implementation.\n    Instead, we need to modify pretrained models for compatibility.\n    Args:\n        in_channels (int): The num of input channels.\n            to gets fully covered by filter and stride you specified..\n            Default: True.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "FFN",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class FFN(BaseModule):\n    \"\"\"Implements feed-forward networks (FFNs) with identity connection.\n    Args:\n        embed_dims (int): The feature dimension. Same as\n            `MultiheadAttention`. Defaults: 256.\n        feedforward_channels (int): The hidden dimension of FFNs.\n            Defaults: 1024.\n        num_fcs (int, optional): The number of fully-connected layers in\n            FFNs. Default: 2.\n        act_cfg (dict, optional): The activation config for FFNs.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DetrTransformerDecoderLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DetrTransformerDecoderLayer(BaseTransformerLayer):\n    \"\"\"Implements decoder layer in DETR transformer.\n    Args:\n        attn_cfgs (list[`mmcv.ConfigDict`] | list[dict] | dict )):\n            Configs for self_attention or cross_attention, the order\n            should be consistent with it in `operation_order`. If it is\n            a dict, it would be expand to the number of attention in\n            `operation_order`.\n        feedforward_channels (int): The hidden dimension for FFNs.\n        ffn_dropout (float): Probability of an element to be zeroed",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DetrTransformerEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DetrTransformerEncoder(TransformerLayerSequence):\n    \"\"\"TransformerEncoder of DETR.\n    Args:\n        post_norm_cfg (dict): Config of last normalization layer. Default\n            `LN`. Only used when `self.pre_norm` is `True`\n    \"\"\"\n    def __init__(self, *args, post_norm_cfg=dict(type=\"LN\"), **kwargs):\n        super(DetrTransformerEncoder, self).__init__(*args, **kwargs)\n        if post_norm_cfg is not None:\n            self.post_norm = build_norm_layer(post_norm_cfg, self.embed_dims)[1] if self.pre_norm else None",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DetrTransformerDecoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DetrTransformerDecoder(TransformerLayerSequence):\n    \"\"\"Implements the decoder in DETR transformer.\n    Args:\n        return_intermediate (bool): Whether to return intermediate outputs.\n        post_norm_cfg (dict): Config of last normalization layer. Default\n            `LN`.\n    \"\"\"\n    def __init__(self, *args, post_norm_cfg=dict(type=\"LN\"), return_intermediate=False, **kwargs):\n        super(DetrTransformerDecoder, self).__init__(*args, **kwargs)\n        self.return_intermediate = return_intermediate",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class Transformer(BaseModule):\n    \"\"\"Implements the DETR transformer.\n    Following the official DETR implementation, this module copy-paste\n    from torch.nn.Transformer with modifications:\n        * positional encodings are passed in MultiheadAttention\n        * extra LN at the end of encoder is removed\n        * decoder returns a stack of activations from all decoding layers\n    See `paper: End-to-End Object Detection with Transformers\n    <https://arxiv.org/pdf/2005.12872>`_ for details.\n    Args:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DeformableDetrTransformerDecoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DeformableDetrTransformerDecoder(TransformerLayerSequence):\n    \"\"\"Implements the decoder in DETR transformer.\n    Args:\n        return_intermediate (bool): Whether to return intermediate outputs.\n        coder_norm_cfg (dict): Config of last normalization layer. Default\n            `LN`.\n    \"\"\"\n    def __init__(self, *args, return_intermediate=False, **kwargs):\n        super(DeformableDetrTransformerDecoder, self).__init__(*args, **kwargs)\n        self.return_intermediate = return_intermediate",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DeformableDetrTransformer",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DeformableDetrTransformer(Transformer):\n    \"\"\"Implements the DeformableDETR transformer.\n    Args:\n        as_two_stage (bool): Generate query from encoder features.\n            Default: False.\n        num_feature_levels (int): Number of feature maps from FPN:\n            Default: 4.\n        two_stage_num_proposals (int): Number of proposals when set\n            `as_two_stage` as True. Default: 300.\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "DynamicConv",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "class DynamicConv(BaseModule):\n    \"\"\"Implements Dynamic Convolution.\n    This module generate parameters for each sample and\n    use bmm to implement 1*1 convolution. Code is modified\n    from the `official github repo <https://github.com/PeizeSun/\n    SparseR-CNN/blob/main/projects/SparseRCNN/sparsercnn/head.py#L258>`_ .\n    Args:\n        in_channels (int): The input feature channel.\n            Defaults to 256.\n        feat_channels (int): The inner feature channel.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "inverse_sigmoid",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "peekOfCode": "def inverse_sigmoid(x, eps=1e-5):\n    \"\"\"Inverse function of sigmoid.\n    Args:\n        x (Tensor): The tensor to do the\n            inverse.\n        eps (float): EPS avoid numerical\n            overflow. Defaults 1e-5.\n    Returns:\n        Tensor: The x has passed the inverse\n            function of sigmoid, has same",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.utils.transformer",
        "documentation": {}
    },
    {
        "label": "build_match_cost",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "def build_match_cost(cfg):\n    \"\"\"Build Match Cost.\"\"\"\n    return MATCH_COST.build(cfg)\ndef build_assigner(cfg):\n    \"\"\"Build Assigner.\"\"\"\n    return MASK_ASSIGNERS.build(cfg)\ndef build_transformer(cfg):\n    \"\"\"Build Transformer.\"\"\"\n    return TRANSFORMER.build(cfg)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "build_assigner",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "def build_assigner(cfg):\n    \"\"\"Build Assigner.\"\"\"\n    return MASK_ASSIGNERS.build(cfg)\ndef build_transformer(cfg):\n    \"\"\"Build Transformer.\"\"\"\n    return TRANSFORMER.build(cfg)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "build_transformer",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "def build_transformer(cfg):\n    \"\"\"Build Transformer.\"\"\"\n    return TRANSFORMER.build(cfg)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "TRANSFORMER",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "TRANSFORMER = Registry(\"Transformer\")\nMASK_ASSIGNERS = Registry(\"mask_assigner\")\nMATCH_COST = Registry(\"match_cost\")\ndef build_match_cost(cfg):\n    \"\"\"Build Match Cost.\"\"\"\n    return MATCH_COST.build(cfg)\ndef build_assigner(cfg):\n    \"\"\"Build Assigner.\"\"\"\n    return MASK_ASSIGNERS.build(cfg)\ndef build_transformer(cfg):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "MASK_ASSIGNERS",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "MASK_ASSIGNERS = Registry(\"mask_assigner\")\nMATCH_COST = Registry(\"match_cost\")\ndef build_match_cost(cfg):\n    \"\"\"Build Match Cost.\"\"\"\n    return MATCH_COST.build(cfg)\ndef build_assigner(cfg):\n    \"\"\"Build Assigner.\"\"\"\n    return MASK_ASSIGNERS.build(cfg)\ndef build_transformer(cfg):\n    \"\"\"Build Transformer.\"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "MATCH_COST",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "peekOfCode": "MATCH_COST = Registry(\"match_cost\")\ndef build_match_cost(cfg):\n    \"\"\"Build Match Cost.\"\"\"\n    return MATCH_COST.build(cfg)\ndef build_assigner(cfg):\n    \"\"\"Build Assigner.\"\"\"\n    return MASK_ASSIGNERS.build(cfg)\ndef build_transformer(cfg):\n    \"\"\"Build Transformer.\"\"\"\n    return TRANSFORMER.build(cfg)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.models.builder",
        "documentation": {}
    },
    {
        "label": "MSDeformAttnFunction",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "peekOfCode": "class MSDeformAttnFunction(Function):\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float32)\n    def forward(\n        ctx, value, value_spatial_shapes, value_level_start_index, sampling_locations, attention_weights, im2col_step\n    ):\n        output = ms_deform_attn_core_pytorch(\n            value,\n            value_spatial_shapes,\n            #  value_level_start_index,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "MSDeformAttn",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "peekOfCode": "class MSDeformAttn(nn.Module):\n    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4, ratio=1.0):\n        \"\"\"Multi-Scale Deformable Attention Module.\n        :param d_model      hidden dimension\n        :param n_levels     number of feature levels\n        :param n_heads      number of attention heads\n        :param n_points     number of sampling points per attention head per feature level\n        \"\"\"\n        super().__init__()\n        if d_model % n_heads != 0:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "ms_deform_attn_core_pytorch",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "peekOfCode": "def ms_deform_attn_core_pytorch(value, value_spatial_shapes, sampling_locations, attention_weights):\n    # for debug and test only,\n    # need to use cuda version instead\n    N_, S_, M_, D_ = value.shape\n    _, Lq_, M_, L_, P_, _ = sampling_locations.shape\n    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)\n    sampling_grids = 2 * sampling_locations - 1\n    sampling_value_list = []\n    for lid_, (H_, W_) in enumerate(value_spatial_shapes):\n        # N_, H_*W_, M_, D_ -> N_, H_*W_, M_*D_ -> N_, M_*D_, H_*W_ -> N_*M_, D_, H_, W_",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.segmentation_m2f.ops.modules.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "KnnModule",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "class KnnModule(torch.nn.Module):\n    \"\"\"\n    Gets knn of test features from all processes on a chunk of the train features\n    Each rank gets a chunk of the train features as well as a chunk of the test features.\n    In `compute_neighbors`, for each rank one after the other, its chunk of test features\n    is sent to all devices, partial knns are computed with each chunk of train features\n    then collated back on the original device.\n    \"\"\"\n    def __init__(self, train_features, train_labels, nb_knn, T, device, num_classes=1000):\n        super().__init__()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "DictKeysModule",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "class DictKeysModule(torch.nn.Module):\n    def __init__(self, keys):\n        super().__init__()\n        self.keys = keys\n    def forward(self, features_dict, targets):\n        for k in self.keys:\n            features_dict = features_dict[k]\n        return {\"preds\": features_dict, \"target\": targets}\ndef create_module_dict(*, module, n_per_class_list, n_tries, nb_knn, train_features, train_labels):\n    modules = {}",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "ModuleDictWithForward",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "class ModuleDictWithForward(torch.nn.ModuleDict):\n    def forward(self, *args, **kwargs):\n        return {k: module(*args, **kwargs) for k, module in self._modules.items()}\ndef eval_knn(\n    model,\n    train_dataset,\n    val_dataset,\n    accuracy_averaging,\n    nb_knn,\n    temperature,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(\n        description=description,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "create_module_dict",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def create_module_dict(*, module, n_per_class_list, n_tries, nb_knn, train_features, train_labels):\n    modules = {}\n    mapping = create_class_indices_mapping(train_labels)\n    for npc in n_per_class_list:\n        if npc < 0:  # Only one try needed when using the full data\n            full_module = module(\n                train_features=train_features,\n                train_labels=train_labels,\n                nb_knn=nb_knn,\n            )",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "filter_train",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def filter_train(mapping, n_per_class, seed):\n    torch.manual_seed(seed)\n    final_indices = []\n    for k in mapping.keys():\n        index = torch.randperm(len(mapping[k]))[:n_per_class]\n        final_indices.append(mapping[k][index])\n    return torch.cat(final_indices).squeeze()\ndef create_class_indices_mapping(labels):\n    unique_labels, inverse = torch.unique(labels, return_inverse=True)\n    mapping = {unique_labels[i]: (inverse == i).nonzero() for i in range(len(unique_labels))}",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "create_class_indices_mapping",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def create_class_indices_mapping(labels):\n    unique_labels, inverse = torch.unique(labels, return_inverse=True)\n    mapping = {unique_labels[i]: (inverse == i).nonzero() for i in range(len(unique_labels))}\n    return mapping\nclass ModuleDictWithForward(torch.nn.ModuleDict):\n    def forward(self, *args, **kwargs):\n        return {k: module(*args, **kwargs) for k, module in self._modules.items()}\ndef eval_knn(\n    model,\n    train_dataset,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "eval_knn",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def eval_knn(\n    model,\n    train_dataset,\n    val_dataset,\n    accuracy_averaging,\n    nb_knn,\n    temperature,\n    batch_size,\n    num_workers,\n    gather_on_cpu,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "eval_knn_with_model",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def eval_knn_with_model(\n    model,\n    output_dir,\n    train_dataset_str=\"ImageNet:split=TRAIN\",\n    val_dataset_str=\"ImageNet:split=VAL\",\n    nb_knn=(10, 20, 100, 200),\n    temperature=0.07,\n    autocast_dtype=torch.float,\n    accuracy_averaging=AccuracyAveraging.MEAN_ACCURACY,\n    transform=None,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "def main(args):\n    model, autocast_dtype = setup_and_build_model(args)\n    eval_knn_with_model(\n        model=model,\n        output_dir=args.output_dir,\n        train_dataset_str=args.train_dataset_str,\n        val_dataset_str=args.val_dataset_str,\n        nb_knn=args.nb_knn,\n        temperature=args.temperature,\n        autocast_dtype=autocast_dtype,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.knn",
        "documentation": {}
    },
    {
        "label": "LinearClassifier",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "class LinearClassifier(nn.Module):\n    \"\"\"Linear layer to train on top of frozen features\"\"\"\n    def __init__(self, out_dim, use_n_blocks, use_avgpool, num_classes=1000):\n        super().__init__()\n        self.out_dim = out_dim\n        self.use_n_blocks = use_n_blocks\n        self.use_avgpool = use_avgpool\n        self.num_classes = num_classes\n        self.linear = nn.Linear(out_dim, num_classes)\n        self.linear.weight.data.normal_(mean=0.0, std=0.01)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "AllClassifiers",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "class AllClassifiers(nn.Module):\n    def __init__(self, classifiers_dict):\n        super().__init__()\n        self.classifiers_dict = nn.ModuleDict()\n        self.classifiers_dict.update(classifiers_dict)\n    def forward(self, inputs):\n        return {k: v.forward(inputs) for k, v in self.classifiers_dict.items()}\n    def __len__(self):\n        return len(self.classifiers_dict)\nclass LinearPostprocessor(nn.Module):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "LinearPostprocessor",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "class LinearPostprocessor(nn.Module):\n    def __init__(self, linear_classifier, class_mapping=None):\n        super().__init__()\n        self.linear_classifier = linear_classifier\n        self.register_buffer(\"class_mapping\", None if class_mapping is None else torch.LongTensor(class_mapping))\n    def forward(self, samples, targets):\n        preds = self.linear_classifier(samples)\n        return {\n            \"preds\": preds[:, self.class_mapping] if self.class_mapping is not None else preds,\n            \"target\": targets,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(\n        description=description,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "has_ddp_wrapper",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def has_ddp_wrapper(m: nn.Module) -> bool:\n    return isinstance(m, DistributedDataParallel)\ndef remove_ddp_wrapper(m: nn.Module) -> nn.Module:\n    return m.module if has_ddp_wrapper(m) else m\ndef _pad_and_collate(batch):\n    maxlen = max(len(targets) for image, targets in batch)\n    padded_batch = [\n        (image, np.pad(targets, (0, maxlen - len(targets)), constant_values=-1)) for image, targets in batch\n    ]\n    return torch.utils.data.default_collate(padded_batch)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "remove_ddp_wrapper",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def remove_ddp_wrapper(m: nn.Module) -> nn.Module:\n    return m.module if has_ddp_wrapper(m) else m\ndef _pad_and_collate(batch):\n    maxlen = max(len(targets) for image, targets in batch)\n    padded_batch = [\n        (image, np.pad(targets, (0, maxlen - len(targets)), constant_values=-1)) for image, targets in batch\n    ]\n    return torch.utils.data.default_collate(padded_batch)\ndef create_linear_input(x_tokens_list, use_n_blocks, use_avgpool):\n    intermediate_output = x_tokens_list[-use_n_blocks:]",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "create_linear_input",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def create_linear_input(x_tokens_list, use_n_blocks, use_avgpool):\n    intermediate_output = x_tokens_list[-use_n_blocks:]\n    output = torch.cat([class_token for _, class_token in intermediate_output], dim=-1)\n    if use_avgpool:\n        output = torch.cat(\n            (\n                output,\n                torch.mean(intermediate_output[-1][0], dim=1),  # patch tokens\n            ),\n            dim=-1,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "scale_lr",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def scale_lr(learning_rates, batch_size):\n    return learning_rates * (batch_size * distributed.get_global_size()) / 256.0\ndef setup_linear_classifiers(sample_output, n_last_blocks_list, learning_rates, batch_size, num_classes=1000):\n    linear_classifiers_dict = nn.ModuleDict()\n    optim_param_groups = []\n    for n in n_last_blocks_list:\n        for avgpool in [False, True]:\n            for _lr in learning_rates:\n                lr = scale_lr(_lr, batch_size)\n                out_dim = create_linear_input(sample_output, use_n_blocks=n, use_avgpool=avgpool).shape[1]",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "setup_linear_classifiers",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def setup_linear_classifiers(sample_output, n_last_blocks_list, learning_rates, batch_size, num_classes=1000):\n    linear_classifiers_dict = nn.ModuleDict()\n    optim_param_groups = []\n    for n in n_last_blocks_list:\n        for avgpool in [False, True]:\n            for _lr in learning_rates:\n                lr = scale_lr(_lr, batch_size)\n                out_dim = create_linear_input(sample_output, use_n_blocks=n, use_avgpool=avgpool).shape[1]\n                linear_classifier = LinearClassifier(\n                    out_dim, use_n_blocks=n, use_avgpool=avgpool, num_classes=num_classes",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "evaluate_linear_classifiers",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def evaluate_linear_classifiers(\n    feature_model,\n    linear_classifiers,\n    data_loader,\n    metric_type,\n    metrics_file_path,\n    training_num_classes,\n    iteration,\n    prefixstring=\"\",\n    class_mapping=None,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "eval_linear",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def eval_linear(\n    *,\n    feature_model,\n    linear_classifiers,\n    train_data_loader,\n    val_data_loader,\n    metrics_file_path,\n    optimizer,\n    scheduler,\n    output_dir,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "make_eval_data_loader",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def make_eval_data_loader(test_dataset_str, batch_size, num_workers, metric_type):\n    test_dataset = make_dataset(\n        dataset_str=test_dataset_str,\n        transform=make_classification_eval_transform(),\n    )\n    test_data_loader = make_data_loader(\n        dataset=test_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        sampler_type=SamplerType.DISTRIBUTED,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "test_on_datasets",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def test_on_datasets(\n    feature_model,\n    linear_classifiers,\n    test_dataset_strs,\n    batch_size,\n    num_workers,\n    test_metric_types,\n    metrics_file_path,\n    training_num_classes,\n    iteration,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "run_eval_linear",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def run_eval_linear(\n    model,\n    output_dir,\n    train_dataset_str,\n    val_dataset_str,\n    batch_size,\n    epochs,\n    epoch_length,\n    num_workers,\n    save_checkpoint_frequency,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "def main(args):\n    model, autocast_dtype = setup_and_build_model(args)\n    run_eval_linear(\n        model=model,\n        output_dir=args.output_dir,\n        train_dataset_str=args.train_dataset_str,\n        val_dataset_str=args.val_dataset_str,\n        test_dataset_strs=args.test_dataset_strs,\n        batch_size=args.batch_size,\n        epochs=args.epochs,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.linear",
        "documentation": {}
    },
    {
        "label": "LogRegModule",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "class LogRegModule(nn.Module):\n    def __init__(\n        self,\n        C,\n        max_iter=DEFAULT_MAX_ITER,\n        dtype=torch.float64,\n        device=_CPU_DEVICE,\n    ):\n        super().__init__()\n        self.dtype = dtype",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(\n        description=description,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "evaluate_model",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def evaluate_model(*, logreg_model, logreg_metric, test_data_loader, device):\n    postprocessors = {\"metrics\": logreg_model}\n    metrics = {\"metrics\": logreg_metric}\n    return evaluate(nn.Identity(), test_data_loader, postprocessors, metrics, device)\ndef train_for_C(*, C, max_iter, train_features, train_labels, dtype=torch.float64, device=_CPU_DEVICE):\n    logreg_model = LogRegModule(C, max_iter=max_iter, dtype=dtype, device=device)\n    logreg_model.fit(train_features, train_labels)\n    return logreg_model\ndef train_and_evaluate(\n    *,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "train_for_C",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def train_for_C(*, C, max_iter, train_features, train_labels, dtype=torch.float64, device=_CPU_DEVICE):\n    logreg_model = LogRegModule(C, max_iter=max_iter, dtype=dtype, device=device)\n    logreg_model.fit(train_features, train_labels)\n    return logreg_model\ndef train_and_evaluate(\n    *,\n    C,\n    max_iter,\n    train_features,\n    train_labels,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "train_and_evaluate",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def train_and_evaluate(\n    *,\n    C,\n    max_iter,\n    train_features,\n    train_labels,\n    logreg_metric,\n    test_data_loader,\n    train_dtype=torch.float64,\n    train_features_device,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "sweep_C_values",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def sweep_C_values(\n    *,\n    train_features,\n    train_labels,\n    test_data_loader,\n    metric_type,\n    num_classes,\n    train_dtype=torch.float64,\n    train_features_device=_CPU_DEVICE,\n    max_train_iters=DEFAULT_MAX_ITER,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "eval_log_regression",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def eval_log_regression(\n    *,\n    model,\n    train_dataset,\n    val_dataset,\n    finetune_dataset,\n    metric_type,\n    batch_size,\n    num_workers,\n    finetune_on_val=False,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "eval_log_regression_with_model",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def eval_log_regression_with_model(\n    model,\n    train_dataset_str=\"ImageNet:split=TRAIN\",\n    val_dataset_str=\"ImageNet:split=VAL\",\n    finetune_dataset_str=None,\n    autocast_dtype=torch.float,\n    finetune_on_val=False,\n    metric_type=MetricType.MEAN_ACCURACY,\n    train_dtype=torch.float64,\n    train_features_device=_CPU_DEVICE,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "def main(args):\n    model, autocast_dtype = setup_and_build_model(args)\n    eval_log_regression_with_model(\n        model=model,\n        train_dataset_str=args.train_dataset_str,\n        val_dataset_str=args.val_dataset_str,\n        finetune_dataset_str=args.finetune_dataset_str,\n        autocast_dtype=autocast_dtype,\n        finetune_on_val=args.finetune_on_val,\n        metric_type=args.metric_type,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nDEFAULT_MAX_ITER = 1_000\nC_POWER_RANGE = torch.linspace(-6, 5, 45)\n_CPU_DEVICE = torch.device(\"cpu\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MAX_ITER",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "DEFAULT_MAX_ITER = 1_000\nC_POWER_RANGE = torch.linspace(-6, 5, 45)\n_CPU_DEVICE = torch.device(\"cpu\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "C_POWER_RANGE",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "C_POWER_RANGE = torch.linspace(-6, 5, 45)\n_CPU_DEVICE = torch.device(\"cpu\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "_CPU_DEVICE",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "peekOfCode": "_CPU_DEVICE = torch.device(\"cpu\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parents = parents or []\n    setup_args_parser = get_setup_args_parser(parents=parents, add_help=False)\n    parents = [setup_args_parser]\n    parser = argparse.ArgumentParser(",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "MetricType",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "class MetricType(Enum):\n    MEAN_ACCURACY = \"mean_accuracy\"\n    MEAN_PER_CLASS_ACCURACY = \"mean_per_class_accuracy\"\n    PER_CLASS_ACCURACY = \"per_class_accuracy\"\n    IMAGENET_REAL_ACCURACY = \"imagenet_real_accuracy\"\n    @property\n    def accuracy_averaging(self):\n        return getattr(AccuracyAveraging, self.name, None)\n    def __str__(self):\n        return self.value",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "AccuracyAveraging",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "class AccuracyAveraging(Enum):\n    MEAN_ACCURACY = \"micro\"\n    MEAN_PER_CLASS_ACCURACY = \"macro\"\n    PER_CLASS_ACCURACY = \"none\"\n    def __str__(self):\n        return self.value\ndef build_metric(metric_type: MetricType, *, num_classes: int, ks: Optional[tuple] = None):\n    if metric_type.accuracy_averaging is not None:\n        return build_topk_accuracy_metric(\n            average_type=metric_type.accuracy_averaging,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "ImageNetReaLAccuracy",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "class ImageNetReaLAccuracy(Metric):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = None\n    full_state_update: bool = False\n    def __init__(\n        self,\n        num_classes: int,\n        top_k: int = 1,\n        **kwargs: Any,\n    ) -> None:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_metric",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "def build_metric(metric_type: MetricType, *, num_classes: int, ks: Optional[tuple] = None):\n    if metric_type.accuracy_averaging is not None:\n        return build_topk_accuracy_metric(\n            average_type=metric_type.accuracy_averaging,\n            num_classes=num_classes,\n            ks=(1, 5) if ks is None else ks,\n        )\n    elif metric_type == MetricType.IMAGENET_REAL_ACCURACY:\n        return build_topk_imagenet_real_accuracy_metric(\n            num_classes=num_classes,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_topk_accuracy_metric",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "def build_topk_accuracy_metric(average_type: AccuracyAveraging, num_classes: int, ks: tuple = (1, 5)):\n    metrics: Dict[str, Metric] = {\n        f\"top-{k}\": MulticlassAccuracy(top_k=k, num_classes=int(num_classes), average=average_type.value) for k in ks\n    }\n    return MetricCollection(metrics)\ndef build_topk_imagenet_real_accuracy_metric(num_classes: int, ks: tuple = (1, 5)):\n    metrics: Dict[str, Metric] = {f\"top-{k}\": ImageNetReaLAccuracy(top_k=k, num_classes=int(num_classes)) for k in ks}\n    return MetricCollection(metrics)\nclass ImageNetReaLAccuracy(Metric):\n    is_differentiable: bool = False",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "build_topk_imagenet_real_accuracy_metric",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "def build_topk_imagenet_real_accuracy_metric(num_classes: int, ks: tuple = (1, 5)):\n    metrics: Dict[str, Metric] = {f\"top-{k}\": ImageNetReaLAccuracy(top_k=k, num_classes=int(num_classes)) for k in ks}\n    return MetricCollection(metrics)\nclass ImageNetReaLAccuracy(Metric):\n    is_differentiable: bool = False\n    higher_is_better: Optional[bool] = None\n    full_state_update: bool = False\n    def __init__(\n        self,\n        num_classes: int,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass MetricType(Enum):\n    MEAN_ACCURACY = \"mean_accuracy\"\n    MEAN_PER_CLASS_ACCURACY = \"mean_per_class_accuracy\"\n    PER_CLASS_ACCURACY = \"per_class_accuracy\"\n    IMAGENET_REAL_ACCURACY = \"imagenet_real_accuracy\"\n    @property\n    def accuracy_averaging(self):\n        return getattr(AccuracyAveraging, self.name, None)\n    def __str__(self):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.metrics",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "peekOfCode": "def get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n):\n    parser = argparse.ArgumentParser(\n        description=description,\n        parents=parents or [],\n        add_help=add_help,\n    )",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "get_autocast_dtype",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "peekOfCode": "def get_autocast_dtype(config):\n    teacher_dtype_str = config.compute_precision.teacher.backbone.mixed_precision.param_dtype\n    if teacher_dtype_str == \"fp16\":\n        return torch.half\n    elif teacher_dtype_str == \"bf16\":\n        return torch.bfloat16\n    else:\n        return torch.float\ndef build_model_for_eval(config, pretrained_weights):\n    model, _ = build_model_from_cfg(config, only_teacher=True)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "build_model_for_eval",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "peekOfCode": "def build_model_for_eval(config, pretrained_weights):\n    model, _ = build_model_from_cfg(config, only_teacher=True)\n    dinov2_utils.load_pretrained_weights(model, pretrained_weights, \"teacher\")\n    model.eval()\n    model.cuda()\n    return model\ndef setup_and_build_model(args) -> Tuple[Any, torch.dtype]:\n    cudnn.benchmark = True\n    config = setup(args)\n    model = build_model_for_eval(config, args.pretrained_weights)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "setup_and_build_model",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "peekOfCode": "def setup_and_build_model(args) -> Tuple[Any, torch.dtype]:\n    cudnn.benchmark = True\n    config = setup(args)\n    model = build_model_for_eval(config, args.pretrained_weights)\n    autocast_dtype = get_autocast_dtype(config)\n    return model, autocast_dtype",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.setup",
        "documentation": {}
    },
    {
        "label": "ModelWithNormalize",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "class ModelWithNormalize(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n    def forward(self, samples):\n        return nn.functional.normalize(self.model(samples), dim=1, p=2)\nclass ModelWithIntermediateLayers(nn.Module):\n    def __init__(self, feature_model, n_last_blocks, autocast_ctx):\n        super().__init__()\n        self.feature_model = feature_model",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "ModelWithIntermediateLayers",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "class ModelWithIntermediateLayers(nn.Module):\n    def __init__(self, feature_model, n_last_blocks, autocast_ctx):\n        super().__init__()\n        self.feature_model = feature_model\n        self.feature_model.eval()\n        self.n_last_blocks = n_last_blocks\n        self.autocast_ctx = autocast_ctx\n    def forward(self, images):\n        with torch.inference_mode():\n            with self.autocast_ctx():",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "def evaluate(\n    model: nn.Module,\n    data_loader,\n    postprocessors: Dict[str, nn.Module],\n    metrics: Dict[str, MetricCollection],\n    device: torch.device,\n    criterion: Optional[nn.Module] = None,\n):\n    model.eval()\n    if criterion is not None:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "all_gather_and_flatten",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "def all_gather_and_flatten(tensor_rank):\n    tensor_all_ranks = torch.empty(\n        distributed.get_global_size(),\n        *tensor_rank.shape,\n        dtype=tensor_rank.dtype,\n        device=tensor_rank.device,\n    )\n    tensor_list = list(tensor_all_ranks.unbind(0))\n    torch.distributed.all_gather(tensor_list, tensor_rank.contiguous())\n    return tensor_all_ranks.flatten(end_dim=1)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "def extract_features(model, dataset, batch_size, num_workers, gather_on_cpu=False):\n    dataset_with_enumerated_targets = DatasetWithEnumeratedTargets(dataset)\n    sample_count = len(dataset_with_enumerated_targets)\n    data_loader = make_data_loader(\n        dataset=dataset_with_enumerated_targets,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        sampler_type=SamplerType.DISTRIBUTED,\n        drop_last=False,\n        shuffle=False,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "extract_features_with_dataloader",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "def extract_features_with_dataloader(model, data_loader, sample_count, gather_on_cpu=False):\n    gather_device = torch.device(\"cpu\") if gather_on_cpu else torch.device(\"cuda\")\n    metric_logger = MetricLogger(delimiter=\"  \")\n    features, all_labels = None, None\n    for samples, (index, labels_rank) in metric_logger.log_every(data_loader, 10):\n        samples = samples.cuda(non_blocking=True)\n        labels_rank = labels_rank.cuda(non_blocking=True)\n        index = index.cuda(non_blocking=True)\n        features_rank = model(samples).float()\n        # init storage feature matrix",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass ModelWithNormalize(torch.nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n    def forward(self, samples):\n        return nn.functional.normalize(self.model(samples), dim=1, p=2)\nclass ModelWithIntermediateLayers(nn.Module):\n    def __init__(self, feature_model, n_last_blocks, autocast_ctx):\n        super().__init__()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.eval.utils",
        "documentation": {}
    },
    {
        "label": "DepthBaseDecodeHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class DepthBaseDecodeHead(nn.Module):\n    \"\"\"Base class for BaseDecodeHead.\n    Args:\n        in_channels (List): Input channels.\n        channels (int): Channels after modules, before conv_depth.\n        conv_layer (nn.Module): Conv layers. Default: None.\n        act_layer (nn.Module): Activation layers. Default: nn.ReLU.\n        loss_decode (dict): Config of decode loss.\n            Default: ().\n        sampler (dict|None): The config of depth map sampler.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "BNHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class BNHead(DepthBaseDecodeHead):\n    \"\"\"Just a batchnorm.\"\"\"\n    def __init__(self, input_transform=\"resize_concat\", in_index=(0, 1, 2, 3), upsample=1, **kwargs):\n        super().__init__(**kwargs)\n        self.input_transform = input_transform\n        self.in_index = in_index\n        self.upsample = upsample\n        # self.bn = nn.SyncBatchNorm(self.in_channels)\n        if self.classify:\n            self.conv_depth = nn.Conv2d(self.channels, self.n_bins, kernel_size=1, padding=0, stride=1)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "ConvModule",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class ConvModule(nn.Module):\n    \"\"\"A conv block that bundles conv/norm/activation layers.\n    This block simplifies the usage of convolution layers, which are commonly\n    used with a norm layer (e.g., BatchNorm) and activation layer (e.g., ReLU).\n    It is based upon three build methods: `build_conv_layer()`,\n    `build_norm_layer()` and `build_activation_layer()`.\n    Besides, we add some additional features in this module.\n    1. Automatically set `bias` of the conv layer.\n    2. Spectral norm is supported.\n    3. More padding modes are supported. Before PyTorch 1.5, nn.Conv2d only",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "Interpolate",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class Interpolate(nn.Module):\n    def __init__(self, scale_factor, mode, align_corners=False):\n        super(Interpolate, self).__init__()\n        self.interp = nn.functional.interpolate\n        self.scale_factor = scale_factor\n        self.mode = mode\n        self.align_corners = align_corners\n    def forward(self, x):\n        x = self.interp(x, scale_factor=self.scale_factor, mode=self.mode, align_corners=self.align_corners)\n        return x",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "HeadDepth",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class HeadDepth(nn.Module):\n    def __init__(self, features):\n        super(HeadDepth, self).__init__()\n        self.head = nn.Sequential(\n            nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n            Interpolate(scale_factor=2, mode=\"bilinear\", align_corners=True),\n            nn.Conv2d(features // 2, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n        )",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "ReassembleBlocks",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class ReassembleBlocks(nn.Module):\n    \"\"\"ViTPostProcessBlock, process cls_token in ViT backbone output and\n    rearrange the feature vector to feature map.\n    Args:\n        in_channels (int): ViT feature channels. Default: 768.\n        out_channels (List): output channels of each stage.\n            Default: [96, 192, 384, 768].\n        readout_type (str): Type of readout operation. Default: 'ignore'.\n        patch_size (int): The patch size. Default: 16.\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "PreActResidualConvUnit",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class PreActResidualConvUnit(nn.Module):\n    \"\"\"ResidualConvUnit, pre-activate residual unit.\n    Args:\n        in_channels (int): number of channels in the input feature map.\n        act_layer (nn.Module): activation layer.\n        norm_layer (nn.Module): norm layer.\n        stride (int): stride of the first block. Default: 1\n        dilation (int): dilation rate for convs layers. Default: 1.\n    \"\"\"\n    def __init__(self, in_channels, act_layer, norm_layer, stride=1, dilation=1):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class FeatureFusionBlock(nn.Module):\n    \"\"\"FeatureFusionBlock, merge feature map from different stages.\n    Args:\n        in_channels (int): Input channels.\n        act_layer (nn.Module): activation layer for ResidualConvUnit.\n        norm_layer (nn.Module): normalization layer.\n        expand (bool): Whether expand the channels in post process block.\n            Default: False.\n        align_corners (bool): align_corner setting for bilinear upsample.\n            Default: True.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "DPTHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "peekOfCode": "class DPTHead(DepthBaseDecodeHead):\n    \"\"\"Vision Transformers for Dense Prediction.\n    This head is implemented of `DPT <https://arxiv.org/abs/2103.13413>`_.\n    Args:\n        embed_dims (int): The embed dimension of the ViT backbone.\n            Default: 768.\n        post_process_channels (List): Out channels of post process conv\n            layers. Default: [96, 192, 384, 768].\n        readout_type (str): Type of readout operation. Default: 'ignore'.\n        patch_size (int): The patch size. Default: 16.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.decode_heads",
        "documentation": {}
    },
    {
        "label": "DepthEncoderDecoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "peekOfCode": "class DepthEncoderDecoder(nn.Module):\n    \"\"\"Encoder Decoder depther.\n    EncoderDecoder typically consists of backbone and decode_head.\n    \"\"\"\n    def __init__(self, backbone, decode_head):\n        super(DepthEncoderDecoder, self).__init__()\n        self.backbone = backbone\n        self.decode_head = decode_head\n        self.align_corners = self.decode_head.align_corners\n    def extract_feat(self, img):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "documentation": {}
    },
    {
        "label": "add_prefix",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "peekOfCode": "def add_prefix(inputs, prefix):\n    \"\"\"Add prefix for dict.\n    Args:\n        inputs (dict): The input dict with str keys.\n        prefix (str): The prefix to add.\n    Returns:\n        dict: The dict with keys updated with ``prefix``.\n    \"\"\"\n    outputs = dict()\n    for name, value in inputs.items():",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.encoder_decoder",
        "documentation": {}
    },
    {
        "label": "resize",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.ops",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.ops",
        "peekOfCode": "def resize(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None, warning=False):\n    if warning:\n        if size is not None and align_corners:\n            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n            output_h, output_w = tuple(int(x) for x in size)\n            if output_h > input_h or output_w > output_h:\n                if (\n                    (output_h > 1 and output_w > 1 and input_h > 1 and input_w > 1)\n                    and (output_h - 1) % (input_h - 1)\n                    and (output_w - 1) % (input_w - 1)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depth.ops",
        "documentation": {}
    },
    {
        "label": "Weights",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "class Weights(Enum):\n    LVD142M = \"LVD142M\"\ndef _make_dinov2_model(\n    *,\n    arch_name: str = \"vit_large\",\n    img_size: int = 518,\n    patch_size: int = 14,\n    init_values: float = 1.0,\n    ffn_layer: str = \"mlp\",\n    block_chunks: int = 0,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vits14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-S/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(arch_name=\"vit_small\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitb14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-B/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(arch_name=\"vit_base\", pretrained=pretrained, weights=weights, **kwargs)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitb14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-B/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(arch_name=\"vit_base\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitl14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-L/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(arch_name=\"vit_large\", pretrained=pretrained, weights=weights, **kwargs)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitl14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-L/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(arch_name=\"vit_large\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitg14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-g/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitg14(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-g/14 model (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(\n        arch_name=\"vit_giant2\",\n        ffn_layer=\"swiglufused\",\n        weights=weights,\n        pretrained=pretrained,\n        **kwargs,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14_reg",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vits14_reg(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-S/14 model with registers (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(\n        arch_name=\"vit_small\",\n        pretrained=pretrained,\n        weights=weights,\n        num_register_tokens=4,\n        interpolate_antialias=True,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14_reg",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitb14_reg(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-B/14 model with registers (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(\n        arch_name=\"vit_base\",\n        pretrained=pretrained,\n        weights=weights,\n        num_register_tokens=4,\n        interpolate_antialias=True,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14_reg",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitl14_reg(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-L/14 model with registers (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(\n        arch_name=\"vit_large\",\n        pretrained=pretrained,\n        weights=weights,\n        num_register_tokens=4,\n        interpolate_antialias=True,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14_reg",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "peekOfCode": "def dinov2_vitg14_reg(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.LVD142M, **kwargs):\n    \"\"\"\n    DINOv2 ViT-g/14 model with registers (optionally) pretrained on the LVD-142M dataset.\n    \"\"\"\n    return _make_dinov2_model(\n        arch_name=\"vit_giant2\",\n        ffn_layer=\"swiglufused\",\n        weights=weights,\n        pretrained=pretrained,\n        num_register_tokens=4,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.backbones",
        "documentation": {}
    },
    {
        "label": "Weights",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "class Weights(Enum):\n    IMAGENET1K = \"IMAGENET1K\"\ndef _make_dinov2_linear_classification_head(\n    *,\n    arch_name: str = \"vit_large\",\n    patch_size: int = 14,\n    embed_dim: int = 1024,\n    layers: int = 4,\n    pretrained: bool = True,\n    weights: Union[Weights, str] = Weights.IMAGENET1K,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "_LinearClassifierWrapper",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "class _LinearClassifierWrapper(nn.Module):\n    def __init__(self, *, backbone: nn.Module, linear_head: nn.Module, layers: int = 4):\n        super().__init__()\n        self.backbone = backbone\n        self.linear_head = linear_head\n        self.layers = layers\n    def forward(self, x):\n        if self.layers == 1:\n            x = self.backbone.forward_features(x)\n            cls_token = x[\"x_norm_clstoken\"]",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14_lc",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vits14_lc(\n    *,\n    layers: int = 4,\n    pretrained: bool = True,\n    weights: Union[Weights, str] = Weights.IMAGENET1K,\n    **kwargs,\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-S/14 backbone (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14_lc",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitb14_lc(\n    *,\n    layers: int = 4,\n    pretrained: bool = True,\n    weights: Union[Weights, str] = Weights.IMAGENET1K,\n    **kwargs,\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-B/14 backbone (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14_lc",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitl14_lc(\n    *,\n    layers: int = 4,\n    pretrained: bool = True,\n    weights: Union[Weights, str] = Weights.IMAGENET1K,\n    **kwargs,\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-L/14 backbone (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14_lc",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitg14_lc(\n    *,\n    layers: int = 4,\n    pretrained: bool = True,\n    weights: Union[Weights, str] = Weights.IMAGENET1K,\n    **kwargs,\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-g/14 backbone (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14_reg_lc",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vits14_reg_lc(\n    *, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.IMAGENET1K, **kwargs\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-S/14 backbone with registers (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"\n    return _make_dinov2_linear_classifier(\n        arch_name=\"vit_small\",\n        layers=layers,\n        pretrained=pretrained,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14_reg_lc",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitb14_reg_lc(\n    *, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.IMAGENET1K, **kwargs\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-B/14 backbone with registers (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"\n    return _make_dinov2_linear_classifier(\n        arch_name=\"vit_base\",\n        layers=layers,\n        pretrained=pretrained,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14_reg_lc",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitl14_reg_lc(\n    *, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.IMAGENET1K, **kwargs\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-L/14 backbone with registers (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"\n    return _make_dinov2_linear_classifier(\n        arch_name=\"vit_large\",\n        layers=layers,\n        pretrained=pretrained,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14_reg_lc",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "peekOfCode": "def dinov2_vitg14_reg_lc(\n    *, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.IMAGENET1K, **kwargs\n):\n    \"\"\"\n    Linear classifier (1 or 4 layers) on top of a DINOv2 ViT-g/14 backbone with registers (optionally) pretrained on the LVD-142M dataset and trained on ImageNet-1k.\n    \"\"\"\n    return _make_dinov2_linear_classifier(\n        arch_name=\"vit_giant2\",\n        layers=layers,\n        ffn_layer=\"swiglufused\",",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.classifiers",
        "documentation": {}
    },
    {
        "label": "Weights",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "class Weights(Enum):\n    NYU = \"NYU\"\n    KITTI = \"KITTI\"\ndef _get_depth_range(pretrained: bool, weights: Weights = Weights.NYU) -> Tuple[float, float]:\n    if not pretrained:  # Default\n        return (0.001, 10.0)\n    # Pretrained, set according to the training dataset for the provided weights\n    if weights == Weights.KITTI:\n        return (0.001, 80.0)\n    if weights == Weights.NYU:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14_ld",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vits14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_small\", layers=layers, pretrained=pretrained, weights=weights, **kwargs\n    )\ndef dinov2_vitb14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_base\", layers=layers, pretrained=pretrained, weights=weights, **kwargs\n    )\ndef dinov2_vitl14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14_ld",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitb14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_base\", layers=layers, pretrained=pretrained, weights=weights, **kwargs\n    )\ndef dinov2_vitl14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_large\", layers=layers, pretrained=pretrained, weights=weights, **kwargs\n    )\ndef dinov2_vitg14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14_ld",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitl14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_large\", layers=layers, pretrained=pretrained, weights=weights, **kwargs\n    )\ndef dinov2_vitg14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_giant2\", layers=layers, ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )\ndef _make_dinov2_dpt_depth_head(*, embed_dim: int, min_depth: float, max_depth: float):\n    return DPTHead(",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14_ld",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitg14_ld(*, layers: int = 4, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_linear_depther(\n        arch_name=\"vit_giant2\", layers=layers, ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )\ndef _make_dinov2_dpt_depth_head(*, embed_dim: int, min_depth: float, max_depth: float):\n    return DPTHead(\n        in_channels=[embed_dim] * 4,\n        channels=256,\n        embed_dims=embed_dim,\n        post_process_channels=[embed_dim // 2 ** (3 - i) for i in range(4)],",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vits14_dd",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vits14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_small\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitb14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_base\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitl14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_large\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitg14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(\n        arch_name=\"vit_giant2\", ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitb14_dd",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitb14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_base\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitl14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_large\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitg14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(\n        arch_name=\"vit_giant2\", ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitl14_dd",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitl14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(arch_name=\"vit_large\", pretrained=pretrained, weights=weights, **kwargs)\ndef dinov2_vitg14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(\n        arch_name=\"vit_giant2\", ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "dinov2_vitg14_dd",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "peekOfCode": "def dinov2_vitg14_dd(*, pretrained: bool = True, weights: Union[Weights, str] = Weights.NYU, **kwargs):\n    return _make_dinov2_dpt_depther(\n        arch_name=\"vit_giant2\", ffn_layer=\"swiglufused\", pretrained=pretrained, weights=weights, **kwargs\n    )",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.depthers",
        "documentation": {}
    },
    {
        "label": "CenterPadding",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "peekOfCode": "class CenterPadding(nn.Module):\n    def __init__(self, multiple):\n        super().__init__()\n        self.multiple = multiple\n    def _get_pad(self, size):\n        new_size = math.ceil(size / self.multiple) * self.multiple\n        pad_size = new_size - size\n        pad_size_left = pad_size // 2\n        pad_size_right = pad_size - pad_size_left\n        return pad_size_left, pad_size_right",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "documentation": {}
    },
    {
        "label": "_DINOV2_BASE_URL",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "peekOfCode": "_DINOV2_BASE_URL = \"https://dl.fbaipublicfiles.com/dinov2\"\ndef _make_dinov2_model_name(arch_name: str, patch_size: int, num_register_tokens: int = 0) -> str:\n    compact_arch_name = arch_name.replace(\"_\", \"\")[:4]\n    registers_suffix = f\"_reg{num_register_tokens}\" if num_register_tokens else \"\"\n    return f\"dinov2_{compact_arch_name}{patch_size}{registers_suffix}\"\nclass CenterPadding(nn.Module):\n    def __init__(self, multiple):\n        super().__init__()\n        self.multiple = multiple\n    def _get_pad(self, size):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.hub.utils",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = False,\n        proj_bias: bool = True,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n    ) -> None:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "documentation": {}
    },
    {
        "label": "MemEffAttention",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "peekOfCode": "class MemEffAttention(Attention):\n    def forward(self, x: Tensor, attn_bias=None) -> Tensor:\n        if not XFORMERS_AVAILABLE:\n            if attn_bias is not None:\n                raise AssertionError(\"xFormers is required for using nested tensors\")\n            return super().forward(x)\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)\n        q, k, v = unbind(qkv, 2)\n        x = memory_efficient_attention(q, k, v, attn_bias=attn_bias)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nXFORMERS_ENABLED = os.environ.get(\"XFORMERS_DISABLED\") is None\ntry:\n    if XFORMERS_ENABLED:\n        from xformers.ops import memory_efficient_attention, unbind\n        XFORMERS_AVAILABLE = True\n        warnings.warn(\"xFormers is available (Attention)\")\n    else:\n        warnings.warn(\"xFormers is disabled (Attention)\")\n        raise ImportError",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "documentation": {}
    },
    {
        "label": "XFORMERS_ENABLED",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "peekOfCode": "XFORMERS_ENABLED = os.environ.get(\"XFORMERS_DISABLED\") is None\ntry:\n    if XFORMERS_ENABLED:\n        from xformers.ops import memory_efficient_attention, unbind\n        XFORMERS_AVAILABLE = True\n        warnings.warn(\"xFormers is available (Attention)\")\n    else:\n        warnings.warn(\"xFormers is disabled (Attention)\")\n        raise ImportError\nexcept ImportError:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.attention",
        "documentation": {}
    },
    {
        "label": "Block",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "class Block(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        qkv_bias: bool = False,\n        proj_bias: bool = True,\n        ffn_bias: bool = True,\n        drop: float = 0.0,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "NestedTensorBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "class NestedTensorBlock(Block):\n    def forward_nested(self, x_list: List[Tensor]) -> List[Tensor]:\n        \"\"\"\n        x_list contains a list of tensors to nest together and run\n        \"\"\"\n        assert isinstance(self.attn, MemEffAttention)\n        if self.training and self.sample_drop_ratio > 0.0:\n            def attn_residual_func(x: Tensor, attn_bias=None) -> Tensor:\n                return self.attn(self.norm1(x), attn_bias=attn_bias)\n            def ffn_residual_func(x: Tensor, attn_bias=None) -> Tensor:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "drop_add_residual_stochastic_depth",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "def drop_add_residual_stochastic_depth(\n    x: Tensor,\n    residual_func: Callable[[Tensor], Tensor],\n    sample_drop_ratio: float = 0.0,\n) -> Tensor:\n    # 1) extract subset using permutation\n    b, n, d = x.shape\n    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)\n    brange = (torch.randperm(b, device=x.device))[:sample_subset_size]\n    x_subset = x[brange]",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "get_branges_scales",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "def get_branges_scales(x, sample_drop_ratio=0.0):\n    b, n, d = x.shape\n    sample_subset_size = max(int(b * (1 - sample_drop_ratio)), 1)\n    brange = (torch.randperm(b, device=x.device))[:sample_subset_size]\n    residual_scale_factor = b / sample_subset_size\n    return brange, residual_scale_factor\ndef add_residual(x, brange, residual, residual_scale_factor, scaling_vector=None):\n    if scaling_vector is None:\n        x_flat = x.flatten(1)\n        residual = residual.flatten(1)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "add_residual",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "def add_residual(x, brange, residual, residual_scale_factor, scaling_vector=None):\n    if scaling_vector is None:\n        x_flat = x.flatten(1)\n        residual = residual.flatten(1)\n        x_plus_residual = torch.index_add(x_flat, 0, brange, residual.to(dtype=x.dtype), alpha=residual_scale_factor)\n    else:\n        x_plus_residual = scaled_index_add(\n            x, brange, residual.to(dtype=x.dtype), scaling=scaling_vector, alpha=residual_scale_factor\n        )\n    return x_plus_residual",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "get_attn_bias_and_cat",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "def get_attn_bias_and_cat(x_list, branges=None):\n    \"\"\"\n    this will perform the index select, cat the tensors, and provide the attn_bias from cache\n    \"\"\"\n    batch_sizes = [b.shape[0] for b in branges] if branges is not None else [x.shape[0] for x in x_list]\n    all_shapes = tuple((b, x.shape[1]) for b, x in zip(batch_sizes, x_list))\n    if all_shapes not in attn_bias_cache.keys():\n        seqlens = []\n        for b, x in zip(batch_sizes, x_list):\n            for _ in range(b):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "drop_add_residual_stochastic_depth_list",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "def drop_add_residual_stochastic_depth_list(\n    x_list: List[Tensor],\n    residual_func: Callable[[Tensor, Any], Tensor],\n    sample_drop_ratio: float = 0.0,\n    scaling_vector=None,\n) -> Tensor:\n    # 1) generate random set of indices for dropping samples in the batch\n    branges_scales = [get_branges_scales(x, sample_drop_ratio=sample_drop_ratio) for x in x_list]\n    branges = [s[0] for s in branges_scales]\n    residual_scale_factors = [s[1] for s in branges_scales]",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nXFORMERS_ENABLED = os.environ.get(\"XFORMERS_DISABLED\") is None\ntry:\n    if XFORMERS_ENABLED:\n        from xformers.ops import fmha, scaled_index_add, index_select_cat\n        XFORMERS_AVAILABLE = True\n        warnings.warn(\"xFormers is available (Block)\")\n    else:\n        warnings.warn(\"xFormers is disabled (Block)\")\n        raise ImportError",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "XFORMERS_ENABLED",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "peekOfCode": "XFORMERS_ENABLED = os.environ.get(\"XFORMERS_DISABLED\") is None\ntry:\n    if XFORMERS_ENABLED:\n        from xformers.ops import fmha, scaled_index_add, index_select_cat\n        XFORMERS_AVAILABLE = True\n        warnings.warn(\"xFormers is available (Block)\")\n    else:\n        warnings.warn(\"xFormers is disabled (Block)\")\n        raise ImportError\nexcept ImportError:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.block",
        "documentation": {}
    },
    {
        "label": "DINOHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.dino_head",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.dino_head",
        "peekOfCode": "class DINOHead(nn.Module):\n    def __init__(\n        self,\n        in_dim,\n        out_dim,\n        use_bn=False,\n        nlayers=3,\n        hidden_dim=2048,\n        bottleneck_dim=256,\n        mlp_bias=True,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.dino_head",
        "documentation": {}
    },
    {
        "label": "DropPath",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "peekOfCode": "class DropPath(nn.Module):\n    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "documentation": {}
    },
    {
        "label": "drop_path",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "peekOfCode": "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n    if drop_prob == 0.0 or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n    if keep_prob > 0.0:\n        random_tensor.div_(keep_prob)\n    output = x * random_tensor\n    return output",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.drop_path",
        "documentation": {}
    },
    {
        "label": "LayerScale",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.layer_scale",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.layer_scale",
        "peekOfCode": "class LayerScale(nn.Module):\n    def __init__(\n        self,\n        dim: int,\n        init_values: Union[float, Tensor] = 1e-5,\n        inplace: bool = False,\n    ) -> None:\n        super().__init__()\n        self.inplace = inplace\n        self.gamma = nn.Parameter(init_values * torch.ones(dim))",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.layer_scale",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.mlp",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.mlp",
        "peekOfCode": "class Mlp(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = nn.GELU,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.mlp",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"\n    2D image to patch embedding: (B,C,H,W) -> (B,N,D)\n    Args:\n        img_size: Image size.\n        patch_size: Patch token size.\n        in_chans: Number of input image channels.\n        embed_dim: Number of linear projection output channels.\n        norm_layer: Normalization layer.\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "documentation": {}
    },
    {
        "label": "make_2tuple",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "peekOfCode": "def make_2tuple(x):\n    if isinstance(x, tuple):\n        assert len(x) == 2\n        return x\n    assert isinstance(x, int)\n    return (x, x)\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    2D image to patch embedding: (B,C,H,W) -> (B,N,D)\n    Args:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.patch_embed",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFN",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "peekOfCode": "class SwiGLUFFN(nn.Module):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = None,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "documentation": {}
    },
    {
        "label": "SwiGLUFFNFused",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "peekOfCode": "class SwiGLUFFNFused(SwiGLU):\n    def __init__(\n        self,\n        in_features: int,\n        hidden_features: Optional[int] = None,\n        out_features: Optional[int] = None,\n        act_layer: Callable[..., nn.Module] = None,\n        drop: float = 0.0,\n        bias: bool = True,\n    ) -> None:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "documentation": {}
    },
    {
        "label": "XFORMERS_ENABLED",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "peekOfCode": "XFORMERS_ENABLED = os.environ.get(\"XFORMERS_DISABLED\") is None\ntry:\n    if XFORMERS_ENABLED:\n        from xformers.ops import SwiGLU\n        XFORMERS_AVAILABLE = True\n        warnings.warn(\"xFormers is available (SwiGLU)\")\n    else:\n        warnings.warn(\"xFormers is disabled (SwiGLU)\")\n        raise ImportError\nexcept ImportError:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.layers.swiglu_ffn",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "peekOfCode": "class MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\", output_file=None):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n        self.output_file = output_file\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "documentation": {}
    },
    {
        "label": "SmoothedValue",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "peekOfCode": "class SmoothedValue:\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({global_avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "description": "stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\", output_file=None):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n        self.output_file = output_file\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.logging.helpers",
        "documentation": {}
    },
    {
        "label": "DINOLoss",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.loss.dino_clstoken_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.loss.dino_clstoken_loss",
        "peekOfCode": "class DINOLoss(nn.Module):\n    def __init__(\n        self,\n        out_dim,\n        student_temp=0.1,\n        center_momentum=0.9,\n    ):\n        super().__init__()\n        self.student_temp = student_temp\n        self.center_momentum = center_momentum",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.loss.dino_clstoken_loss",
        "documentation": {}
    },
    {
        "label": "iBOTPatchLoss",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "peekOfCode": "class iBOTPatchLoss(nn.Module):\n    def __init__(self, patch_out_dim, student_temp=0.1, center_momentum=0.9):\n        super().__init__()\n        self.student_temp = student_temp\n        self.center_momentum = center_momentum\n        self.register_buffer(\"center\", torch.zeros(1, 1, patch_out_dim))\n        self.updated = True\n        self.reduce_handle = None\n        self.len_teacher_patch_tokens = None\n        self.async_batch_center = None",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ntry:\n    from xformers.ops import cross_entropy\n    def lossfunc(t, s, temp):\n        s = s.float()\n        t = t.float()\n        if s.ndim == 2:\n            return -cross_entropy(s.unsqueeze(0), t.unsqueeze(0), temp, bw_inplace=True).squeeze(0)\n        elif s.ndim == 3:\n            return -cross_entropy(s, t, temp, bw_inplace=True)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.loss.ibot_patch_loss",
        "documentation": {}
    },
    {
        "label": "KoLeoLoss",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "peekOfCode": "class KoLeoLoss(nn.Module):\n    \"\"\"Kozachenko-Leonenko entropic loss regularizer from Sablayrolles et al. - 2018 - Spreading vectors for similarity search\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.pdist = nn.PairwiseDistance(2, eps=1e-8)\n    def pairwise_NNs_inner(self, x):\n        \"\"\"\n        Pairwise nearest neighbors for L2-normalized vectors.\n        Uses Torch rather than Faiss to remain on GPU.\n        \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "description": "stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass KoLeoLoss(nn.Module):\n    \"\"\"Kozachenko-Leonenko entropic loss regularizer from Sablayrolles et al. - 2018 - Spreading vectors for similarity search\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.pdist = nn.PairwiseDistance(2, eps=1e-8)\n    def pairwise_NNs_inner(self, x):\n        \"\"\"\n        Pairwise nearest neighbors for L2-normalized vectors.\n        Uses Torch rather than Faiss to remain on GPU.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.loss.koleo_loss",
        "documentation": {}
    },
    {
        "label": "BlockChunk",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "class BlockChunk(nn.ModuleList):\n    def forward(self, x):\n        for b in self:\n            x = b(x)\n        return x\nclass DinoVisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "DinoVisionTransformer",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "class DinoVisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size=224,\n        patch_size=16,\n        in_chans=3,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4.0,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "named_apply",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for child_name, child_module in module.named_children():\n        child_name = \".\".join((name, child_name)) if name else child_name\n        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        fn(module=module, name=name)\n    return module\nclass BlockChunk(nn.ModuleList):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "init_weights_vit_timm",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def init_weights_vit_timm(module: nn.Module, name: str = \"\"):\n    \"\"\"ViT weight initialization, original timm impl (for reproducibility)\"\"\"\n    if isinstance(module, nn.Linear):\n        trunc_normal_(module.weight, std=0.02)\n        if module.bias is not None:\n            nn.init.zeros_(module.bias)\ndef vit_small(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=384,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_small",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def vit_small(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=384,\n        depth=12,\n        num_heads=6,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_base",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def vit_base(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=768,\n        depth=12,\n        num_heads=12,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_large",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def vit_large(patch_size=16, num_register_tokens=0, **kwargs):\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1024,\n        depth=24,\n        num_heads=16,\n        mlp_ratio=4,\n        block_fn=partial(Block, attn_class=MemEffAttention),\n        num_register_tokens=num_register_tokens,\n        **kwargs,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "vit_giant2",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "def vit_giant2(patch_size=16, num_register_tokens=0, **kwargs):\n    \"\"\"\n    Close to ViT-giant, with embed-dim 1536 and 24 heads => embed-dim per head 64\n    \"\"\"\n    model = DinoVisionTransformer(\n        patch_size=patch_size,\n        embed_dim=1536,\n        depth=40,\n        num_heads=24,\n        mlp_ratio=4,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "description": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef named_apply(fn: Callable, module: nn.Module, name=\"\", depth_first=True, include_root=False) -> nn.Module:\n    if not depth_first and include_root:\n        fn(module=module, name=name)\n    for child_name, child_module in module.named_children():\n        child_name = \".\".join((name, child_name)) if name else child_name\n        named_apply(fn=fn, module=child_module, name=child_name, depth_first=depth_first, include_root=True)\n    if depth_first and include_root:\n        fn(module=module, name=name)\n    return module",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.models.vision_transformer",
        "documentation": {}
    },
    {
        "label": "Evaluator",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "peekOfCode": "class Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.knn import main as knn_main\n        self._setup_args()\n        knn_main(self.args)\n    def checkpoint(self):\n        import submitit\n        logger.info(f\"Requeuing {self.args}\")",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "peekOfCode": "def main():\n    description = \"Submitit launcher for DINOv2 k-NN evaluation\"\n    knn_args_parser = get_knn_args_parser(add_help=False)\n    parents = [knn_args_parser]\n    args_parser = get_args_parser(description=description, parents=parents)\n    args = args_parser.parse_args()\n    setup_logging()\n    assert os.path.exists(args.config_file), \"Configuration file does not exist!\"\n    submit_jobs(Evaluator, args, name=\"dinov2:knn\")\n    return 0",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.knn import main as knn_main\n        self._setup_args()\n        knn_main(self.args)\n    def checkpoint(self):\n        import submitit",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.eval.knn",
        "documentation": {}
    },
    {
        "label": "Evaluator",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "peekOfCode": "class Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.linear import main as linear_main\n        self._setup_args()\n        linear_main(self.args)\n    def checkpoint(self):\n        import submitit\n        logger.info(f\"Requeuing {self.args}\")",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "peekOfCode": "def main():\n    description = \"Submitit launcher for DINOv2 linear evaluation\"\n    linear_args_parser = get_linear_args_parser(add_help=False)\n    parents = [linear_args_parser]\n    args_parser = get_args_parser(description=description, parents=parents)\n    args = args_parser.parse_args()\n    setup_logging()\n    assert os.path.exists(args.config_file), \"Configuration file does not exist!\"\n    submit_jobs(Evaluator, args, name=\"dinov2:linear\")\n    return 0",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.linear import main as linear_main\n        self._setup_args()\n        linear_main(self.args)\n    def checkpoint(self):\n        import submitit",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.eval.linear",
        "documentation": {}
    },
    {
        "label": "Evaluator",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "peekOfCode": "class Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.log_regression import main as log_regression_main\n        self._setup_args()\n        log_regression_main(self.args)\n    def checkpoint(self):\n        import submitit\n        logger.info(f\"Requeuing {self.args}\")",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "peekOfCode": "def main():\n    description = \"Submitit launcher for DINOv2 logistic evaluation\"\n    log_regression_args_parser = get_log_regression_args_parser(add_help=False)\n    parents = [log_regression_args_parser]\n    args_parser = get_args_parser(description=description, parents=parents)\n    args = args_parser.parse_args()\n    setup_logging()\n    assert os.path.exists(args.config_file), \"Configuration file does not exist!\"\n    submit_jobs(Evaluator, args, name=\"dinov2:logreg\")\n    return 0",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass Evaluator:\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.eval.log_regression import main as log_regression_main\n        self._setup_args()\n        log_regression_main(self.args)\n    def checkpoint(self):\n        import submitit",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.eval.log_regression",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "peekOfCode": "class Trainer(object):\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.train import main as train_main\n        self._setup_args()\n        train_main(self.args)\n    def checkpoint(self):\n        import submitit\n        logger.info(f\"Requeuing {self.args}\")",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "peekOfCode": "def main():\n    description = \"Submitit launcher for DINOv2 training\"\n    train_args_parser = get_train_args_parser(add_help=False)\n    parents = [train_args_parser]\n    args_parser = get_args_parser(description=description, parents=parents)\n    args = args_parser.parse_args()\n    setup_logging()\n    assert os.path.exists(args.config_file), \"Configuration file does not exist!\"\n    submit_jobs(Trainer, args, name=\"dinov2:train\")\n    return 0",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass Trainer(object):\n    def __init__(self, args):\n        self.args = args\n    def __call__(self):\n        from dinov2.train import main as train_main\n        self._setup_args()\n        train_main(self.args)\n    def checkpoint(self):\n        import submitit",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.train.train",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "peekOfCode": "def get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n) -> argparse.ArgumentParser:\n    parents = parents or []\n    slurm_partition = get_slurm_partition()\n    parser = argparse.ArgumentParser(\n        description=description,\n        parents=parents,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "get_shared_folder",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "peekOfCode": "def get_shared_folder() -> Path:\n    user_checkpoint_path = get_user_checkpoint_path()\n    if user_checkpoint_path is None:\n        raise RuntimeError(\"Path to user checkpoint cannot be determined\")\n    path = user_checkpoint_path / \"experiments\"\n    path.mkdir(exist_ok=True)\n    return path\ndef submit_jobs(task_class, args, name: str):\n    if not args.output_dir:\n        args.output_dir = str(get_shared_folder() / \"%j\")",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "submit_jobs",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "peekOfCode": "def submit_jobs(task_class, args, name: str):\n    if not args.output_dir:\n        args.output_dir = str(get_shared_folder() / \"%j\")\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n    executor = submitit.AutoExecutor(folder=args.output_dir, slurm_max_num_timeout=30)\n    kwargs = {}\n    if args.use_volta32:\n        kwargs[\"slurm_constraint\"] = \"volta32gb\"\n    if args.comment:\n        kwargs[\"slurm_comment\"] = args.comment",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "description": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef get_args_parser(\n    description: Optional[str] = None,\n    parents: Optional[List[argparse.ArgumentParser]] = None,\n    add_help: bool = True,\n) -> argparse.ArgumentParser:\n    parents = parents or []\n    slurm_partition = get_slurm_partition()\n    parser = argparse.ArgumentParser(\n        description=description,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.run.submit",
        "documentation": {}
    },
    {
        "label": "SSLMetaArch",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "peekOfCode": "class SSLMetaArch(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.fp16_scaler = ShardedGradScaler() if cfg.compute_precision.grad_scaler else None\n        student_model_dict = dict()\n        teacher_model_dict = dict()\n        student_backbone, teacher_backbone, embed_dim = build_model_from_cfg(cfg)\n        student_model_dict[\"backbone\"] = student_backbone\n        teacher_model_dict[\"backbone\"] = teacher_backbone",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\nclass SSLMetaArch(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.fp16_scaler = ShardedGradScaler() if cfg.compute_precision.grad_scaler else None\n        student_model_dict = dict()\n        teacher_model_dict = dict()\n        student_backbone, teacher_backbone, embed_dim = build_model_from_cfg(cfg)\n        student_model_dict[\"backbone\"] = student_backbone",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.ssl_meta_arch",
        "documentation": {}
    },
    {
        "label": "get_args_parser",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def get_args_parser(add_help: bool = True):\n    parser = argparse.ArgumentParser(\"DINOv2 training\", add_help=add_help)\n    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--no-resume\",\n        action=\"store_true\",\n        help=\"Whether to not attempt to resume from the checkpoint directory. \",\n    )\n    parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"perform evaluation only\")\n    parser.add_argument(\"--eval\", type=str, default=\"\", help=\"Eval type to perform\")",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "build_optimizer",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def build_optimizer(cfg, params_groups):\n    return torch.optim.AdamW(params_groups, betas=(cfg.optim.adamw_beta1, cfg.optim.adamw_beta2))\ndef build_schedulers(cfg):\n    OFFICIAL_EPOCH_LENGTH = cfg.train.OFFICIAL_EPOCH_LENGTH\n    lr = dict(\n        base_value=cfg.optim[\"lr\"],\n        final_value=cfg.optim[\"min_lr\"],\n        total_iters=cfg.optim[\"epochs\"] * OFFICIAL_EPOCH_LENGTH,\n        warmup_iters=cfg.optim[\"warmup_epochs\"] * OFFICIAL_EPOCH_LENGTH,\n        start_warmup_value=0,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "build_schedulers",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def build_schedulers(cfg):\n    OFFICIAL_EPOCH_LENGTH = cfg.train.OFFICIAL_EPOCH_LENGTH\n    lr = dict(\n        base_value=cfg.optim[\"lr\"],\n        final_value=cfg.optim[\"min_lr\"],\n        total_iters=cfg.optim[\"epochs\"] * OFFICIAL_EPOCH_LENGTH,\n        warmup_iters=cfg.optim[\"warmup_epochs\"] * OFFICIAL_EPOCH_LENGTH,\n        start_warmup_value=0,\n    )\n    wd = dict(",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "apply_optim_scheduler",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def apply_optim_scheduler(optimizer, lr, wd, last_layer_lr):\n    for param_group in optimizer.param_groups:\n        is_last_layer = param_group[\"is_last_layer\"]\n        lr_multiplier = param_group[\"lr_multiplier\"]\n        wd_multiplier = param_group[\"wd_multiplier\"]\n        param_group[\"weight_decay\"] = wd * wd_multiplier\n        param_group[\"lr\"] = (last_layer_lr if is_last_layer else lr) * lr_multiplier\ndef do_test(cfg, model, iteration):\n    new_state_dict = model.teacher.state_dict()\n    if distributed.is_main_process():",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "do_test",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def do_test(cfg, model, iteration):\n    new_state_dict = model.teacher.state_dict()\n    if distributed.is_main_process():\n        iterstring = str(iteration)\n        eval_dir = os.path.join(cfg.train.output_dir, \"eval\", iterstring)\n        os.makedirs(eval_dir, exist_ok=True)\n        # save teacher checkpoint\n        teacher_ckp_path = os.path.join(eval_dir, \"teacher_checkpoint.pth\")\n        torch.save({\"teacher\": new_state_dict}, teacher_ckp_path)\ndef do_train(cfg, model, resume=False):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "do_train",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def do_train(cfg, model, resume=False):\n    model.train()\n    inputs_dtype = torch.half\n    fp16_scaler = model.fp16_scaler  # for mixed precision training\n    # setup optimizer\n    optimizer = build_optimizer(cfg, model.get_params_groups())\n    (\n        lr_schedule,\n        wd_schedule,\n        momentum_schedule,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "def main(args):\n    cfg = setup(args)\n    model = SSLMetaArch(cfg).to(torch.device(\"cuda\"))\n    model.prepare_for_distributed_training()\n    logger.info(\"Model:\\n{}\".format(model))\n    if args.eval_only:\n        iteration = (\n            FSDPCheckpointer(model, save_dir=cfg.train.output_dir)\n            .resume_or_load(cfg.MODEL.WEIGHTS, resume=not args.no_resume)\n            .get(\"iteration\", -1)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "torch.backends.cuda.matmul.allow_tf32",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "torch.backends.cuda.matmul.allow_tf32 = True  # PyTorch 1.12 sets this to False by default\nlogger = logging.getLogger(\"dinov2\")\ndef get_args_parser(add_help: bool = True):\n    parser = argparse.ArgumentParser(\"DINOv2 training\", add_help=add_help)\n    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--no-resume\",\n        action=\"store_true\",\n        help=\"Whether to not attempt to resume from the checkpoint directory. \",\n    )",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "description": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef get_args_parser(add_help: bool = True):\n    parser = argparse.ArgumentParser(\"DINOv2 training\", add_help=add_help)\n    parser.add_argument(\"--config-file\", default=\"\", metavar=\"FILE\", help=\"path to config file\")\n    parser.add_argument(\n        \"--no-resume\",\n        action=\"store_true\",\n        help=\"Whether to not attempt to resume from the checkpoint directory. \",\n    )\n    parser.add_argument(\"--eval-only\", action=\"store_true\", help=\"perform evaluation only\")",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.train.train",
        "documentation": {}
    },
    {
        "label": "ClusterType",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "class ClusterType(Enum):\n    AWS = \"aws\"\n    FAIR = \"fair\"\n    RSC = \"rsc\"\ndef _guess_cluster_type() -> ClusterType:\n    uname = os.uname()\n    if uname.sysname == \"Linux\":\n        if uname.release.endswith(\"-aws\"):\n            # Linux kernel versions on AWS instances are of the form \"5.4.0-1051-aws\"\n            return ClusterType.AWS",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_cluster_type",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "def get_cluster_type(cluster_type: Optional[ClusterType] = None) -> Optional[ClusterType]:\n    if cluster_type is None:\n        return _guess_cluster_type()\n    return cluster_type\ndef get_checkpoint_path(cluster_type: Optional[ClusterType] = None) -> Optional[Path]:\n    cluster_type = get_cluster_type(cluster_type)\n    if cluster_type is None:\n        return None\n    CHECKPOINT_DIRNAMES = {\n        ClusterType.AWS: \"checkpoints\",",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_checkpoint_path",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "def get_checkpoint_path(cluster_type: Optional[ClusterType] = None) -> Optional[Path]:\n    cluster_type = get_cluster_type(cluster_type)\n    if cluster_type is None:\n        return None\n    CHECKPOINT_DIRNAMES = {\n        ClusterType.AWS: \"checkpoints\",\n        ClusterType.FAIR: \"checkpoint\",\n        ClusterType.RSC: \"checkpoint/dino\",\n    }\n    return Path(\"/\") / CHECKPOINT_DIRNAMES[cluster_type]",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_user_checkpoint_path",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "def get_user_checkpoint_path(cluster_type: Optional[ClusterType] = None) -> Optional[Path]:\n    checkpoint_path = get_checkpoint_path(cluster_type)\n    if checkpoint_path is None:\n        return None\n    username = os.environ.get(\"USER\")\n    assert username is not None\n    return checkpoint_path / username\ndef get_slurm_partition(cluster_type: Optional[ClusterType] = None) -> Optional[str]:\n    cluster_type = get_cluster_type(cluster_type)\n    if cluster_type is None:",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_slurm_partition",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "def get_slurm_partition(cluster_type: Optional[ClusterType] = None) -> Optional[str]:\n    cluster_type = get_cluster_type(cluster_type)\n    if cluster_type is None:\n        return None\n    SLURM_PARTITIONS = {\n        ClusterType.AWS: \"learnlab\",\n        ClusterType.FAIR: \"learnlab\",\n        ClusterType.RSC: \"learn\",\n    }\n    return SLURM_PARTITIONS[cluster_type]",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "get_slurm_executor_parameters",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "peekOfCode": "def get_slurm_executor_parameters(\n    nodes: int, num_gpus_per_node: int, cluster_type: Optional[ClusterType] = None, **kwargs\n) -> Dict[str, Any]:\n    # create default parameters\n    params = {\n        \"mem_gb\": 0,  # Requests all memory on a node, see https://slurm.schedmd.com/sbatch.html\n        \"gpus_per_node\": num_gpus_per_node,\n        \"tasks_per_node\": num_gpus_per_node,  # one task per GPU\n        \"cpus_per_task\": 10,\n        \"nodes\": nodes,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.cluster",
        "documentation": {}
    },
    {
        "label": "apply_scaling_rules_to_cfg",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "def apply_scaling_rules_to_cfg(cfg):  # to fix\n    if cfg.optim.scaling_rule == \"sqrt_wrt_1024\":\n        base_lr = cfg.optim.base_lr\n        cfg.optim.lr = base_lr\n        cfg.optim.lr *= math.sqrt(cfg.train.batch_size_per_gpu * distributed.get_global_size() / 1024.0)\n        logger.info(f\"sqrt scaling learning rate; base: {base_lr}, new: {cfg.optim.lr}\")\n    else:\n        raise NotImplementedError\n    return cfg\ndef write_config(cfg, output_dir, name=\"config.yaml\"):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "write_config",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "def write_config(cfg, output_dir, name=\"config.yaml\"):\n    logger.info(OmegaConf.to_yaml(cfg))\n    saved_cfg_path = os.path.join(output_dir, name)\n    with open(saved_cfg_path, \"w\") as f:\n        OmegaConf.save(config=cfg, f=f)\n    return saved_cfg_path\ndef get_cfg_from_args(args):\n    args.output_dir = os.path.abspath(args.output_dir)\n    args.opts += [f\"train.output_dir={args.output_dir}\"]\n    default_cfg = OmegaConf.create(dinov2_default_config)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "get_cfg_from_args",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "def get_cfg_from_args(args):\n    args.output_dir = os.path.abspath(args.output_dir)\n    args.opts += [f\"train.output_dir={args.output_dir}\"]\n    default_cfg = OmegaConf.create(dinov2_default_config)\n    cfg = OmegaConf.load(args.config_file)\n    cfg = OmegaConf.merge(default_cfg, cfg, OmegaConf.from_cli(args.opts))\n    return cfg\ndef default_setup(args):\n    distributed.enable(overwrite=True)\n    seed = getattr(args, \"seed\", 0)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "default_setup",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "def default_setup(args):\n    distributed.enable(overwrite=True)\n    seed = getattr(args, \"seed\", 0)\n    rank = distributed.get_global_rank()\n    global logger\n    setup_logging(output=args.output_dir, level=logging.INFO)\n    logger = logging.getLogger(\"dinov2\")\n    utils.fix_random_seeds(seed + rank)\n    logger.info(\"git:\\n  {}\\n\".format(utils.get_sha()))\n    logger.info(\"\\n\".join(\"%s: %s\" % (k, str(v)) for k, v in sorted(dict(vars(args)).items())))",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "setup",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "def setup(args):\n    \"\"\"\n    Create configs and perform basic setups.\n    \"\"\"\n    cfg = get_cfg_from_args(args)\n    os.makedirs(args.output_dir, exist_ok=True)\n    default_setup(args)\n    apply_scaling_rules_to_cfg(cfg)\n    write_config(cfg, args.output_dir)\n    return cfg",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef apply_scaling_rules_to_cfg(cfg):  # to fix\n    if cfg.optim.scaling_rule == \"sqrt_wrt_1024\":\n        base_lr = cfg.optim.base_lr\n        cfg.optim.lr = base_lr\n        cfg.optim.lr *= math.sqrt(cfg.train.batch_size_per_gpu * distributed.get_global_size() / 1024.0)\n        logger.info(f\"sqrt scaling learning rate; base: {base_lr}, new: {cfg.optim.lr}\")\n    else:\n        raise NotImplementedError\n    return cfg",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.config",
        "documentation": {}
    },
    {
        "label": "as_torch_dtype",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "peekOfCode": "def as_torch_dtype(dtype: TypeSpec) -> torch.dtype:\n    if isinstance(dtype, torch.dtype):\n        return dtype\n    if isinstance(dtype, str):\n        dtype = np.dtype(dtype)\n    assert isinstance(dtype, np.dtype), f\"Expected an instance of nunpy dtype, got {type(dtype)}\"\n    return _NUMPY_TO_TORCH_DTYPE[dtype]",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "documentation": {}
    },
    {
        "label": "TypeSpec",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "peekOfCode": "TypeSpec = Union[str, np.dtype, torch.dtype]\n_NUMPY_TO_TORCH_DTYPE: Dict[np.dtype, torch.dtype] = {\n    np.dtype(\"bool\"): torch.bool,\n    np.dtype(\"uint8\"): torch.uint8,\n    np.dtype(\"int8\"): torch.int8,\n    np.dtype(\"int16\"): torch.int16,\n    np.dtype(\"int32\"): torch.int32,\n    np.dtype(\"int64\"): torch.int64,\n    np.dtype(\"float16\"): torch.float16,\n    np.dtype(\"float32\"): torch.float32,",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.dtype",
        "documentation": {}
    },
    {
        "label": "get_vit_lr_decay_rate",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "peekOfCode": "def get_vit_lr_decay_rate(name, lr_decay_rate=1.0, num_layers=12, force_is_backbone=False, chunked_blocks=False):\n    \"\"\"\n    Calculate lr decay rate for different ViT blocks.\n    Args:\n        name (string): parameter name.\n        lr_decay_rate (float): base lr decay rate.\n        num_layers (int): number of ViT blocks.\n    Returns:\n        lr decay rate for the given parameter.\n    \"\"\"",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "get_params_groups_with_decay",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "peekOfCode": "def get_params_groups_with_decay(model, lr_decay_rate=1.0, patch_embed_lr_mult=1.0):\n    chunked_blocks = False\n    if hasattr(model, \"n_blocks\"):\n        logger.info(\"chunked fsdp\")\n        n_blocks = model.n_blocks\n        chunked_blocks = model.chunked_blocks\n    elif hasattr(model, \"blocks\"):\n        logger.info(\"first code branch\")\n        n_blocks = len(model.blocks)\n    elif hasattr(model, \"backbone\"):",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "fuse_params_groups",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "peekOfCode": "def fuse_params_groups(all_params_groups, keys=(\"lr_multiplier\", \"wd_multiplier\", \"is_last_layer\")):\n    fused_params_groups = defaultdict(lambda: {\"params\": []})\n    for d in all_params_groups:\n        identifier = \"\"\n        for k in keys:\n            identifier += k + str(d[k]) + \"_\"\n        for k in keys:\n            fused_params_groups[identifier][k] = d[k]\n        fused_params_groups[identifier][\"params\"].append(d[\"params\"])\n    return fused_params_groups.values()",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef get_vit_lr_decay_rate(name, lr_decay_rate=1.0, num_layers=12, force_is_backbone=False, chunked_blocks=False):\n    \"\"\"\n    Calculate lr decay rate for different ViT blocks.\n    Args:\n        name (string): parameter name.\n        lr_decay_rate (float): base lr decay rate.\n        num_layers (int): number of ViT blocks.\n    Returns:\n        lr decay rate for the given parameter.",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.param_groups",
        "documentation": {}
    },
    {
        "label": "CosineScheduler",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "class CosineScheduler(object):\n    def __init__(self, base_value, final_value, total_iters, warmup_iters=0, start_warmup_value=0, freeze_iters=0):\n        super().__init__()\n        self.final_value = final_value\n        self.total_iters = total_iters\n        freeze_schedule = np.zeros((freeze_iters))\n        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n        iters = np.arange(total_iters - warmup_iters - freeze_iters)\n        schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n        self.schedule = np.concatenate((freeze_schedule, warmup_schedule, schedule))",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "load_pretrained_weights",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "def load_pretrained_weights(model, pretrained_weights, checkpoint_key):\n    if urlparse(pretrained_weights).scheme:  # If it looks like an URL\n        state_dict = torch.hub.load_state_dict_from_url(pretrained_weights, map_location=\"cpu\")\n    else:\n        state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n    if checkpoint_key is not None and checkpoint_key in state_dict:\n        logger.info(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n        state_dict = state_dict[checkpoint_key]\n    # remove `module.` prefix\n    state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "fix_random_seeds",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "def fix_random_seeds(seed=31):\n    \"\"\"\n    Fix random seeds.\n    \"\"\"\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\ndef get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "get_sha",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "def get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))\n    def _run(command):\n        return subprocess.check_output(command, cwd=cwd).decode(\"ascii\").strip()\n    sha = \"N/A\"\n    diff = \"clean\"\n    branch = \"N/A\"\n    try:\n        sha = _run([\"git\", \"rev-parse\", \"HEAD\"])\n        subprocess.check_output([\"git\", \"diff\"], cwd=cwd)",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "has_batchnorms",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "def has_batchnorms(model):\n    bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.SyncBatchNorm)\n    for name, module in model.named_modules():\n        if isinstance(module, bn_types):\n            return True\n    return False",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "description": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "peekOfCode": "logger = logging.getLogger(\"dinov2\")\ndef load_pretrained_weights(model, pretrained_weights, checkpoint_key):\n    if urlparse(pretrained_weights).scheme:  # If it looks like an URL\n        state_dict = torch.hub.load_state_dict_from_url(pretrained_weights, map_location=\"cpu\")\n    else:\n        state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n    if checkpoint_key is not None and checkpoint_key in state_dict:\n        logger.info(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n        state_dict = state_dict[checkpoint_key]\n    # remove `module.` prefix",
        "detail": "stereo.modeling.models.foundationstereo.dinov2.utils.utils",
        "documentation": {}
    },
    {
        "label": "set_logging_format",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def set_logging_format(level=logging.INFO):\n  importlib.reload(logging)\n  FORMAT = '%(message)s'\n  logging.basicConfig(level=level, format=FORMAT, datefmt='%m-%d|%H:%M:%S')\nset_logging_format()\ndef set_seed(random_seed):\n  import torch,random\n  np.random.seed(random_seed)\n  random.seed(random_seed)\n  torch.manual_seed(random_seed)",
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def set_seed(random_seed):\n  import torch,random\n  np.random.seed(random_seed)\n  random.seed(random_seed)\n  torch.manual_seed(random_seed)\n  torch.cuda.manual_seed_all(random_seed)\n  torch.backends.cudnn.deterministic = True\n  torch.backends.cudnn.benchmark = False\ndef toOpen3dCloud(points,colors=None,normals=None):\n  cloud = o3d.geometry.PointCloud()",
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "toOpen3dCloud",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def toOpen3dCloud(points,colors=None,normals=None):\n  cloud = o3d.geometry.PointCloud()\n  cloud.points = o3d.utility.Vector3dVector(points.astype(np.float64))\n  if colors is not None:\n    if colors.max()>1:\n      colors = colors/255.0\n    cloud.colors = o3d.utility.Vector3dVector(colors.astype(np.float64))\n  if normals is not None:\n    cloud.normals = o3d.utility.Vector3dVector(normals.astype(np.float64))\n  return cloud",
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "depth2xyzmap",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def depth2xyzmap(depth:np.ndarray, K, uvs:np.ndarray=None, zmin=0.1):\n  invalid_mask = (depth<zmin)\n  H,W = depth.shape[:2]\n  if uvs is None:\n    vs,us = np.meshgrid(np.arange(0,H),np.arange(0,W), sparse=False, indexing='ij')\n    vs = vs.reshape(-1)\n    us = us.reshape(-1)\n  else:\n    uvs = uvs.round().astype(int)\n    us = uvs[:,0]",
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "freeze_model",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def freeze_model(model):\n  model = model.eval()\n  for p in model.parameters():\n    p.requires_grad = False\n  for p in model.buffers():\n    p.requires_grad = False\n  return model\ndef get_resize_keep_aspect_ratio(H, W, divider=16, max_H=1232, max_W=1232):\n  assert max_H%divider==0\n  assert max_W%divider==0",
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "get_resize_keep_aspect_ratio",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def get_resize_keep_aspect_ratio(H, W, divider=16, max_H=1232, max_W=1232):\n  assert max_H%divider==0\n  assert max_W%divider==0\n  def round_by_divider(x):\n    return int(np.ceil(x/divider)*divider)\n  H_resize = round_by_divider(H)   #!NOTE KITTI width=1242\n  W_resize = round_by_divider(W)\n  if H_resize>max_H or W_resize>max_W:\n    if H_resize>W_resize:\n      W_resize = round_by_divider(W_resize*max_H/H_resize)",
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "vis_disparity",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def vis_disparity(disp, min_val=None, max_val=None, invalid_thres=np.inf, color_map=cv2.COLORMAP_TURBO, cmap=None, other_output={}):\n  \"\"\"\n  @disp: np array (H,W)\n  @invalid_thres: > thres is invalid\n  \"\"\"\n  disp = disp.copy()\n  H,W = disp.shape[:2]\n  invalid_mask = disp>=invalid_thres\n  if (invalid_mask==0).sum()==0:\n    other_output['min_val'] = None",
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "depth_uint8_decoding",
        "kind": 2,
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "def depth_uint8_decoding(depth_uint8, scale=1000):\n  depth_uint8 = depth_uint8.astype(float)\n  out = depth_uint8[...,0]*255*255 + depth_uint8[...,1]*255 + depth_uint8[...,2]\n  return out/float(scale)",
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "code_dir",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.Utils",
        "description": "stereo.modeling.models.foundationstereo.Utils",
        "peekOfCode": "code_dir = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(code_dir)\ndef set_logging_format(level=logging.INFO):\n  importlib.reload(logging)\n  FORMAT = '%(message)s'\n  logging.basicConfig(level=level, format=FORMAT, datefmt='%m-%d|%H:%M:%S')\nset_logging_format()\ndef set_seed(random_seed):\n  import torch,random\n  np.random.seed(random_seed)",
        "detail": "stereo.modeling.models.foundationstereo.Utils",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.foundationstereo.trainer",
        "description": "stereo.modeling.models.foundationstereo.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.foundationstereo.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.foundationstereo.trainer",
        "description": "stereo.modeling.models.foundationstereo.trainer",
        "peekOfCode": "__all__ = {\n    'FoundationStereo': FoundationStereo,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.foundationstereo.trainer",
        "documentation": {}
    },
    {
        "label": "GwcNet",
        "kind": 6,
        "importPath": "stereo.modeling.models.gwcnet.gwcnet",
        "description": "stereo.modeling.models.gwcnet.gwcnet",
        "peekOfCode": "class GwcNet(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        use_concat_volume = cfgs.USE_CONCAT_VOLUME\n        concat_channels = cfgs.CONCAT_CHANNELS\n        downsample = cfgs.DOWNSAMPLE\n        num_groups = cfgs.NUM_GROUPS\n        self.Backbone = GwcNetBackbone(use_concat_volume=use_concat_volume, concat_channels=concat_channels)\n        self.CostProcessor = GwcVolumeCostProcessor(maxdisp=self.maxdisp, downsample=downsample, num_groups=num_groups,",
        "detail": "stereo.modeling.models.gwcnet.gwcnet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "description": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = convbn(planes, planes, 3, 1, pad, dilation)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):",
        "detail": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "description": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self, concat_feature=False, concat_feature_channel=12):\n        super(feature_extraction, self).__init__()\n        self.concat_feature = concat_feature\n        self.inplanes = 32\n        self.firstconv = nn.Sequential(convbn(3, 32, 3, 2, 1, 1),\n                                       nn.ReLU(inplace=True),\n                                       convbn(32, 32, 3, 1, 1, 1),\n                                       nn.ReLU(inplace=True),\n                                       convbn(32, 32, 3, 1, 1, 1),",
        "detail": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "documentation": {}
    },
    {
        "label": "GwcNet",
        "kind": 6,
        "importPath": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "description": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "peekOfCode": "class GwcNet(nn.Module):\n    def __init__(self, use_concat_volume=True, concat_channels=12):\n        super().__init__()\n        self.use_concat_volume = use_concat_volume\n        self.concat_channels = concat_channels if use_concat_volume else 0\n        self.feature_extraction = feature_extraction(self.use_concat_volume, self.concat_channels)\n    def forward(self, inputs):\n        ref_img = inputs[\"left\"]\n        tgt_img = inputs[\"right\"]\n        ref_feature = self.feature_extraction(ref_img)",
        "detail": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "description": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "peekOfCode": "def convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                                   padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n                         nn.BatchNorm2d(out_channels))\nclass BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Sequential(convbn(inplanes, planes, 3, stride, pad, dilation),\n                                   nn.ReLU(inplace=True))",
        "detail": "stereo.modeling.models.gwcnet.gwcnet_backbone",
        "documentation": {}
    },
    {
        "label": "GwcVolumeCostProcessor",
        "kind": 6,
        "importPath": "stereo.modeling.models.gwcnet.gwcnet_cost_processor",
        "description": "stereo.modeling.models.gwcnet.gwcnet_cost_processor",
        "peekOfCode": "class GwcVolumeCostProcessor(nn.Module):\n    def __init__(self, maxdisp=192, downsample=4, num_groups=40, use_concat_volume=True, *args, **kwargs):\n        super().__init__()\n        self.maxdisp = maxdisp\n        self.downsample = downsample\n        self.num_groups = num_groups\n        self.use_concat_volume = use_concat_volume\n    def groupwise_correlation(self, fea1, fea2):\n        B, C, H, W = fea1.shape\n        num_groups = self.num_groups",
        "detail": "stereo.modeling.models.gwcnet.gwcnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "GwcDispProcessor",
        "kind": 6,
        "importPath": "stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "description": "stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "peekOfCode": "class GwcDispProcessor(nn.Module):\n    def __init__(self, maxdisp=192, downsample=4, num_groups=40, use_concat_volume=True, concat_channels=12, *args,\n                 **kwargs):\n        super().__init__()\n        self.maxdisp = maxdisp\n        self.downsample = downsample\n        self.num_groups = num_groups\n        self.use_concat_volume = use_concat_volume\n        self.concat_channels = concat_channels if use_concat_volume else 0\n        self.dres0 = nn.Sequential(",
        "detail": "stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "description": "stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "peekOfCode": "def convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(\n        nn.Conv3d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=pad,\n            bias=False\n        ),",
        "detail": "stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "description": "stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=False)\nclass GwcDispProcessor(nn.Module):\n    def __init__(self, maxdisp=192, downsample=4, num_groups=40, use_concat_volume=True, concat_channels=12, *args,\n                 **kwargs):\n        super().__init__()\n        self.maxdisp = maxdisp",
        "detail": "stereo.modeling.models.gwcnet.gwcnet_disp_processor",
        "documentation": {}
    },
    {
        "label": "Hourglass",
        "kind": 6,
        "importPath": "stereo.modeling.models.gwcnet.hourglass",
        "description": "stereo.modeling.models.gwcnet.hourglass",
        "peekOfCode": "class Hourglass(nn.Module):\n    def __init__(self, in_channels):\n        super().__init__()\n        self.conv1 = nn.Sequential(convbn_3d(in_channels, in_channels * 2, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv2 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 2, 3, 1, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv3 = nn.Sequential(convbn_3d(in_channels * 2, in_channels * 4, 3, 2, 1),\n                                   nn.ReLU(inplace=True))\n        self.conv4 = nn.Sequential(convbn_3d(in_channels * 4, in_channels * 4, 3, 1, 1),",
        "detail": "stereo.modeling.models.gwcnet.hourglass",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "stereo.modeling.models.gwcnet.hourglass",
        "description": "stereo.modeling.models.gwcnet.hourglass",
        "peekOfCode": "def convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(\n        nn.Conv3d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=pad,\n            bias=False\n        ),",
        "detail": "stereo.modeling.models.gwcnet.hourglass",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.gwcnet.trainer",
        "description": "stereo.modeling.models.gwcnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.gwcnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.gwcnet.trainer",
        "description": "stereo.modeling.models.gwcnet.trainer",
        "peekOfCode": "__all__ = {\n    'GwcNet': GwcNet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.gwcnet.trainer",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.extractor",
        "description": "stereo.modeling.models.igev.extractor",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)",
        "detail": "stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "BottleneckBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.extractor",
        "description": "stereo.modeling.models.igev.extractor",
        "peekOfCode": "class BottleneckBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(BottleneckBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes // 4, kernel_size=1, padding=0)\n        self.conv2 = nn.Conv2d(planes // 4, planes // 4, kernel_size=3, padding=1, stride=stride)\n        self.conv3 = nn.Conv2d(planes // 4, planes, kernel_size=1, padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes // 4)",
        "detail": "stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "BasicEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.extractor",
        "description": "stereo.modeling.models.igev.extractor",
        "peekOfCode": "class BasicEncoder(nn.Module):\n    def __init__(self, output_dim=128, norm_fn='batch', dropout=0.0, downsample=3):\n        super(BasicEncoder, self).__init__()\n        self.norm_fn = norm_fn\n        self.downsample = downsample\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n        elif self.norm_fn == 'batch':\n            self.norm1 = nn.BatchNorm2d(64)\n        elif self.norm_fn == 'instance':",
        "detail": "stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "MultiBasicEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.extractor",
        "description": "stereo.modeling.models.igev.extractor",
        "peekOfCode": "class MultiBasicEncoder(nn.Module):\n    def __init__(self, output_dim=[128], norm_fn='batch', dropout=0.0, downsample=3):\n        super(MultiBasicEncoder, self).__init__()\n        self.norm_fn = norm_fn\n        self.downsample = downsample\n        # self.norm_111 = nn.BatchNorm2d(128, affine=False, track_running_stats=False)\n        # self.norm_222 = nn.BatchNorm2d(128, affine=False, track_running_stats=False)\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)\n        elif self.norm_fn == 'batch':",
        "detail": "stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "SubModule",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.extractor",
        "description": "stereo.modeling.models.igev.extractor",
        "peekOfCode": "class SubModule(nn.Module):\n    def __init__(self):\n        super(SubModule, self).__init__()\n    def weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels",
        "detail": "stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.extractor",
        "description": "stereo.modeling.models.igev.extractor",
        "peekOfCode": "class Feature(SubModule):\n    def __init__(self):\n        super(Feature, self).__init__()\n        pretrained = True\n        model = timm.create_model('mobilenetv2_100', pretrained=pretrained, features_only=True)\n        layers = [1, 2, 3, 5, 6]\n        chans = [16, 24, 32, 96, 160]\n        self.conv_stem = model.conv_stem\n        self.bn1 = model.bn1\n        self.act1 = model.act1",
        "detail": "stereo.modeling.models.igev.extractor",
        "documentation": {}
    },
    {
        "label": "Combined_Geo_Encoding_Volume",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.geometry",
        "description": "stereo.modeling.models.igev.geometry",
        "peekOfCode": "class Combined_Geo_Encoding_Volume:\n    def __init__(self, init_fmap1, init_fmap2, geo_volume, num_levels=2, radius=4):\n        self.num_levels = num_levels\n        self.radius = radius\n        self.geo_volume_pyramid = []\n        self.init_corr_pyramid = []\n        # all pairs correlation\n        init_corr = Combined_Geo_Encoding_Volume.corr(init_fmap1, init_fmap2)\n        b, h, w, _, w2 = init_corr.shape\n        b, c, d, h, w = geo_volume.shape",
        "detail": "stereo.modeling.models.igev.geometry",
        "documentation": {}
    },
    {
        "label": "hourglass",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.igev_stereo",
        "description": "stereo.modeling.models.igev.igev_stereo",
        "peekOfCode": "class hourglass(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass, self).__init__()\n        self.conv1 = nn.Sequential(\n            BasicConv(in_channels, in_channels * 2, is_3d=True, bn=True, relu=True, kernel_size=3,\n                      padding=1, stride=2, dilation=1),\n            BasicConv(in_channels * 2, in_channels * 2, is_3d=True, bn=True, relu=True, kernel_size=3,\n                      padding=1, stride=1, dilation=1))\n        self.conv2 = nn.Sequential(\n            BasicConv(in_channels * 2, in_channels * 4, is_3d=True, bn=True, relu=True, kernel_size=3,",
        "detail": "stereo.modeling.models.igev.igev_stereo",
        "documentation": {}
    },
    {
        "label": "IGEVStereo",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.igev_stereo",
        "description": "stereo.modeling.models.igev.igev_stereo",
        "peekOfCode": "class IGEVStereo(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.args = args\n        self.max_disp = args.MAX_DISP\n        context_dims = args.HIDDEN_DIMS\n        self.cnet = MultiBasicEncoder(output_dim=[args.HIDDEN_DIMS, context_dims], norm_fn=\"batch\",\n                                      downsample=args.N_DOWNSAMPLE)\n        self.update_block = BasicMultiUpdateBlock(self.args, hidden_dims=args.HIDDEN_DIMS)\n        self.context_zqr_convs = nn.ModuleList(",
        "detail": "stereo.modeling.models.igev.igev_stereo",
        "documentation": {}
    },
    {
        "label": "BasicConv",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "class BasicConv(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, bn=True, relu=True, **kwargs):\n        super(BasicConv, self).__init__()\n        self.relu = relu\n        self.use_bn = bn\n        if is_3d:\n            if deconv:\n                self.conv = nn.ConvTranspose3d(in_channels, out_channels, bias=False, **kwargs)\n            else:\n                self.conv = nn.Conv3d(in_channels, out_channels, bias=False, **kwargs)",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "Conv2x",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "class Conv2x(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, concat=True, keep_concat=True, bn=True,\n                 relu=True, keep_dispc=False):\n        super(Conv2x, self).__init__()\n        self.concat = concat\n        self.is_3d = is_3d\n        if deconv and is_3d:\n            kernel = (4, 4, 4)\n        elif deconv:\n            kernel = 4",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "BasicConv_IN",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "class BasicConv_IN(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, IN=True, relu=True, **kwargs):\n        super(BasicConv_IN, self).__init__()\n        self.relu = relu\n        self.use_in = IN\n        if is_3d:\n            if deconv:\n                self.conv = nn.ConvTranspose3d(in_channels, out_channels, bias=False, **kwargs)\n            else:\n                self.conv = nn.Conv3d(in_channels, out_channels, bias=False, **kwargs)",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "Conv2x_IN",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "class Conv2x_IN(nn.Module):\n    def __init__(self, in_channels, out_channels, deconv=False, is_3d=False, concat=True, keep_concat=True, IN=True,\n                 relu=True, keep_dispc=False):\n        super(Conv2x_IN, self).__init__()\n        self.concat = concat\n        self.is_3d = is_3d\n        if deconv and is_3d:\n            kernel = (4, 4, 4)\n        elif deconv:\n            kernel = 4",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "FeatureAtt",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "class FeatureAtt(nn.Module):\n    def __init__(self, cv_chan, feat_chan):\n        super(FeatureAtt, self).__init__()\n        self.feat_att = nn.Sequential(\n            BasicConv(feat_chan, feat_chan // 2, kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(feat_chan // 2, cv_chan, 1))\n    def forward(self, cv, feat):\n        '''\n        '''\n        feat_att = self.feat_att(feat).unsqueeze(2)",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "norm_correlation",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "def norm_correlation(fea1, fea2):\n    cost = torch.mean(\n        ((fea1 / (torch.norm(fea1, 2, 1, True) + 1e-05)) * (fea2 / (torch.norm(fea2, 2, 1, True) + 1e-05))), dim=1,\n        keepdim=True)\n    return cost\ndef build_norm_correlation_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 1, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "build_norm_correlation_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "def build_norm_correlation_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 1, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = norm_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i])\n        else:\n            volume[:, :, i, :, :] = norm_correlation(refimg_fea, targetimg_fea)\n    volume = volume.contiguous()\n    return volume",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "correlation",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "def correlation(fea1, fea2):\n    cost = torch.sum((fea1 * fea2), dim=1, keepdim=True)\n    return cost\ndef build_correlation_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 1, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i])\n        else:",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "build_correlation_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "def build_correlation_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 1, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i])\n        else:\n            volume[:, :, i, :, :] = correlation(refimg_fea, targetimg_fea)\n    volume = volume.contiguous()\n    return volume",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "build_concat_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "def build_concat_volume(refimg_fea, targetimg_fea, maxdisp):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, 2 * C, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :C, i, :, :] = refimg_fea[:, :, :, :]\n            volume[:, C:, i, :, i:] = targetimg_fea[:, :, :, :-i]\n        else:\n            volume[:, :C, i, :, :] = refimg_fea\n            volume[:, C:, i, :, :] = targetimg_fea",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=True)\nclass FeatureAtt(nn.Module):\n    def __init__(self, cv_chan, feat_chan):\n        super(FeatureAtt, self).__init__()\n        self.feat_att = nn.Sequential(\n            BasicConv(feat_chan, feat_chan // 2, kernel_size=1, stride=1, padding=0),",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "context_upsample",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.submodule",
        "description": "stereo.modeling.models.igev.submodule",
        "peekOfCode": "def context_upsample(disp_low, up_weights):\n    ###\n    # cv (b,1,h,w)\n    # sp (b,9,4*h,4*w)\n    ###\n    b, c, h, w = disp_low.shape\n    disp_unfold = F.unfold(disp_low.reshape(b, c, h, w), 3, 1, 1).reshape(b, -1, h, w)\n    disp_unfold = F.interpolate(disp_unfold, (h * 4, w * 4), mode='nearest').reshape(b, 9, h * 4, w * 4)\n    disp = (disp_unfold * up_weights).sum(1)\n    return disp",
        "detail": "stereo.modeling.models.igev.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.trainer",
        "description": "stereo.modeling.models.igev.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.igev.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.igev.trainer",
        "description": "stereo.modeling.models.igev.trainer",
        "peekOfCode": "__all__ = {\n    'IGEV': IGEVStereo,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.igev.trainer",
        "documentation": {}
    },
    {
        "label": "FlowHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.update",
        "description": "stereo.modeling.models.igev.update",
        "peekOfCode": "class FlowHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=2):\n        super(FlowHead, self).__init__()\n        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 3, padding=1)\n        self.conv2 = nn.Conv2d(hidden_dim, output_dim, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\nclass DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):",
        "detail": "stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "DispHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.update",
        "description": "stereo.modeling.models.igev.update",
        "peekOfCode": "class DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):\n        super(DispHead, self).__init__()\n        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 3, padding=1)\n        self.conv2 = nn.Conv2d(hidden_dim, output_dim, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\nclass ConvGRU(nn.Module):\n    def __init__(self, hidden_dim, input_dim, kernel_size=3):",
        "detail": "stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "ConvGRU",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.update",
        "description": "stereo.modeling.models.igev.update",
        "peekOfCode": "class ConvGRU(nn.Module):\n    def __init__(self, hidden_dim, input_dim, kernel_size=3):\n        super(ConvGRU, self).__init__()\n        self.convz = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convq = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n    def forward(self, h, cz, cr, cq, *x_list):\n        x = torch.cat(x_list, dim=1)\n        hx = torch.cat([h, x], dim=1)\n        z = torch.sigmoid(self.convz(hx) + cz)",
        "detail": "stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "SepConvGRU",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.update",
        "description": "stereo.modeling.models.igev.update",
        "peekOfCode": "class SepConvGRU(nn.Module):\n    def __init__(self, hidden_dim=128, input_dim=192 + 128):\n        super(SepConvGRU, self).__init__()\n        self.convz1 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (1, 5), padding=(0, 2))\n        self.convr1 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (1, 5), padding=(0, 2))\n        self.convq1 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (1, 5), padding=(0, 2))\n        self.convz2 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (5, 1), padding=(2, 0))\n        self.convr2 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (5, 1), padding=(2, 0))\n        self.convq2 = nn.Conv2d(hidden_dim + input_dim, hidden_dim, (5, 1), padding=(2, 0))\n    def forward(self, h, *x):",
        "detail": "stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "BasicMotionEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.update",
        "description": "stereo.modeling.models.igev.update",
        "peekOfCode": "class BasicMotionEncoder(nn.Module):\n    def __init__(self, args):\n        super(BasicMotionEncoder, self).__init__()\n        self.args = args\n        cor_planes = args.CORR_LEVELS * (2 * args.CORR_RADIUS + 1) * (8 + 1)\n        self.convc1 = nn.Conv2d(cor_planes, 64, 1, padding=0)\n        self.convc2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.convd1 = nn.Conv2d(1, 64, 7, padding=3)\n        self.convd2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.conv = nn.Conv2d(64 + 64, 128 - 1, 3, padding=1)",
        "detail": "stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "BasicMultiUpdateBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.update",
        "description": "stereo.modeling.models.igev.update",
        "peekOfCode": "class BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, args, hidden_dims=[]):\n        super().__init__()\n        self.args = args\n        self.encoder = BasicMotionEncoder(args)\n        encoder_output_dim = 128\n        self.gru04 = ConvGRU(hidden_dims[2], encoder_output_dim + hidden_dims[1] * (args.N_GRU_LAYERS > 1))\n        self.gru08 = ConvGRU(hidden_dims[1], hidden_dims[0] * (args.N_GRU_LAYERS == 3) + hidden_dims[2])\n        self.gru16 = ConvGRU(hidden_dims[0], hidden_dims[1])\n        self.disp_head = DispHead(hidden_dims[2], hidden_dim=256, output_dim=1)",
        "detail": "stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "pool2x",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.update",
        "description": "stereo.modeling.models.igev.update",
        "peekOfCode": "def pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)\ndef pool4x(x):\n    return F.avg_pool2d(x, 5, stride=4, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, args, hidden_dims=[]):\n        super().__init__()",
        "detail": "stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "pool4x",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.update",
        "description": "stereo.modeling.models.igev.update",
        "peekOfCode": "def pool4x(x):\n    return F.avg_pool2d(x, 5, stride=4, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, args, hidden_dims=[]):\n        super().__init__()\n        self.args = args\n        self.encoder = BasicMotionEncoder(args)",
        "detail": "stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "interp",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.update",
        "description": "stereo.modeling.models.igev.update",
        "peekOfCode": "def interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, args, hidden_dims=[]):\n        super().__init__()\n        self.args = args\n        self.encoder = BasicMotionEncoder(args)\n        encoder_output_dim = 128\n        self.gru04 = ConvGRU(hidden_dims[2], encoder_output_dim + hidden_dims[1] * (args.N_GRU_LAYERS > 1))",
        "detail": "stereo.modeling.models.igev.update",
        "documentation": {}
    },
    {
        "label": "InputPadder",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.utils",
        "description": "stereo.modeling.models.igev.utils",
        "peekOfCode": "class InputPadder:\n    \"\"\" Pads images such that dimensions are divisible by 8 \"\"\"\n    def __init__(self, dims, mode='sintel', divis_by=8):\n        self.ht, self.wd = dims[-2:]\n        pad_ht = (((self.ht // divis_by) + 1) * divis_by - self.ht) % divis_by\n        pad_wd = (((self.wd // divis_by) + 1) * divis_by - self.wd) % divis_by\n        if mode == 'sintel':\n            self._pad = [pad_wd // 2, pad_wd - pad_wd // 2, pad_ht // 2, pad_ht - pad_ht // 2]\n        else:\n            self._pad = [pad_wd // 2, pad_wd - pad_wd // 2, 0, pad_ht]",
        "detail": "stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "Map",
        "kind": 6,
        "importPath": "stereo.modeling.models.igev.utils",
        "description": "stereo.modeling.models.igev.utils",
        "peekOfCode": "class Map(dict):\n    \"\"\"\n    Example:\n    m = Map({'first_name': 'Eduardo'}, last_name='Pool', age=24, sports=['Soccer'])\n    \"\"\"\n    def __init__(self, *args, **kwargs):\n        super(Map, self).__init__(*args, **kwargs)\n        for arg in args:\n            if isinstance(arg, dict):\n                for k, v in arg.items():",
        "detail": "stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "forward_interpolate",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.utils",
        "description": "stereo.modeling.models.igev.utils",
        "peekOfCode": "def forward_interpolate(flow):\n    flow = flow.detach().cpu().numpy()\n    dx, dy = flow[0], flow[1]\n    ht, wd = dx.shape\n    x0, y0 = np.meshgrid(np.arange(wd), np.arange(ht))\n    x1 = x0 + dx\n    y1 = y0 + dy\n    x1 = x1.reshape(-1)\n    y1 = y1.reshape(-1)\n    dx = dx.reshape(-1)",
        "detail": "stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.utils",
        "description": "stereo.modeling.models.igev.utils",
        "peekOfCode": "def bilinear_sampler(img, coords, mode='bilinear', mask=False):\n    \"\"\" Wrapper for grid_sample, uses pixel coordinates \"\"\"\n    H, W = img.shape[-2:]\n    # print(\"$$$55555\", img.shape, coords.shape)\n    xgrid, ygrid = coords.split([1, 1], dim=-1)\n    xgrid = 2 * xgrid / (W - 1) - 1\n    # print(\"######88888\", xgrid)\n    assert torch.unique(ygrid).numel() == 1 and H == 1  # This is a stereo problem\n    grid = torch.cat([xgrid, ygrid], dim=-1)\n    # print(\"###37777\", grid.shape)",
        "detail": "stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "coords_grid",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.utils",
        "description": "stereo.modeling.models.igev.utils",
        "peekOfCode": "def coords_grid(batch, ht, wd):\n    coords = torch.meshgrid(torch.arange(ht), torch.arange(wd))\n    coords = torch.stack(coords[::-1], dim=0).float()\n    return coords[None].repeat(batch, 1, 1, 1)\ndef upflow8(flow, mode='bilinear'):\n    new_size = (8 * flow.shape[2], 8 * flow.shape[3])\n    return 8 * F.interpolate(flow, size=new_size, mode=mode, align_corners=True)\ndef gauss_blur(input, N=5, std=1):\n    B, D, H, W = input.shape\n    x, y = torch.meshgrid(torch.arange(N).float() - N // 2, torch.arange(N).float() - N // 2)",
        "detail": "stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "upflow8",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.utils",
        "description": "stereo.modeling.models.igev.utils",
        "peekOfCode": "def upflow8(flow, mode='bilinear'):\n    new_size = (8 * flow.shape[2], 8 * flow.shape[3])\n    return 8 * F.interpolate(flow, size=new_size, mode=mode, align_corners=True)\ndef gauss_blur(input, N=5, std=1):\n    B, D, H, W = input.shape\n    x, y = torch.meshgrid(torch.arange(N).float() - N // 2, torch.arange(N).float() - N // 2)\n    unnormalized_gaussian = torch.exp(-(x.pow(2) + y.pow(2)) / (2 * std ** 2))\n    weights = unnormalized_gaussian / unnormalized_gaussian.sum().clamp(min=1e-4)\n    weights = weights.view(1, 1, N, N).to(input)\n    output = F.conv2d(input.reshape(B * D, 1, H, W), weights, padding=N // 2)",
        "detail": "stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "gauss_blur",
        "kind": 2,
        "importPath": "stereo.modeling.models.igev.utils",
        "description": "stereo.modeling.models.igev.utils",
        "peekOfCode": "def gauss_blur(input, N=5, std=1):\n    B, D, H, W = input.shape\n    x, y = torch.meshgrid(torch.arange(N).float() - N // 2, torch.arange(N).float() - N // 2)\n    unnormalized_gaussian = torch.exp(-(x.pow(2) + y.pow(2)) / (2 * std ** 2))\n    weights = unnormalized_gaussian / unnormalized_gaussian.sum().clamp(min=1e-4)\n    weights = weights.view(1, 1, N, N).to(input)\n    output = F.conv2d(input.reshape(B * D, 1, H, W), weights, padding=N // 2)\n    return output.view(B, D, H, W)\nclass Map(dict):\n    \"\"\"",
        "detail": "stereo.modeling.models.igev.utils",
        "documentation": {}
    },
    {
        "label": "MsCostVolumeManager",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.cost_volume",
        "description": "stereo.modeling.models.iinet.cost_volume",
        "peekOfCode": "class MsCostVolumeManager(nn.Module):\n    def __init__(self,\n                 num_depth_bins=64,\n                 multiscale=1,\n                 disp_scale=2,\n                 matching_dim_size=16,\n                 dot_dim=1):\n        super().__init__()\n        self.num_depth_bins = [4, 6, num_depth_bins]\n        self.disp_scale = disp_scale",
        "detail": "stereo.modeling.models.iinet.cost_volume",
        "documentation": {}
    },
    {
        "label": "IINet",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.iinet",
        "description": "stereo.modeling.models.iinet.iinet",
        "peekOfCode": "class IINet(nn.Module):\n    def __init__(self, opts):\n        super().__init__()\n        self.run_opts = opts\n        self.num_ch_enc = [16, 24, 40, 112, 160]\n        # iniitalize the first half of the U-Net, encoding the cost volume\n        # and image prior image feautres\n        if self.run_opts.CV_ENCODER_TYPE == \"multi_scale_encoder\":\n            self.cost_volume_net = CVEncoder(\n                num_ch_cv=self.run_opts.MAX_DISP // 2 ** (self.run_opts.MATCHING_SCALE + 1),",
        "detail": "stereo.modeling.models.iinet.iinet",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.layers",
        "description": "stereo.modeling.models.iinet.layers",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,\n        groups: int = 1,\n        base_width: int = 64,\n        dilation: int = 1,",
        "detail": "stereo.modeling.models.iinet.layers",
        "documentation": {}
    },
    {
        "label": "TensorFormatter",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.layers",
        "description": "stereo.modeling.models.iinet.layers",
        "peekOfCode": "class TensorFormatter(nn.Module):\n    \"\"\"Helper to format, apply operation, format back tensor.\n    Class to format tensors of shape B x D x C_i x H x W into B*D x C_i x H x W,\n    apply an operation, and reshape back into B x D x C_o x H x W.\n    Used for multidepth - batching feature extraction on source images\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.batch_size = None\n        self.depth_chns = None\n    def _expand_batch_with_channels(self, x):",
        "detail": "stereo.modeling.models.iinet.layers",
        "documentation": {}
    },
    {
        "label": "conv3x3",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.layers",
        "description": "stereo.modeling.models.iinet.layers",
        "peekOfCode": "def conv3x3(\n            in_planes: int, \n            out_planes: int, \n            stride: int = 1, \n            groups: int = 1, \n            dilation: int = 1, \n            bias: bool = False\n        ) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,",
        "detail": "stereo.modeling.models.iinet.layers",
        "documentation": {}
    },
    {
        "label": "conv1x1",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.layers",
        "description": "stereo.modeling.models.iinet.layers",
        "peekOfCode": "def conv1x1(in_planes: int, out_planes: int, stride: int = 1, bias: bool = False) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=bias)\nclass BasicBlock(nn.Module):\n    expansion: int = 1\n    def __init__(\n        self,\n        inplanes: int,\n        planes: int,\n        stride: int = 1,",
        "detail": "stereo.modeling.models.iinet.layers",
        "documentation": {}
    },
    {
        "label": "multiple_basic_block",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.layers",
        "description": "stereo.modeling.models.iinet.layers",
        "peekOfCode": "def multiple_basic_block(num_ch_in, num_ch_out, num_repeats=2):\n    layers = nn.Sequential(BasicBlock(num_ch_in, num_ch_out))\n    for i in range(num_repeats - 1):\n        layers.add_module(f\"conv_{i}\", BasicBlock(num_ch_out, num_ch_out))\n    return layers\nclass TensorFormatter(nn.Module):\n    \"\"\"Helper to format, apply operation, format back tensor.\n    Class to format tensors of shape B x D x C_i x H x W into B*D x C_i x H x W,\n    apply an operation, and reshape back into B x D x C_o x H x W.\n    Used for multidepth - batching feature extraction on source images\"\"\"",
        "detail": "stereo.modeling.models.iinet.layers",
        "documentation": {}
    },
    {
        "label": "VolumeBiFocalLoss",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.loss",
        "description": "stereo.modeling.models.iinet.loss",
        "peekOfCode": "class VolumeBiFocalLoss(nn.Module):\n    def __init__(self, alpha, gamma):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n    def forward(self, pt, target, weight):\n        loss = - weight * self.alpha * ((1 - pt) ** self.gamma) * (target * torch.log(pt)) - (1 - self.alpha) * (\n                        pt ** self.gamma) * ((1 - target) * torch.log(1 - pt))\n        return loss\nclass Criterion(nn.Module):",
        "detail": "stereo.modeling.models.iinet.loss",
        "documentation": {}
    },
    {
        "label": "Criterion",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.loss",
        "description": "stereo.modeling.models.iinet.loss",
        "peekOfCode": "class Criterion(nn.Module):\n    \"\"\"\n    Compute loss and evaluation metrics\n    \"\"\"\n    def __init__(self, max_disp: int = -1, loss_weight: dict = None, out_scales=4, disp_scale=16):\n        super(Criterion, self).__init__()\n        if loss_weight is None:\n            loss_weight = {}\n        self.num_scales = out_scales\n        self.max_disp = max_disp",
        "detail": "stereo.modeling.models.iinet.loss",
        "documentation": {}
    },
    {
        "label": "build_criterion",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.loss",
        "description": "stereo.modeling.models.iinet.loss",
        "peekOfCode": "def build_criterion(opts, loss_weight):\n    return Criterion(opts.MAX_DISP, loss_weight, opts.OUT_SCALE, opts.DISP_SCALE)",
        "detail": "stereo.modeling.models.iinet.loss",
        "documentation": {}
    },
    {
        "label": "make_nograd_func",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.metrics",
        "description": "stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def make_nograd_func(func):\n    def wrapper(*f_args, **f_kwargs):\n        with torch.no_grad():\n            ret = func(*f_args, **f_kwargs)\n        return ret\n    return wrapper\ndef check_shape_for_metric_computation(*vars):\n    assert isinstance(vars, tuple)\n    for var in vars:\n        assert var.size() == vars[0].size()",
        "detail": "stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "check_shape_for_metric_computation",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.metrics",
        "description": "stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def check_shape_for_metric_computation(*vars):\n    assert isinstance(vars, tuple)\n    for var in vars:\n        assert var.size() == vars[0].size()\n# a wrapper to compute metrics for each image individually\ndef compute_metric_for_each_image(metric_func):\n    def wrapper(D_ests, D_gts, masks, *nargs):\n        check_shape_for_metric_computation(D_ests, D_gts, masks)\n        bn = D_gts.shape[0]  # batch size\n        results = []  # a list to store results for each image",
        "detail": "stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "compute_metric_for_each_image",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.metrics",
        "description": "stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def compute_metric_for_each_image(metric_func):\n    def wrapper(D_ests, D_gts, masks, *nargs):\n        check_shape_for_metric_computation(D_ests, D_gts, masks)\n        bn = D_gts.shape[0]  # batch size\n        results = []  # a list to store results for each image\n        # compute result one by one\n        for idx in range(bn):\n            # if tensor, then pick idx, else pass the same value\n            cur_nargs = [x[idx] if isinstance(x, (Tensor, Variable)) else x for x in nargs]\n            #if masks[idx].float().mean() / (D_gts[idx] > 0).float().mean() < 0.1:",
        "detail": "stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "D1_metric",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.metrics",
        "description": "stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def D1_metric(D_est, D_gt, mask):\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    E = torch.abs(D_gt - D_est)\n    err_mask = (E > 3) & (E / D_gt.abs() > 0.05)\n    return torch.mean(err_mask.float())\n@make_nograd_func\n@compute_metric_for_each_image\ndef Thres_metric(D_est, D_gt, mask, thres):\n    assert isinstance(thres, (int, float))\n    D_est, D_gt = D_est[mask], D_gt[mask]",
        "detail": "stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "Thres_metric",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.metrics",
        "description": "stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def Thres_metric(D_est, D_gt, mask, thres):\n    assert isinstance(thres, (int, float))\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    E = torch.abs(D_gt - D_est)\n    err_mask = E > thres\n    return torch.mean(err_mask.float())\n# NOTE: please do not use this to build up training loss\n@make_nograd_func\n@compute_metric_for_each_image\ndef EPE_metric(D_est, D_gt, mask):",
        "detail": "stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "EPE_metric",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.metrics",
        "description": "stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def EPE_metric(D_est, D_gt, mask):\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    return F.l1_loss(D_est, D_gt, size_average=True)\n@make_nograd_func\n@compute_metric_for_each_image\ndef D1_metric_mask(D_est, D_gt, mask, mask_img):\n    # D_est, D_gt = D_est[(mask&mask_img)], D_gt[(mask&mask_img)]\n    D_est, D_gt = D_est[mask_img], D_gt[mask_img]\n    E = torch.abs(D_gt - D_est)\n    err_mask = (E > 3) & (E / D_gt.abs() > 0.05)",
        "detail": "stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "D1_metric_mask",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.metrics",
        "description": "stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def D1_metric_mask(D_est, D_gt, mask, mask_img):\n    # D_est, D_gt = D_est[(mask&mask_img)], D_gt[(mask&mask_img)]\n    D_est, D_gt = D_est[mask_img], D_gt[mask_img]\n    E = torch.abs(D_gt - D_est)\n    err_mask = (E > 3) & (E / D_gt.abs() > 0.05)\n    return torch.mean(err_mask.float())\n@make_nograd_func\n@compute_metric_for_each_image\ndef Thres_metric_mask(D_est, D_gt, mask, thres, mask_img):\n    assert isinstance(thres, (int, float))",
        "detail": "stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "Thres_metric_mask",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.metrics",
        "description": "stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def Thres_metric_mask(D_est, D_gt, mask, thres, mask_img):\n    assert isinstance(thres, (int, float))\n    # D_est, D_gt = D_est[(mask&mask_img)], D_gt[(mask&mask_img)]\n    D_est, D_gt = D_est[mask_img], D_gt[mask_img]\n    E = torch.abs(D_gt - D_est)\n    err_mask = E > thres\n    return torch.mean(err_mask.float())\n# NOTE: please do not use this to build up training loss\n@make_nograd_func\n@compute_metric_for_each_image",
        "detail": "stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "EPE_metric_mask",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.metrics",
        "description": "stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def EPE_metric_mask(D_est, D_gt, mask, mask_img):\n    # print((mask&mask_img).size(), D_est.size(), mask, mask_img)\n    # D_est, D_gt = D_est[(mask&mask_img)], D_gt[(mask&mask_img)]\n    D_est, D_gt = D_est[mask_img], D_gt[mask_img]\n    return F.l1_loss(D_est, D_gt, size_average=True)\n@make_nograd_func\ndef cal_metric(disp_est: Tensor, disp_gt: Tensor, metric_dict: dict, valid_mask: Tensor):\n    with torch.no_grad():\n        metric_dict[\"epe\"] = EPE_metric(disp_est, disp_gt, valid_mask)\n        metric_dict[\"d1\"] = D1_metric(disp_est, disp_gt, valid_mask) * 100",
        "detail": "stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "cal_metric",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.metrics",
        "description": "stereo.modeling.models.iinet.metrics",
        "peekOfCode": "def cal_metric(disp_est: Tensor, disp_gt: Tensor, metric_dict: dict, valid_mask: Tensor):\n    with torch.no_grad():\n        metric_dict[\"epe\"] = EPE_metric(disp_est, disp_gt, valid_mask)\n        metric_dict[\"d1\"] = D1_metric(disp_est, disp_gt, valid_mask) * 100\n        metric_dict[\"bad1\"] = Thres_metric(disp_est, disp_gt, valid_mask, 1.0) * 100\n        metric_dict[\"bad2\"] = Thres_metric(disp_est, disp_gt, valid_mask, 2.0) * 100\n        metric_dict[\"bad3\"] = Thres_metric(disp_est, disp_gt, valid_mask, 3.0) * 100\n    return",
        "detail": "stereo.modeling.models.iinet.metrics",
        "documentation": {}
    },
    {
        "label": "RaftUpSampler",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.networks",
        "description": "stereo.modeling.models.iinet.networks",
        "peekOfCode": "class RaftUpSampler(nn.Module):\n    def __init__(\n                self,\n                fdim=128,\n                hdim=128,\n                priordim=1,\n            ):\n        super().__init__()\n        self.fdim = fdim\n        self.hdim = hdim",
        "detail": "stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "DepthDecoderMSR",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.networks",
        "description": "stereo.modeling.models.iinet.networks",
        "peekOfCode": "class DepthDecoderMSR(nn.Module):\n    def __init__(\n                self,\n                num_ch_enc,\n                lrcv_scale=3,\n                multiscale=1,\n                scales=4,\n                num_output_channels=1,\n                use_skips=True\n            ):",
        "detail": "stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "CVEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.networks",
        "description": "stereo.modeling.models.iinet.networks",
        "peekOfCode": "class CVEncoder(nn.Module):\n    def __init__(self, num_ch_cv, num_ch_encs, num_ch_outs, lrcv_level, multi_scale=1):\n        super().__init__()\n        self.convs = nn.ModuleDict()\n        self.num_ch_enc = []\n        self.multi_scale = multi_scale\n        start_level = lrcv_level - multi_scale\n        num_ch_subencs = num_ch_encs[start_level:] #lrcv_level=0 represent stat with (1/2) resolution\n        num_ch_subouts = num_ch_outs[start_level:]\n        self.num_blocks = len(num_ch_subouts)",
        "detail": "stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.networks",
        "description": "stereo.modeling.models.iinet.networks",
        "peekOfCode": "class MLP(nn.Module):\n    def __init__(self, channel_list, disable_final_activation=True):\n        super(MLP, self).__init__()\n        layer_list = []\n        for layer_index in list(range(len(channel_list)))[:-1]:\n            layer_list.append(\n                            nn.Linear(channel_list[layer_index], \n                                channel_list[layer_index+1])\n                            )\n            layer_list.append(nn.LeakyReLU(inplace=True))",
        "detail": "stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "ResnetMatchingEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.networks",
        "description": "stereo.modeling.models.iinet.networks",
        "peekOfCode": "class ResnetMatchingEncoder(nn.Module):\n    \"\"\"Pytorch module for a resnet encoder\n    \"\"\"\n    def __init__(\n                self,\n                num_layers,\n                num_ch_out,\n                matching_scale=1,\n                multiscale=1,\n                pretrainedfp=None,",
        "detail": "stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "UnetMatchingEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.networks",
        "description": "stereo.modeling.models.iinet.networks",
        "peekOfCode": "class UnetMatchingEncoder(nn.Module):\n    \"\"\"Pytorch module for a resnet encoder\n        \"\"\"\n    def __init__(\n            self,\n            num_ch_out,\n            matching_scale=1,\n            multiscale=1,\n            pretrainedfp=None,\n    ):",
        "detail": "stereo.modeling.models.iinet.networks",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.trainer",
        "description": "stereo.modeling.models.iinet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)\n    def _get_pos_fullres(self, fx, w, h):\n        x_range = (np.linspace(0, w - 1, w) + 0.5 - w // 2) / fx\n        y_range = (np.linspace(0, h - 1, h) + 0.5 - h // 2) / fx\n        x, y = np.meshgrid(x_range, y_range)\n        z = np.ones_like(x)\n        pos_grid = np.stack([x, y, z], axis=0).astype(np.float32)",
        "detail": "stereo.modeling.models.iinet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.iinet.trainer",
        "description": "stereo.modeling.models.iinet.trainer",
        "peekOfCode": "__all__ = {\n    'IINet': IINet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)\n    def _get_pos_fullres(self, fx, w, h):\n        x_range = (np.linspace(0, w - 1, w) + 0.5 - w // 2) / fx\n        y_range = (np.linspace(0, h - 1, h) + 0.5 - h // 2) / fx",
        "detail": "stereo.modeling.models.iinet.trainer",
        "documentation": {}
    },
    {
        "label": "DictAverageMeter",
        "kind": 6,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "class DictAverageMeter(object):\n    def __init__(self, wsize=100):\n        self.wsize = wsize\n        self.sum_data = {}\n        self.avg_data = {}\n        self.count = {}\n        return\n    def update(self, new_input):\n        if len(self.sum_data) == 0:\n            for k, v in new_input.items():",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "make_nograd_func",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def make_nograd_func(func):\n    def wrapper(*f_args, **f_kwargs):\n        with torch.no_grad():\n            ret = func(*f_args, **f_kwargs)\n        return ret\n    return wrapper\n# convert a function into recursive style to handle nested dict/list/tuple variables\ndef make_recursive_func(func):\n    def wrapper(vars):\n        if isinstance(vars, list):",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "make_recursive_func",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def make_recursive_func(func):\n    def wrapper(vars):\n        if isinstance(vars, list):\n            return [wrapper(x) for x in vars]\n        elif isinstance(vars, tuple):\n            return tuple([wrapper(x) for x in vars])\n        elif isinstance(vars, dict):\n            return {k: wrapper(v) for k, v in vars.items()}\n        else:\n            return func(vars)",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "tensor2float",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def tensor2float(vars):\n    if isinstance(vars, float):\n        return vars\n    elif isinstance(vars, torch.Tensor):\n        return vars.data.item()\n    else:\n        raise NotImplementedError(\"invalid input type {} for tensor2float\".format(type(vars)))\n@make_recursive_func\ndef tensor2numpy(vars):\n    if isinstance(vars, np.ndarray):",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "tensor2numpy",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def tensor2numpy(vars):\n    if isinstance(vars, np.ndarray):\n        return vars\n    elif isinstance(vars, torch.Tensor):\n        return vars.detach().cpu().numpy().copy()\n    else:\n        raise NotImplementedError(\"invalid input type {} for tensor2numpy\".format(type(vars)))\n@make_recursive_func\ndef tocuda(vars):\n    if isinstance(vars, torch.Tensor):",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "tocuda",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def tocuda(vars):\n    if isinstance(vars, torch.Tensor):\n        return vars.to(torch.device(\"cuda\"))\n    elif isinstance(vars, str):\n        return vars\n    else:\n        raise NotImplementedError(\"invalid input type {} for tensor2numpy\".format(type(vars)))\ndef upsample(x):\n    \"\"\"\n    Upsample input tensor by a factor of 2",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "upsample",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def upsample(x):\n    \"\"\"\n    Upsample input tensor by a factor of 2\n    \"\"\"\n    return nn.functional.interpolate(\n                                x,\n                                scale_factor=2,\n                                mode=\"bilinear\",\n                                align_corners=False,\n                            )",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "save_scalars",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def save_scalars(logger, mode, scalar_dict, global_step):\n    scalar_dict = tensor2float(scalar_dict)\n    for key, value in scalar_dict.items():\n        if not isinstance(value, (list, tuple)):\n            name = '{}/{}'.format(mode, key)\n            logger.add_scalar(name, value, global_step)\n        else:\n            for idx in range(len(value)):\n                name = '{}/{}_{}'.format(mode, key, idx)\n                logger.add_scalar(name, value[idx], global_step)",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "save_images",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def save_images(logger, mode, images_dict, global_step):\n    images_dict = tensor2numpy(images_dict)\n    def preprocess(name, img):\n        if not (len(img.shape) == 3 or len(img.shape) == 4):\n            raise NotImplementedError(\"invalid img shape {}:{} in save_images\".format(name, img.shape))\n        if len(img.shape) == 3:\n            normalizer = mpl.colors.Normalize(vmin=img[0].min(), vmax=img[0].max())\n            mapper = cm.ScalarMappable(norm=normalizer, cmap='jet')\n            cimg = (mapper.to_rgba(img[0])[:,:,:3] *255).astype(np.uint8).transpose(2,0,1)\n            cimg = torch.from_numpy(cimg)",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "savepreprocess",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def savepreprocess(img, ifRgb=False):\n    if not ifRgb:\n        normalizer = mpl.colors.Normalize(vmin=img[0].min(), vmax=img[0].max())\n        mapper = cm.ScalarMappable(norm=normalizer, cmap='jet')\n        cimg = (mapper.to_rgba(img[0])[:, :, :3] * 255).astype(np.uint8)\n        return cimg\n    else:\n        channel_max = np.max(img[0], axis=(1, 2), keepdims=True)\n        channel_min = np.min(img[0], axis=(1, 2), keepdims=True)\n        cimg = (img[0] - channel_min) / (channel_max - channel_min)",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "compute_metrics_for_each_image",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def compute_metrics_for_each_image(metric_func):\n    def wrapper(depth_est, depth_gt, mask, *args):\n        batch_size = depth_gt.shape[0]\n        results = []\n        # compute result one by one\n        for idx in range(batch_size):\n            ret = metric_func(depth_est[idx], depth_gt[idx], mask[idx], *args)\n            results.append(ret)\n        return torch.stack(results).mean()\n    return wrapper",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "D1_metric",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def D1_metric(D_est, D_gt, mask):\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    E = torch.abs(D_gt - D_est)\n    err_mask = (E > 3) & (E / D_gt.abs() > 0.05)\n    return torch.mean(err_mask.float())\n@make_nograd_func\n@compute_metrics_for_each_image\ndef Thres_metric(D_est, D_gt, mask, thres):\n    assert isinstance(thres, (int, float))\n    D_est, D_gt = D_est[mask], D_gt[mask]",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "Thres_metric",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def Thres_metric(D_est, D_gt, mask, thres):\n    assert isinstance(thres, (int, float))\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    E = torch.abs(D_gt - D_est)\n    err_mask = E > thres\n    return torch.mean(err_mask.float())\n# NOTE: please do not use this to build up training loss\n@make_nograd_func\n@compute_metrics_for_each_image\ndef EPE_metric(D_est, D_gt, mask):",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "EPE_metric",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def EPE_metric(D_est, D_gt, mask):\n    D_est, D_gt = D_est[mask], D_gt[mask]\n    return F.l1_loss(D_est, D_gt, size_average=True)\nimport torch.distributed as dist\ndef synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "synchronize",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def get_world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\ndef reduce_scalar_outputs(scalar_outputs):\n    world_size = get_world_size()\n    if world_size < 2:\n        return scalar_outputs",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "reduce_scalar_outputs",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def reduce_scalar_outputs(scalar_outputs):\n    world_size = get_world_size()\n    if world_size < 2:\n        return scalar_outputs\n    with torch.no_grad():\n        names = []\n        scalars = []\n        for k in sorted(scalar_outputs.keys()):\n            names.append(k)\n            scalars.append(scalar_outputs[k])",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "adjust_learning_rate",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def adjust_learning_rate(optimizer, epoch, base_lr, lrepochs):\n    splits = lrepochs.split(':')\n    assert len(splits) == 2\n    # parse the epochs to downscale the learning rate (before :)\n    downscale_epochs = [int(eid_str) for eid_str in splits[0].split(',')]\n    # parse downscale rate (after :)\n    downscale_rates = [float(eid_str) for eid_str in splits[1].split(',')]\n    print(\"downscale epochs: {}, downscale rate: {}\".format(downscale_epochs, downscale_rates))\n    lr = base_lr\n    for eid, downscale_rate in zip(downscale_epochs, downscale_rates):",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "set_random_seed",
        "kind": 2,
        "importPath": "stereo.modeling.models.iinet.utils",
        "description": "stereo.modeling.models.iinet.utils",
        "peekOfCode": "def set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)",
        "detail": "stereo.modeling.models.iinet.utils",
        "documentation": {}
    },
    {
        "label": "Aggregation",
        "kind": 6,
        "importPath": "stereo.modeling.models.lightstereo.aggregation",
        "description": "stereo.modeling.models.lightstereo.aggregation",
        "peekOfCode": "class Aggregation(nn.Module):\n    def __init__(self, in_channels, left_att, blocks, expanse_ratio, backbone_channels):\n        super(Aggregation, self).__init__()\n        self.left_att = left_att\n        self.expanse_ratio = expanse_ratio\n        conv0 = [MobileV2Residual(in_channels, in_channels, stride=1, expanse_ratio=self.expanse_ratio)\n                 for i in range(blocks[0])]\n        self.conv0 = nn.Sequential(*conv0)\n        self.conv1 = MobileV2Residual(in_channels, in_channels * 2, stride=2, expanse_ratio=self.expanse_ratio)\n        conv2_add = [MobileV2Residual(in_channels * 2, in_channels * 2, stride=1, expanse_ratio=self.expanse_ratio)",
        "detail": "stereo.modeling.models.lightstereo.aggregation",
        "documentation": {}
    },
    {
        "label": "MobileV2Residual",
        "kind": 6,
        "importPath": "stereo.modeling.models.lightstereo.aggregation",
        "description": "stereo.modeling.models.lightstereo.aggregation",
        "peekOfCode": "class MobileV2Residual(nn.Module):\n    def __init__(self, inp, oup, stride, expanse_ratio, dilation=1):\n        super(MobileV2Residual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n        hidden_dim = int(inp * expanse_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n        pad = dilation\n        # v2\n        self.pwconv = nn.Sequential(",
        "detail": "stereo.modeling.models.lightstereo.aggregation",
        "documentation": {}
    },
    {
        "label": "AttentionModule",
        "kind": 6,
        "importPath": "stereo.modeling.models.lightstereo.aggregation",
        "description": "stereo.modeling.models.lightstereo.aggregation",
        "peekOfCode": "class AttentionModule(nn.Module):\n    def __init__(self, dim, img_feat_dim):\n        super().__init__()\n        self.conv0 = nn.Conv2d(img_feat_dim, dim, 1)\n        self.conv0_1 = nn.Conv2d(dim, dim, (1, 7), padding=(0, 3), groups=dim)\n        self.conv0_2 = nn.Conv2d(dim, dim, (7, 1), padding=(3, 0), groups=dim)\n        self.conv1_1 = nn.Conv2d(dim, dim, (1, 11), padding=(0, 5), groups=dim)\n        self.conv1_2 = nn.Conv2d(dim, dim, (11, 1), padding=(5, 0), groups=dim)\n        self.conv2_1 = nn.Conv2d(dim, dim, (1, 21), padding=(0, 10), groups=dim)\n        self.conv2_2 = nn.Conv2d(dim, dim, (21, 1), padding=(10, 0), groups=dim)",
        "detail": "stereo.modeling.models.lightstereo.aggregation",
        "documentation": {}
    },
    {
        "label": "FPNLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.lightstereo.backbone",
        "description": "stereo.modeling.models.lightstereo.backbone",
        "peekOfCode": "class FPNLayer(nn.Module):\n    def __init__(self, chan_low, chan_high):\n        super().__init__()\n        self.deconv = BasicDeconv2d(chan_low, chan_high, kernel_size=4, stride=2, padding=1,\n                                    norm_layer=nn.BatchNorm2d,\n                                    act_layer=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True))\n        self.conv = BasicConv2d(chan_high * 2, chan_high, kernel_size=3, padding=1,\n                                norm_layer=nn.BatchNorm2d,\n                                act_layer=partial(nn.LeakyReLU, negative_slope=0.2, inplace=True))\n    def forward(self, low, high):",
        "detail": "stereo.modeling.models.lightstereo.backbone",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "stereo.modeling.models.lightstereo.backbone",
        "description": "stereo.modeling.models.lightstereo.backbone",
        "peekOfCode": "class Backbone(nn.Module):\n    def __init__(self, backbone='MobileNetv2'):\n        super().__init__()\n        if backbone == 'MobileNetv2':\n            model = timm.create_model('mobilenetv2_100', pretrained=True, features_only=True)\n            channels = [160, 96, 32, 24]\n        elif backbone == 'EfficientNetv2':\n            model = timm.create_model('efficientnetv2_rw_s', pretrained=True, features_only=True)\n            channels = [272, 160, 64, 48]\n        else:",
        "detail": "stereo.modeling.models.lightstereo.backbone",
        "documentation": {}
    },
    {
        "label": "LightStereo",
        "kind": 6,
        "importPath": "stereo.modeling.models.lightstereo.lightstereo",
        "description": "stereo.modeling.models.lightstereo.lightstereo",
        "peekOfCode": "class LightStereo(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.max_disp = cfgs.MAX_DISP\n        self.left_att = cfgs.LEFT_ATT\n        # backbobe\n        self.backbone = Backbone(cfgs.get('BACKCONE', 'MobileNetv2'))\n        # aggregation\n        self.cost_agg = Aggregation(in_channels=48,\n                                    left_att=self.left_att,",
        "detail": "stereo.modeling.models.lightstereo.lightstereo",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.lightstereo.trainer",
        "description": "stereo.modeling.models.lightstereo.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.lightstereo.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.lightstereo.trainer",
        "description": "stereo.modeling.models.lightstereo.trainer",
        "peekOfCode": "__all__ = {\n    'LightStereo': LightStereo,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.lightstereo.trainer",
        "documentation": {}
    },
    {
        "label": "MobileV2Residual",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "description": "stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "peekOfCode": "class MobileV2Residual(nn.Module):\n    def __init__(self, inp, oup, stride, expanse_ratio, dilation=1):\n        super(MobileV2Residual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n        hidden_dim = int(inp * expanse_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n        pad = dilation\n        if expanse_ratio == 1:\n            self.conv = nn.Sequential(",
        "detail": "stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "documentation": {}
    },
    {
        "label": "Hourglass2D",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "description": "stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "peekOfCode": "class Hourglass2D(nn.Module):\n    def __init__(self, in_channels):\n        super(Hourglass2D, self).__init__()\n        self.expanse_ratio = 2\n        self.conv1 = MobileV2Residual(in_channels, in_channels * 2, stride=2, expanse_ratio=self.expanse_ratio)\n        self.conv2 = MobileV2Residual(in_channels * 2, in_channels * 2, stride=1, expanse_ratio=self.expanse_ratio)\n        self.conv3 = MobileV2Residual(in_channels * 2, in_channels * 4, stride=2, expanse_ratio=self.expanse_ratio)\n        self.conv4 = MobileV2Residual(in_channels * 4, in_channels * 4, stride=1, expanse_ratio=self.expanse_ratio)\n        self.conv5 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels * 4, in_channels * 2, 3, padding=1, output_padding=1, stride=2, bias=False),",
        "detail": "stereo.modeling.models.msnet.msnet_utils.msnet_blocks",
        "documentation": {}
    },
    {
        "label": "hourglass2D",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.MSNet2D",
        "description": "stereo.modeling.models.msnet.MSNet2D",
        "peekOfCode": "class hourglass2D(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass2D, self).__init__()\n        self.expanse_ratio = 2\n        self.conv1 = MobileV2_Residual(in_channels, in_channels * 2, stride=2, expanse_ratio=self.expanse_ratio)\n        self.conv2 = MobileV2_Residual(in_channels * 2, in_channels * 2, stride=1, expanse_ratio=self.expanse_ratio)\n        self.conv3 = MobileV2_Residual(in_channels * 2, in_channels * 4, stride=2, expanse_ratio=self.expanse_ratio)\n        self.conv4 = MobileV2_Residual(in_channels * 4, in_channels * 4, stride=1, expanse_ratio=self.expanse_ratio)\n        self.conv5 = nn.Sequential(\n            nn.ConvTranspose2d(in_channels * 4, in_channels * 2, 3, padding=1, output_padding=1, stride=2, bias=False),",
        "detail": "stereo.modeling.models.msnet.MSNet2D",
        "documentation": {}
    },
    {
        "label": "MSNet2D",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.MSNet2D",
        "description": "stereo.modeling.models.msnet.MSNet2D",
        "peekOfCode": "class MSNet2D(nn.Module):\n    def __init__(self, cfgs):\n        super(MSNet2D, self).__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.num_groups = 1\n        self.volume_size = 48\n        self.hg_size = 48\n        self.dres_expanse_ratio = 3\n        self.feature_extraction = feature_extraction(add_relus=True)\n        self.preconv11 = nn.Sequential(convbn(320, 256, 1, 1, 0, 1),",
        "detail": "stereo.modeling.models.msnet.MSNet2D",
        "documentation": {}
    },
    {
        "label": "hourglass3D",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.MSNet3D",
        "description": "stereo.modeling.models.msnet.MSNet3D",
        "peekOfCode": "class hourglass3D(nn.Module):\n    def __init__(self, in_channels):\n        super(hourglass3D, self).__init__()\n        self.expanse_ratio = 2\n        self.conv1 = MobileV2_Residual_3D(in_channels, in_channels * 2, 2, self.expanse_ratio)\n        self.conv2 = MobileV2_Residual_3D(in_channels * 2, in_channels * 2, 1, self.expanse_ratio)\n        self.conv3 = MobileV2_Residual_3D(in_channels * 2, in_channels * 4, 2, self.expanse_ratio)\n        self.conv4 = MobileV2_Residual_3D(in_channels * 4, in_channels * 4, 1, self.expanse_ratio)\n        self.conv5 = nn.Sequential(\n            nn.ConvTranspose3d(in_channels * 4, in_channels * 2, 3, padding=1, output_padding=1, stride=2, bias=False),",
        "detail": "stereo.modeling.models.msnet.MSNet3D",
        "documentation": {}
    },
    {
        "label": "MSNet3D",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.MSNet3D",
        "description": "stereo.modeling.models.msnet.MSNet3D",
        "peekOfCode": "class MSNet3D(nn.Module):\n    def __init__(self, cfgs):\n        super(MSNet3D, self).__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.hourglass_size = 32\n        self.dres_expanse_ratio = 3\n        self.num_groups = 40\n        self.feature_extraction = feature_extraction()\n        self.dres0 = nn.Sequential(\n            MobileV2_Residual_3D(self.num_groups, self.hourglass_size, 1, self.dres_expanse_ratio),",
        "detail": "stereo.modeling.models.msnet.MSNet3D",
        "documentation": {}
    },
    {
        "label": "MobileV1_Residual",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "class MobileV1_Residual(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super(MobileV1_Residual, self).__init__()\n        self.stride = stride\n        self.downsample = downsample\n        self.conv1 = convbn_dws(inplanes, planes, 3, stride, pad, dilation)\n        self.conv2 = convbn_dws(planes, planes, 3, 1, pad, dilation, second_relu=False)\n    def forward(self, x):\n        out = self.conv1(x)",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "MobileV2_Residual",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "class MobileV2_Residual(nn.Module):\n    def __init__(self, inp, oup, stride, expanse_ratio, dilation=1):\n        super(MobileV2_Residual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n        hidden_dim = int(inp * expanse_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n        pad = dilation\n        if expanse_ratio == 1:\n            self.conv = nn.Sequential(",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "MobileV2_Residual_3D",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "class MobileV2_Residual_3D(nn.Module):\n    def __init__(self, inp, oup, stride, expanse_ratio):\n        super(MobileV2_Residual_3D, self).__init__()\n        self.stride = stride\n        hidden_dim = round(inp * expanse_ratio)\n        self.use_res_connect = self.stride == (1, 1, 1) and inp == oup\n        if expanse_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv3d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "feature_extraction",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "class feature_extraction(nn.Module):\n    def __init__(self, add_relus=False):\n        super(feature_extraction, self).__init__()\n        self.expanse_ratio = 3\n        self.inplanes = 32\n        if add_relus:\n            self.firstconv = nn.Sequential(MobileV2_Residual(3, 32, 2, self.expanse_ratio),\n                                           nn.ReLU(inplace=True),\n                                           MobileV2_Residual(32, 32, 1, self.expanse_ratio),\n                                           nn.ReLU(inplace=True),",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn",
        "kind": 2,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def convbn(in_channels, out_channels, kernel_size, stride, pad, dilation):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                  padding=dilation if dilation > 1 else pad, dilation=dilation, bias=False),\n        nn.BatchNorm2d(out_channels)\n    )\ndef convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(\n        nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                  padding=pad, bias=False),",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_3d",
        "kind": 2,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def convbn_3d(in_channels, out_channels, kernel_size, stride, pad):\n    return nn.Sequential(\n        nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n                  padding=pad, bias=False),\n        nn.BatchNorm3d(out_channels)\n    )\ndef convbn_dws(inp, oup, kernel_size, stride, pad, dilation, second_relu=True):\n    if second_relu:\n        return nn.Sequential(\n            # dw",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "convbn_dws",
        "kind": 2,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def convbn_dws(inp, oup, kernel_size, stride, pad, dilation, second_relu=True):\n    if second_relu:\n        return nn.Sequential(\n            # dw\n            nn.Conv2d(inp, inp, kernel_size=kernel_size, stride=stride, padding=dilation if dilation > 1 else pad,\n                      dilation=dilation, groups=inp, bias=False),\n            nn.BatchNorm2d(inp),\n            nn.ReLU6(inplace=True),\n            # pw\n            nn.Conv2d(inp, oup, 1, 1, 0, bias=False),",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "interweave_tensors",
        "kind": 2,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def interweave_tensors(refimg_fea, targetimg_fea):\n    B, C, H, W = refimg_fea.shape\n    interwoven_features = refimg_fea.new_zeros([B, 2 * C, H, W])\n    interwoven_features[:, ::2, :, :] = refimg_fea\n    interwoven_features[:, 1::2, :, :] = targetimg_fea\n    interwoven_features = interwoven_features.contiguous()\n    return interwoven_features\ndef groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "build_gwc_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def build_gwc_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "disparity_regression",
        "kind": 2,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def disparity_regression(x, maxdisp):\n    assert len(x.shape) == 4\n    disp_values = torch.arange(0, maxdisp, dtype=x.dtype, device=x.device)\n    disp_values = disp_values.view(1, maxdisp, 1, 1)\n    return torch.sum(x * disp_values, 1, keepdim=False)\n###############################################################################\n\"\"\" Loss Function \"\"\"\n###############################################################################\ndef model_loss(disp_ests, disp_gt, mask):\n    weights = [0.5, 0.5, 0.7, 1.0]",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "model_loss",
        "kind": 2,
        "importPath": "stereo.modeling.models.msnet.submodule",
        "description": "stereo.modeling.models.msnet.submodule",
        "peekOfCode": "def model_loss(disp_ests, disp_gt, mask):\n    weights = [0.5, 0.5, 0.7, 1.0]\n    all_losses = []\n    for disp_est, weight in zip(disp_ests, weights):\n        all_losses.append(weight * F.smooth_l1_loss(disp_est[mask], disp_gt[mask], reduction='mean'))\n    return sum(all_losses)",
        "detail": "stereo.modeling.models.msnet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.msnet.trainer",
        "description": "stereo.modeling.models.msnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.msnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.msnet.trainer",
        "description": "stereo.modeling.models.msnet.trainer",
        "peekOfCode": "__all__ = {\n    'MSNet2D': MSNet2D,\n    'MSNet3D': MSNet3D\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.msnet.trainer",
        "documentation": {}
    },
    {
        "label": "CfgNode",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.config.config",
        "description": "stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "class CfgNode(_CfgNode):\n    \"\"\"\n    Our extended version of :class:`yacs.config.CfgNode`.\n    It contains the following extra features:\n    1. The :meth:`merge_from_file` method supports the \"__BASE__\" key,\n       which allows the new CfgNode to inherit all the attributes from the\n       base configuration file(s).\n    2. Keys that start with \"COMPUTED_\" are treated as insertion-only\n       \"computed\" attributes. They can be inserted regardless of whether\n       the CfgNode is frozen or not.",
        "detail": "stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "get_cfg",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.config.config",
        "description": "stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "def get_cfg() -> CfgNode:\n    \"\"\"\n    Get a copy of the default config.\n    Returns:\n        a CfgNode instance.\n    \"\"\"\n    from .default import _CN\n    return _CN.clone()\ndef set_global_cfg(cfg: CfgNode) -> None:\n    \"\"\"",
        "detail": "stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "set_global_cfg",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.config.config",
        "description": "stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "def set_global_cfg(cfg: CfgNode) -> None:\n    \"\"\"\n    Let the global config point to the given cfg.\n    Assume that the given \"cfg\" has the key \"KEY\", after calling\n    `set_global_cfg(cfg)`, the key can be accessed by:\n    ::\n        from nmrf.config import global_cfg\n        print(global_cfg.KEY)\n    By using a hacky global config, you can access these config anywhere,\n    without having to pass the config object or the values deep into the code.",
        "detail": "stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "configurable",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.config.config",
        "description": "stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "def configurable(init_func=None, *, from_config=None):\n    \"\"\"\n    Decorate a function or a class's __init__ method so that it can be called\n    with a :class:`CfgNode` object using a :func:`from_config` function that translates\n    :class:`CfgNode` to arguments.\n    Examples:\n    ::\n        # Usage 1: Decorate on __init__:\n        class A:\n            @configurable",
        "detail": "stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "BASE_KEY",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.config",
        "description": "stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "BASE_KEY = \"__BASE__\"\nclass CfgNode(_CfgNode):\n    \"\"\"\n    Our extended version of :class:`yacs.config.CfgNode`.\n    It contains the following extra features:\n    1. The :meth:`merge_from_file` method supports the \"__BASE__\" key,\n       which allows the new CfgNode to inherit all the attributes from the\n       base configuration file(s).\n    2. Keys that start with \"COMPUTED_\" are treated as insertion-only\n       \"computed\" attributes. They can be inserted regardless of whether",
        "detail": "stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "global_cfg",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.config",
        "description": "stereo.modeling.models.nmrf.config.config",
        "peekOfCode": "global_cfg = CfgNode()\ndef get_cfg() -> CfgNode:\n    \"\"\"\n    Get a copy of the default config.\n    Returns:\n        a CfgNode instance.\n    \"\"\"\n    from .default import _CN\n    return _CN.clone()\ndef set_global_cfg(cfg: CfgNode) -> None:",
        "detail": "stereo.modeling.models.nmrf.config.config",
        "documentation": {}
    },
    {
        "label": "_CN",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN = CN()\n# The version number, to upgrade from old config to new ones if any\n# changes happen. It's recommended to keep a VERSION in your config file.\n_CN.VERSION = 2\n# ---------------------------------------------------------------------------- #\n# Model\n# ---------------------------------------------------------------------------- #\n_CN.BACKBONE = CN()\n_CN.BACKBONE.MODEL_TYPE = \"resnet\"\n_CN.BACKBONE.NORM_FN = \"instance\"",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.VERSION",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.VERSION = 2\n# ---------------------------------------------------------------------------- #\n# Model\n# ---------------------------------------------------------------------------- #\n_CN.BACKBONE = CN()\n_CN.BACKBONE.MODEL_TYPE = \"resnet\"\n_CN.BACKBONE.NORM_FN = \"instance\"\n_CN.BACKBONE.OUT_CHANNELS = 256\n_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE = CN()\n_CN.BACKBONE.MODEL_TYPE = \"resnet\"\n_CN.BACKBONE.NORM_FN = \"instance\"\n_CN.BACKBONE.OUT_CHANNELS = 256\n_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.MODEL_TYPE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.MODEL_TYPE = \"resnet\"\n_CN.BACKBONE.NORM_FN = \"instance\"\n_CN.BACKBONE.OUT_CHANNELS = 256\n_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.NORM_FN",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.NORM_FN = \"instance\"\n_CN.BACKBONE.OUT_CHANNELS = 256\n_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.OUT_CHANNELS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.OUT_CHANNELS = 256\n_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.WEIGHT_URL",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.WEIGHT_URL = \"\"\n_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.DROP_PATH",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.DROP_PATH = 0.0\n_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.BACKBONE.COMPAT",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.BACKBONE.COMPAT = True\n_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DPN",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DPN = CN()\n_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DPN.MAX_DISP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DPN.MAX_DISP = 320\n_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DPN.COST_GROUP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DPN.COST_GROUP = 4\n_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DPN.NUM_PROPOSALS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DPN.NUM_PROPOSALS = 4\n_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DPN.CONTEXT_DIM",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DPN.CONTEXT_DIM = 64\n_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP = CN()\n_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.PROP_EMBED_DIM",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.PROP_EMBED_DIM = 128\n_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.INFER_EMBED_DIM",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.INFER_EMBED_DIM = 128\n_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.MLP_RATIO",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.MLP_RATIO = 4\n_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.SPLIT_SIZE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.SPLIT_SIZE = 1\n_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.WINDOW_SIZE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.WINDOW_SIZE = 6\n_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.REFINE_WINDOW_SIZE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.REFINE_WINDOW_SIZE = 4\n_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.PROP_N_HEADS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.PROP_N_HEADS = 4\n_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.INFER_N_HEADS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.INFER_N_HEADS = 4\n_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.NUM_PROP_LAYERS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.NUM_PROP_LAYERS = 5\n_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.NUM_INFER_LAYERS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.NUM_INFER_LAYERS = 5\n_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.NUM_REFINE_LAYERS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.NUM_REFINE_LAYERS = 5\n_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.RETURN_INTERMEDIATE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.RETURN_INTERMEDIATE = True\n_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.ATTN_DROP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.ATTN_DROP = 0.0\n_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.PROJ_DROP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.PROJ_DROP = 0.0\n_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()\n# List of dataset name for training",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.DROP_PATH",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.DROP_PATH = 0.0\n_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()\n# List of dataset name for training\n_CN.DATASETS.TRAIN = [\"sceneflow\"]",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.DROPOUT",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.DROPOUT = 0.0\n_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()\n# List of dataset name for training\n_CN.DATASETS.TRAIN = [\"sceneflow\"]\n# List of dataset name for testing",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.NORMALIZE_BEFORE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.NORMALIZE_BEFORE = True\n_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()\n# List of dataset name for training\n_CN.DATASETS.TRAIN = [\"sceneflow\"]\n# List of dataset name for testing\n_CN.DATASETS.TEST = [\"things\"]",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.NMP.WITH_REFINEMENT",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.NMP.WITH_REFINEMENT = True\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATASETS = CN()\n# List of dataset name for training\n_CN.DATASETS.TRAIN = [\"sceneflow\"]\n# List of dataset name for testing\n_CN.DATASETS.TEST = [\"things\"]\n# Image gamma",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS = CN()\n# List of dataset name for training\n_CN.DATASETS.TRAIN = [\"sceneflow\"]\n# List of dataset name for testing\n_CN.DATASETS.TEST = [\"things\"]\n# Image gamma\n_CN.DATASETS.IMG_GAMMA = None\n# Color saturation\n_CN.DATASETS.SATURATION_RANGE = [0, 1.4]\n# Flip the images horizontally or vertically, valid choice [False, 'h', 'v']",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.TRAIN",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.TRAIN = [\"sceneflow\"]\n# List of dataset name for testing\n_CN.DATASETS.TEST = [\"things\"]\n# Image gamma\n_CN.DATASETS.IMG_GAMMA = None\n# Color saturation\n_CN.DATASETS.SATURATION_RANGE = [0, 1.4]\n# Flip the images horizontally or vertically, valid choice [False, 'h', 'v']\n_CN.DATASETS.DO_FLIP = False\n# Re-scale the image randomly",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.TEST",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.TEST = [\"things\"]\n# Image gamma\n_CN.DATASETS.IMG_GAMMA = None\n# Color saturation\n_CN.DATASETS.SATURATION_RANGE = [0, 1.4]\n# Flip the images horizontally or vertically, valid choice [False, 'h', 'v']\n_CN.DATASETS.DO_FLIP = False\n# Re-scale the image randomly\n_CN.DATASETS.SPATIAL_SCALE = [-0.2, 0.4]\n# Simulate imperfect rectification",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.IMG_GAMMA",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.IMG_GAMMA = None\n# Color saturation\n_CN.DATASETS.SATURATION_RANGE = [0, 1.4]\n# Flip the images horizontally or vertically, valid choice [False, 'h', 'v']\n_CN.DATASETS.DO_FLIP = False\n# Re-scale the image randomly\n_CN.DATASETS.SPATIAL_SCALE = [-0.2, 0.4]\n# Simulate imperfect rectification\n_CN.DATASETS.YJITTER = False\n# Image size for training",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.SATURATION_RANGE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.SATURATION_RANGE = [0, 1.4]\n# Flip the images horizontally or vertically, valid choice [False, 'h', 'v']\n_CN.DATASETS.DO_FLIP = False\n# Re-scale the image randomly\n_CN.DATASETS.SPATIAL_SCALE = [-0.2, 0.4]\n# Simulate imperfect rectification\n_CN.DATASETS.YJITTER = False\n# Image size for training\n_CN.DATASETS.CROP_SIZE = [384, 768]\n_CN.DATASETS.DIVIS_BY = 8",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.DO_FLIP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.DO_FLIP = False\n# Re-scale the image randomly\n_CN.DATASETS.SPATIAL_SCALE = [-0.2, 0.4]\n# Simulate imperfect rectification\n_CN.DATASETS.YJITTER = False\n# Image size for training\n_CN.DATASETS.CROP_SIZE = [384, 768]\n_CN.DATASETS.DIVIS_BY = 8\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.SPATIAL_SCALE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.SPATIAL_SCALE = [-0.2, 0.4]\n# Simulate imperfect rectification\n_CN.DATASETS.YJITTER = False\n# Image size for training\n_CN.DATASETS.CROP_SIZE = [384, 768]\n_CN.DATASETS.DIVIS_BY = 8\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATALOADER = CN()",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.YJITTER",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.YJITTER = False\n# Image size for training\n_CN.DATASETS.CROP_SIZE = [384, 768]\n_CN.DATASETS.DIVIS_BY = 8\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATALOADER = CN()\n# Number of data loading threads\n_CN.DATALOADER.NUM_WORKERS = 0",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.CROP_SIZE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.CROP_SIZE = [384, 768]\n_CN.DATASETS.DIVIS_BY = 8\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATALOADER = CN()\n# Number of data loading threads\n_CN.DATALOADER.NUM_WORKERS = 0\n# ---------------------------------------------------------------------------- #\n# Solver",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATASETS.DIVIS_BY",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATASETS.DIVIS_BY = 8\n# ---------------------------------------------------------------------------- #\n# Dataset and data augmentation\n# ---------------------------------------------------------------------------- #\n_CN.DATALOADER = CN()\n# Number of data loading threads\n_CN.DATALOADER.NUM_WORKERS = 0\n# ---------------------------------------------------------------------------- #\n# Solver\n# ---------------------------------------------------------------------------- #",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATALOADER",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATALOADER = CN()\n# Number of data loading threads\n_CN.DATALOADER.NUM_WORKERS = 0\n# ---------------------------------------------------------------------------- #\n# Solver\n# ---------------------------------------------------------------------------- #\n_CN.SOLVER = CN()\n_CN.SOLVER.MAX_ITER = 300000\n_CN.SOLVER.BASE_LR = 0.0005\n_CN.SOLVER.BASE_LR_END = 0.0",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.DATALOADER.NUM_WORKERS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.DATALOADER.NUM_WORKERS = 0\n# ---------------------------------------------------------------------------- #\n# Solver\n# ---------------------------------------------------------------------------- #\n_CN.SOLVER = CN()\n_CN.SOLVER.MAX_ITER = 300000\n_CN.SOLVER.BASE_LR = 0.0005\n_CN.SOLVER.BASE_LR_END = 0.0\n_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER = CN()\n_CN.SOLVER.MAX_ITER = 300000\n_CN.SOLVER.BASE_LR = 0.0005\n_CN.SOLVER.BASE_LR_END = 0.0\n_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.MAX_ITER",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.MAX_ITER = 300000\n_CN.SOLVER.BASE_LR = 0.0005\n_CN.SOLVER.BASE_LR_END = 0.0\n_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.BASE_LR",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.BASE_LR = 0.0005\n_CN.SOLVER.BASE_LR_END = 0.0\n_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.BASE_LR_END",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.BASE_LR_END = 0.0\n_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.BACKBONE_LR_DECAY",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.BACKBONE_LR_DECAY = 0.1\n_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.WEIGHT_DECAY",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.WEIGHT_DECAY = 0.00001\n# The weight decay that's applied to parameters of normalization layers\n# (typically the affine transformation)\n_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number\n# of training images per step (i.e. per iteration). If we use 16 GPUs",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.WEIGHT_DECAY_NORM",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.WEIGHT_DECAY_NORM = 0.00001\n_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number\n# of training images per step (i.e. per iteration). If we use 16 GPUs\n# and IMS_PER_BATCH = 32, each GPU will see 2 images per batch.\n_CN.SOLVER.IMS_PER_BATCH = 8\n# Gradient clipping",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.BACKBONE_WEIGHT_DECAY",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.BACKBONE_WEIGHT_DECAY = 0.00001\n# Save a checkpoint after every this number of iterations\n_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number\n# of training images per step (i.e. per iteration). If we use 16 GPUs\n# and IMS_PER_BATCH = 32, each GPU will see 2 images per batch.\n_CN.SOLVER.IMS_PER_BATCH = 8\n# Gradient clipping\n_CN.SOLVER.GRAD_CLIP = 1.0",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.CHECKPOINT_PERIOD",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.CHECKPOINT_PERIOD = 100000\n_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number\n# of training images per step (i.e. per iteration). If we use 16 GPUs\n# and IMS_PER_BATCH = 32, each GPU will see 2 images per batch.\n_CN.SOLVER.IMS_PER_BATCH = 8\n# Gradient clipping\n_CN.SOLVER.GRAD_CLIP = 1.0\n_CN.SOLVER.LOSS_WEIGHTS = [1.0, 1.0, 1.0, 1.4, 1.4, 1.4, 1.4, 1.6, 2.0, 2.0]\n# resume from pretrain model for finetuning or resuming from terminated training",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.LATEST_CHECKPOINT_PERIOD",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.LATEST_CHECKPOINT_PERIOD = 1000\n# Number of images per batch across all machines. This is also the number\n# of training images per step (i.e. per iteration). If we use 16 GPUs\n# and IMS_PER_BATCH = 32, each GPU will see 2 images per batch.\n_CN.SOLVER.IMS_PER_BATCH = 8\n# Gradient clipping\n_CN.SOLVER.GRAD_CLIP = 1.0\n_CN.SOLVER.LOSS_WEIGHTS = [1.0, 1.0, 1.0, 1.4, 1.4, 1.4, 1.4, 1.6, 2.0, 2.0]\n# resume from pretrain model for finetuning or resuming from terminated training\n_CN.SOLVER.RESUME = None",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.IMS_PER_BATCH",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.IMS_PER_BATCH = 8\n# Gradient clipping\n_CN.SOLVER.GRAD_CLIP = 1.0\n_CN.SOLVER.LOSS_WEIGHTS = [1.0, 1.0, 1.0, 1.4, 1.4, 1.4, 1.4, 1.6, 2.0, 2.0]\n# resume from pretrain model for finetuning or resuming from terminated training\n_CN.SOLVER.RESUME = None\n_CN.SOLVER.STRICT_RESUME = True\n_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.GRAD_CLIP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.GRAD_CLIP = 1.0\n_CN.SOLVER.LOSS_WEIGHTS = [1.0, 1.0, 1.0, 1.4, 1.4, 1.4, 1.4, 1.6, 2.0, 2.0]\n# resume from pretrain model for finetuning or resuming from terminated training\n_CN.SOLVER.RESUME = None\n_CN.SOLVER.STRICT_RESUME = True\n_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.LOSS_WEIGHTS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.LOSS_WEIGHTS = [1.0, 1.0, 1.0, 1.4, 1.4, 1.4, 1.4, 1.6, 2.0, 2.0]\n# resume from pretrain model for finetuning or resuming from terminated training\n_CN.SOLVER.RESUME = None\n_CN.SOLVER.STRICT_RESUME = True\n_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.RESUME",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.RESUME = None\n_CN.SOLVER.STRICT_RESUME = True\n_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement\n_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.STRICT_RESUME",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.STRICT_RESUME = True\n_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement\n_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #\n# Specific test options",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.NO_RESUME_OPTIMIZER",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.NO_RESUME_OPTIMIZER = False\n_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement\n_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #\n# Specific test options\n# ---------------------------------------------------------------------------- #",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.AUX_LOSS",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.AUX_LOSS = True\n# Maximum disparity for training,\n# ground truth disparities exceed than this threshold will be ignored for loss computation\n_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement\n_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #\n# Specific test options\n# ---------------------------------------------------------------------------- #\n_CN.TEST = CN()",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.MAX_DISP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.MAX_DISP = 192\n# Loss type used in cost aggregation and refinement\n_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #\n# Specific test options\n# ---------------------------------------------------------------------------- #\n_CN.TEST = CN()\n# The period (in terms of steps) to evaluate the model during training.\n# Set to 0 to disable.\n_CN.TEST.EVAL_PERIOD = 20000",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SOLVER.LOSS_TYPE",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SOLVER.LOSS_TYPE = \"L1\"\n# ---------------------------------------------------------------------------- #\n# Specific test options\n# ---------------------------------------------------------------------------- #\n_CN.TEST = CN()\n# The period (in terms of steps) to evaluate the model during training.\n# Set to 0 to disable.\n_CN.TEST.EVAL_PERIOD = 20000\n# Threshold for metric computation for testing\n_CN.TEST.EVAL_THRESH = [['1.0', '3.0']]",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST = CN()\n# The period (in terms of steps) to evaluate the model during training.\n# Set to 0 to disable.\n_CN.TEST.EVAL_PERIOD = 20000\n# Threshold for metric computation for testing\n_CN.TEST.EVAL_THRESH = [['1.0', '3.0']]\n# Maximum disparity for metric computation mask\n_CN.TEST.EVAL_MAX_DISP = [192]\n# Whether use only valid pixels in evaluation\n_CN.TEST.EVAL_ONLY_VALID = [True]",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST.EVAL_PERIOD",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST.EVAL_PERIOD = 20000\n# Threshold for metric computation for testing\n_CN.TEST.EVAL_THRESH = [['1.0', '3.0']]\n# Maximum disparity for metric computation mask\n_CN.TEST.EVAL_MAX_DISP = [192]\n# Whether use only valid pixels in evaluation\n_CN.TEST.EVAL_ONLY_VALID = [True]\n# Whether evaluate disparity proposal\n_CN.TEST.EVAL_PROP = [True]\n# ---------------------------------------------------------------------------- #",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST.EVAL_THRESH",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST.EVAL_THRESH = [['1.0', '3.0']]\n# Maximum disparity for metric computation mask\n_CN.TEST.EVAL_MAX_DISP = [192]\n# Whether use only valid pixels in evaluation\n_CN.TEST.EVAL_ONLY_VALID = [True]\n# Whether evaluate disparity proposal\n_CN.TEST.EVAL_PROP = [True]\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST.EVAL_MAX_DISP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST.EVAL_MAX_DISP = [192]\n# Whether use only valid pixels in evaluation\n_CN.TEST.EVAL_ONLY_VALID = [True]\n# Whether evaluate disparity proposal\n_CN.TEST.EVAL_PROP = [True]\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Set seed to negative to fully randomize everything.\n# Set seed to positive to use a fixed seed. Note that a fixed seed increases",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST.EVAL_ONLY_VALID",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST.EVAL_ONLY_VALID = [True]\n# Whether evaluate disparity proposal\n_CN.TEST.EVAL_PROP = [True]\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Set seed to negative to fully randomize everything.\n# Set seed to positive to use a fixed seed. Note that a fixed seed increases\n# reproducibility but does not guarantee fully deterministic behavior.\n# Disabling all parallelism further increases reproducibility.",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.TEST.EVAL_PROP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.TEST.EVAL_PROP = [True]\n# ---------------------------------------------------------------------------- #\n# Misc options\n# ---------------------------------------------------------------------------- #\n# Set seed to negative to fully randomize everything.\n# Set seed to positive to use a fixed seed. Note that a fixed seed increases\n# reproducibility but does not guarantee fully deterministic behavior.\n# Disabling all parallelism further increases reproducibility.\n_CN.SEED = 326\n# Benchmark different cudnn algorithms.",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.SEED",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.SEED = 326\n# Benchmark different cudnn algorithms.\n# If input images have very different sizes, this option will have large overhead\n# for about 10k iterations. It usually hurts total time, but can benefit for certain models.\n# If input images have the same of similar sizes, benchmark is often helpful.\n_CN.CUDNN_BENCHMARK = True\n# global config is for quick hack purposes.\n# You can set them in command line or config files,\n# and access it with:\n#",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.CUDNN_BENCHMARK",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.CUDNN_BENCHMARK = True\n# global config is for quick hack purposes.\n# You can set them in command line or config files,\n# and access it with:\n#\n# from nmrf.config import global_cfg\n# print(global_cfg.HACK)\n#\n# Do not commit any configs into it.\n_CN.GLOBAL = CN()",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.GLOBAL",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.GLOBAL = CN()\n_CN.GLOBAL.HACK = 1.0",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_CN.GLOBAL.HACK",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.config.default",
        "description": "stereo.modeling.models.nmrf.config.default",
        "peekOfCode": "_CN.GLOBAL.HACK = 1.0",
        "detail": "stereo.modeling.models.nmrf.config.default",
        "documentation": {}
    },
    {
        "label": "_DownSample",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "description": "stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "peekOfCode": "class _DownSample(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, label, nms_thresh):\n        return MSDA.downsample_forward(input, label, nms_thresh)\n    @staticmethod\n    def backward(ctx, grad_output):\n        return None, None, None\ndownsample = _DownSample.apply",
        "detail": "stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "documentation": {}
    },
    {
        "label": "downsample",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "description": "stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "peekOfCode": "downsample = _DownSample.apply",
        "detail": "stereo.modeling.models.nmrf.ops.functions.downsample_func",
        "documentation": {}
    },
    {
        "label": "MSDeformAttnFunction",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "description": "stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "peekOfCode": "class MSDeformAttnFunction(Function):\n    @staticmethod\n    @custom_fwd(cast_inputs=torch.float32)\n    def forward(ctx, value, value_spatial_shapes, value_level_start_index,\n                sampling_locations, attention_weights, im2col_step):\n        ctx.im2col_step = im2col_step\n        output = MSDA.ms_deform_attn_forward(value, value_spatial_shapes,\n                                             value_level_start_index,\n                                             sampling_locations,\n                                             attention_weights,",
        "detail": "stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "documentation": {}
    },
    {
        "label": "ms_deform_attn_core_pytorch",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "description": "stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "peekOfCode": "def ms_deform_attn_core_pytorch(value, value_spatial_shapes,\n                                sampling_locations, attention_weights):\n    # for debug and test only,\n    # need to use cuda version instead\n    N_, S_, M_, D_ = value.shape\n    _, Lq_, M_, L_, P_, _ = sampling_locations.shape\n    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)\n    sampling_grids = 2 * sampling_locations - 1\n    sampling_value_list = []\n    for lid_, (H_, W_) in enumerate(value_spatial_shapes):",
        "detail": "stereo.modeling.models.nmrf.ops.functions.ms_deform_attn_func",
        "documentation": {}
    },
    {
        "label": "MSDeformAttn",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.ops.modules.ms_deform_attn",
        "description": "stereo.modeling.models.nmrf.ops.modules.ms_deform_attn",
        "peekOfCode": "class MSDeformAttn(nn.Module):\n    def __init__(self, d_model=256, n_levels=4, n_heads=8, n_points=4, ratio=1.0):\n        \"\"\"Multi-Scale Deformable Attention Module.\n        :param d_model      hidden dimension\n        :param n_levels     number of feature levels\n        :param n_heads      number of attention heads\n        :param n_points     number of sampling points per attention head per feature level\n        \"\"\"\n        super().__init__()\n        if d_model % n_heads != 0:",
        "detail": "stereo.modeling.models.nmrf.ops.modules.ms_deform_attn",
        "documentation": {}
    },
    {
        "label": "get_extensions",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.ops.setup",
        "description": "stereo.modeling.models.nmrf.ops.setup",
        "peekOfCode": "def get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, 'src')\n    main_file = glob.glob(os.path.join(extensions_dir, '*.cpp'))\n    source_cpu = glob.glob(os.path.join(extensions_dir, 'cpu', '*.cpp'))\n    source_cuda = glob.glob(os.path.join(extensions_dir, 'cuda', '*.cu'))\n    sources = main_file + source_cpu\n    extension = CppExtension\n    extra_compile_args = {'cxx': []}\n    define_macros = []",
        "detail": "stereo.modeling.models.nmrf.ops.setup",
        "documentation": {}
    },
    {
        "label": "requirements",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.ops.setup",
        "description": "stereo.modeling.models.nmrf.ops.setup",
        "peekOfCode": "requirements = ['torch', 'torchvision']\ndef get_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    extensions_dir = os.path.join(this_dir, 'src')\n    main_file = glob.glob(os.path.join(extensions_dir, '*.cpp'))\n    source_cpu = glob.glob(os.path.join(extensions_dir, 'cpu', '*.cpp'))\n    source_cuda = glob.glob(os.path.join(extensions_dir, 'cuda', '*.cu'))\n    sources = main_file + source_cpu\n    extension = CppExtension\n    extra_compile_args = {'cxx': []}",
        "detail": "stereo.modeling.models.nmrf.ops.setup",
        "documentation": {}
    },
    {
        "label": "check_forward_equal_with_pytorch_double",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.ops.test",
        "description": "stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "def check_forward_equal_with_pytorch_double():\n    value = torch.rand(N, S, M, D).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5\n    attention_weights /= attention_weights.sum(-1,\n                                               keepdim=True).sum(-2,\n                                                                 keepdim=True)\n    im2col_step = 2\n    output_pytorch = ms_deform_attn_core_pytorch(\n        value.double(), shapes, sampling_locations.double(),",
        "detail": "stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "check_forward_equal_with_pytorch_float",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.ops.test",
        "description": "stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "def check_forward_equal_with_pytorch_float():\n    value = torch.rand(N, S, M, D).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5\n    attention_weights /= attention_weights.sum(-1,\n                                               keepdim=True).sum(-2,\n                                                                 keepdim=True)\n    im2col_step = 2\n    output_pytorch = ms_deform_attn_core_pytorch(\n        value, shapes, sampling_locations, attention_weights).detach().cpu()",
        "detail": "stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "check_gradient_numerical",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.ops.test",
        "description": "stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "def check_gradient_numerical(channels=4,\n                             grad_value=True,\n                             grad_sampling_loc=True,\n                             grad_attn_weight=True):\n    value = torch.rand(N, S, M, channels).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5\n    attention_weights /= attention_weights.sum(-1,\n                                               keepdim=True).sum(-2,\n                                                                 keepdim=True)",
        "detail": "stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "shapes",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.ops.test",
        "description": "stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "shapes = torch.as_tensor([(6, 4), (3, 2)], dtype=torch.long).cuda()\nlevel_start_index = torch.cat((shapes.new_zeros(\n    (1, )), shapes.prod(1).cumsum(0)[:-1]))\nS = sum([(H * W).item() for H, W in shapes])\ntorch.manual_seed(3)\n@torch.no_grad()\ndef check_forward_equal_with_pytorch_double():\n    value = torch.rand(N, S, M, D).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5",
        "detail": "stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "level_start_index",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.ops.test",
        "description": "stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "level_start_index = torch.cat((shapes.new_zeros(\n    (1, )), shapes.prod(1).cumsum(0)[:-1]))\nS = sum([(H * W).item() for H, W in shapes])\ntorch.manual_seed(3)\n@torch.no_grad()\ndef check_forward_equal_with_pytorch_double():\n    value = torch.rand(N, S, M, D).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5\n    attention_weights /= attention_weights.sum(-1,",
        "detail": "stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "S",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.ops.test",
        "description": "stereo.modeling.models.nmrf.ops.test",
        "peekOfCode": "S = sum([(H * W).item() for H, W in shapes])\ntorch.manual_seed(3)\n@torch.no_grad()\ndef check_forward_equal_with_pytorch_double():\n    value = torch.rand(N, S, M, D).cuda() * 0.01\n    sampling_locations = torch.rand(N, Lq, M, L, P, 2).cuda()\n    attention_weights = torch.rand(N, Lq, M, L, P).cuda() + 1e-5\n    attention_weights /= attention_weights.sum(-1,\n                                               keepdim=True).sum(-2,\n                                                                 keepdim=True)",
        "detail": "stereo.modeling.models.nmrf.ops.test",
        "documentation": {}
    },
    {
        "label": "get_color_map",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.colorize",
        "description": "stereo.modeling.models.nmrf.utils.colorize",
        "peekOfCode": "def get_color_map():\n    return   np.array([[0.18995,0.07176,0.23217],\n                       [0.19483,0.08339,0.26149],\n                       [0.19956,0.09498,0.29024],\n                       [0.20415,0.10652,0.31844],\n                       [0.20860,0.11802,0.34607],\n                       [0.21291,0.12947,0.37314],\n                       [0.21708,0.14087,0.39964],\n                       [0.22111,0.15223,0.42558],\n                       [0.22500,0.16354,0.45096],",
        "detail": "stereo.modeling.models.nmrf.utils.colorize",
        "documentation": {}
    },
    {
        "label": "init_dist",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def init_dist(launcher, backend='nccl', **kwargs):\n    if mp.get_start_method(allow_none=True) is None:\n        mp.set_start_method('spawn')\n    if launcher == 'pytorch':\n        _init_dist_pytorch(backend, **kwargs)\n    elif launcher == 'mpi':\n        _init_dist_mpi(backend, **kwargs)\n    elif launcher == 'slurm':\n        _init_dist_slurm(backend, **kwargs)\n    else:",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "get_dist_info",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def get_dist_info():\n    if dist.is_available():\n        initialized = dist.is_initialized()\n    else:\n        initialized = False\n    if initialized:\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n    else:\n        rank = 0",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "setup_for_distributed",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def setup_for_distributed(is_master):\n    \"\"\"\n    This function disables printing when not in master process\n    \"\"\"\n    import builtins as __builtin__\n    builtin_print = __builtin__.print\n    def print(*args, **kwargs):\n        force = kwargs.pop('force', False)\n        if is_master or force:\n            builtin_print(*args, **kwargs)",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "is_dist_avail_and_initialized",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def is_dist_avail_and_initialized():\n    if not dist.is_available():\n        return False\n    if not dist.is_initialized():\n        return False\n    return True\ndef is_main_process() -> bool:\n    return get_dist_info()[0] == 0\ndef get_world_size() -> int:\n    return get_dist_info()[1]",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def is_main_process() -> bool:\n    return get_dist_info()[0] == 0\ndef get_world_size() -> int:\n    return get_dist_info()[1]\ndef get_rank() -> int:\n    return get_dist_info()[0]\ndef synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "get_world_size",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def get_world_size() -> int:\n    return get_dist_info()[1]\ndef get_rank() -> int:\n    return get_dist_info()[0]\ndef synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "get_rank",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def get_rank() -> int:\n    return get_dist_info()[0]\ndef synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "synchronize",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def synchronize():\n    \"\"\"\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    \"\"\"\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "gather",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def gather(data, dst=0, group=None):\n    \"\"\"\n    Run gather on arbitrary picklable data (not necessarily tensors).\n    Args:\n        data: any picklable object\n        dst (int): destination rank\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n    Returns:\n        list[data]: on dst, a list of data gathered from each rank. Otherwise,",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "create_local_process_group",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def create_local_process_group(num_workers_per_machine: int) -> None:\n    \"\"\"\n    Create a process group that contains ranks within the same machine.\n    launch() will call this function. If you start\n    workers without launch(), you will have to also call this. Otherwise utilities\n    like `get_local_rank()` will not work.\n    This function contains a barrier. All processes must call it together.\n    Args:\n        num_workers_per_machine: the number of worker processes per machine. Typically\n            the number of GPUs.",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "get_local_rank",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "def get_local_rank() -> int:\n    \"\"\"\n    Returns:\n        the rank of the current process within the local (per-machine) process group.\n    \"\"\"\n    if not is_dist_avail_and_initialized():\n        return 0\n    assert _LOCAL_PROCESS_GROUP is not None, _MISSING_LOCAL_PG_ERROR\n    return dist.get_rank(group=_LOCAL_PROCESS_GROUP)",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "_LOCAL_PROCESS_GROUP",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "_LOCAL_PROCESS_GROUP = None\n_MISSING_LOCAL_PG_ERROR = (\n    \"Local process group is not yet created! Please use `launch()` \"\n    \"to start processes and initialize pytorch process group. If you need to start \"\n    \"processes in other ways, please call create_local_process_group(\"\n    \"num_workers_per_machine) after calling torch.distributed.init_process_group().\"\n)\n@functools.lru_cache()\ndef create_local_process_group(num_workers_per_machine: int) -> None:\n    \"\"\"",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "_MISSING_LOCAL_PG_ERROR",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.utils.dist_utils",
        "description": "stereo.modeling.models.nmrf.utils.dist_utils",
        "peekOfCode": "_MISSING_LOCAL_PG_ERROR = (\n    \"Local process group is not yet created! Please use `launch()` \"\n    \"to start processes and initialize pytorch process group. If you need to start \"\n    \"processes in other ways, please call create_local_process_group(\"\n    \"num_workers_per_machine) after calling torch.distributed.init_process_group().\"\n)\n@functools.lru_cache()\ndef create_local_process_group(num_workers_per_machine: int) -> None:\n    \"\"\"\n    Create a process group that contains ranks within the same machine.",
        "detail": "stereo.modeling.models.nmrf.utils.dist_utils",
        "documentation": {}
    },
    {
        "label": "InferenceSampler",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.utils.evaluation",
        "description": "stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "class InferenceSampler(Sampler):\n    \"\"\"\n    Produce indices for inference across all workers.\n    Inference needs to run on the __exact__ set of samples,\n    therefore when the total number of samples is not divisible by the number of workers,\n    this sampler produces different number of samples on different workers.\n    \"\"\"\n    def __init__(self, size):\n        \"\"\"\n        rgs:",
        "detail": "stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "DatasetEvaluator",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.utils.evaluation",
        "description": "stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "class DatasetEvaluator:\n    \"\"\"\n    Base class for a dataset evaluator.\n    The function :func:`inference_on_dataset` runs the model over\n    all samples in the dataset, and have a DatasetEvaluator to process the inputs/outputs.\n    This class will accumulate information of the inputs/outputs (by:meth:`process`),\n    add produce evaluation results in the end (by :meth:`evaluate`).\n    \"\"\"\n    def reset(self):\n        \"\"\"",
        "detail": "stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "DatasetEvaluators",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.utils.evaluation",
        "description": "stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "class DatasetEvaluators(DatasetEvaluator):\n    \"\"\"\n    Wrapper class to combine multiple :class:`DatasetEvaluator` instances.\n    This class dispatches every evaluation call to\n    all of its :class:`DatasetEvaluator`.\n    \"\"\"\n    def __init__(self, evaluators):\n        \"\"\"\n        Args:\n            evaluators (list): the evaluators to combine.",
        "detail": "stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "DispEvaluator",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.utils.evaluation",
        "description": "stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "class DispEvaluator(DatasetEvaluator):\n    \"\"\"Evaluate disparity accuracy using metrics.\"\"\"\n    def __init__(self, thres, only_valid, max_disp=None, eval_prop=True, divis_by=8):\n        \"\"\"\n        Args:\n            thres (list[str] or None): threshold for outlier\n            only_valid (bool): whether invalid pixels are excluded from evaluation\n            max_disp (int or None): If None, maximum disparity will be regarded as infinity\n            eval_prop (bool): whether evaluate the proposal quality.\n        \"\"\"",
        "detail": "stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "print_csv_format",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.evaluation",
        "description": "stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "def print_csv_format(results):\n    \"\"\"\n    Print main metrics in a format similar to Detectron2,\n    so that they are easy to copypaste into a spreadsheet.\n    Args:\n        results (OrderedDict[dict]): task_name -> {metric -> score}\n            unordered dict can also be printed, but in arbitrary order\n    \"\"\"\n    assert isinstance(results, abc.Mapping) or not len(results), results\n    logger = logging.getLogger(__name__)",
        "detail": "stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "inference_on_dataset",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.evaluation",
        "description": "stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "def inference_on_dataset(\n        model, data_loader, evaluator: Union[DatasetEvaluator, List[DatasetEvaluator], None], **kwargs\n):\n    \"\"\"\n    Run model on the data_loader and evaluate the metrics with evaluator.\n    Also benchmark the inference speed of `model.__call__` accurately.\n    The model will be used in eval mode.\n    Args:\n        model (callable): a callable which takes an object from\n            `data_loader` and returns some outputs.",
        "detail": "stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "inference_context",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.evaluation",
        "description": "stereo.modeling.models.nmrf.utils.evaluation",
        "peekOfCode": "def inference_context(model):\n    \"\"\"\n    A context where the model is temporarily changed to eval mode,\n    and restored to previous mode afterwards,\n    Args:\n        model: a torch Module\n    \"\"\"\n    training_mode = model.training\n    model.eval()\n    yield",
        "detail": "stereo.modeling.models.nmrf.utils.evaluation",
        "documentation": {}
    },
    {
        "label": "InputPadder",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "class InputPadder:\n    \"\"\" Pads images such that dimensions are divisible by 8 \"\"\"\n    def __init__(self, dims, mode='sintel', divis_by=8):\n        self.ht, self.wd = dims[-2:]\n        pad_ht = (((self.ht // divis_by) + 1) * divis_by - self.ht) % divis_by\n        pad_wd = (((self.wd // divis_by) + 1) * divis_by - self.wd) % divis_by\n        if mode == 'sintel':\n            self._pad = [pad_wd//2, pad_wd - pad_wd//2, pad_ht//2, pad_ht - pad_ht//2]\n        elif mode == 'proposal':\n            self._pad = [0, pad_wd, 0, pad_ht]",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readFlow",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readFlow(fn):\n    \"\"\" Read .flo file in Middlebury format\"\"\"\n    # Code adapted from:\n    # http://stackoverflow.com/questions/28013200/reading-middlebury-flow-files-with-python-bytes-array-numpy\n    # WARNING: this will work on little-endian architectures (eg Intel x86) only!\n    # print 'fn = %s'%(fn)\n    with open(fn, 'rb') as f:\n        magic = np.fromfile(f, np.float32, count=1)\n        if 202021.25 != magic:\n            print('Magic number incorrect. Invalid .flo file')",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readPFM",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readPFM(file):\n    file = open(file, 'rb')\n    color = None\n    width = None\n    height = None\n    scale = None\n    endian = None\n    header = file.readline().rstrip()\n    if header == b'PF':\n        color = True",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "writePFM",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def writePFM(file, array):\n    import os\n    assert type(file) is str and type(array) is np.ndarray and \\\n           os.path.splitext(file)[1] == \".pfm\"\n    with open(file, 'wb') as f:\n        H, W = array.shape\n        headers = [\"Pf\\n\", f\"{W} {H}\\n\", \"-1\\n\"]\n        for header in headers:\n            f.write(str.encode(header))\n        array = np.flip(array, axis=0).astype(np.float32)",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "writeFlow",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def writeFlow(filename, uv, v=None):\n    \"\"\" Write optical flow to file.\n    If v is None, uv is assumed to contain both u and v channels,\n    stacked in depth.\n    Original code by Deqing Sun, adapted from Daniel Scharstein.\n    \"\"\"\n    nBands = 2\n    if v is None:\n        assert (uv.ndim == 3)\n        assert (uv.shape[2] == 2)",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readFlowKITTI",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readFlowKITTI(filename):\n    flow = cv2.imread(filename, cv2.IMREAD_ANYDEPTH | cv2.IMREAD_COLOR)\n    flow = flow[:, :, ::-1].astype(np.float32)\n    flow, valid = flow[:, :, :2], flow[:, :, 2]\n    flow = (flow - 2 ** 15) / 64.0\n    return flow, valid\ndef readDispKITTI(filename):\n    disp = cv2.imread(filename, cv2.IMREAD_ANYDEPTH) / 256.0\n    valid = disp > 0.0\n    return disp, valid",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispKITTI",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispKITTI(filename):\n    disp = cv2.imread(filename, cv2.IMREAD_ANYDEPTH) / 256.0\n    valid = disp > 0.0\n    return disp, valid\ndef readDispVKITTI(filename):\n    depth = cv2.imread(filename, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH)\n    f = 725.0087\n    baseline = 0.532725\n    disp = f * baseline * 100 / depth\n    # magic value = f * baseline * 100 / 65535 = 0.589",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispVKITTI",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispVKITTI(filename):\n    depth = cv2.imread(filename, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH)\n    f = 725.0087\n    baseline = 0.532725\n    disp = f * baseline * 100 / depth\n    # magic value = f * baseline * 100 / 65535 = 0.589\n    valid = disp > 0.59\n    return disp, valid\ndef readDispCarla(filename, max_disp=0.9):\n    import math",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispCarla",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispCarla(filename, max_disp=0.9):\n    import math\n    bgr = cv2.imread(filename)\n    bgr = bgr.astype(np.float32)\n    # Apply (R + G * 256 + B * 256 * 256) / (256 * 256 * 256 - 1).\n    normalized_depth = np.dot(bgr, [65536.0, 256.0, 1.0])\n    normalized_depth /= 16777215.0  # (256.0 * 256.0 * 256.0 - 1.0)\n    far = 1000.0  # max depth in meters.\n    depth = normalized_depth * far\n    valid = normalized_depth < max_disp",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispArgoverse",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispArgoverse(filename):\n    \"\"\"\n    The disparity maps are saved as uint16 PNG images. A zero-value (\"0\") indicates that no ground truth exists\n    for that pixel. The true disparity for a pixel can be recovered by first converting the uint16 value to float\n    and then dividing it by 256.\n    Args:\n        filename: Path to the disparity map file;\n    Returns:\n        a tuple (disparity_map, valid),\n        disparity map is an array of shape (H, W) representing a float32 single-channel disparity map.",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispSintelStereo",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispSintelStereo(file_name):\n    a = np.array(Image.open(file_name))\n    d_r, d_g, d_b = np.split(a, axis=2, indices_or_sections=3)\n    disp = (d_r * 4 + d_g / (2 ** 6) + d_b / (2 ** 14))[..., 0]\n    mask = np.array(Image.open(file_name.replace('disparities', 'occlusions')))\n    valid = ((mask == 0) & (disp > 0))\n    return disp, valid\n# Method taken from https://research.nvidia.com/sites/default/files/pubs/2018-06_Falling-Things/readme_0.txt\ndef readDispFallingThings(file_name):\n    a = np.array(Image.open(file_name))",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispFallingThings",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispFallingThings(file_name):\n    a = np.array(Image.open(file_name))\n    with open('/'.join(file_name.split('/')[:-1] + ['_camera_settings.json']), 'r') as f:\n        intrinsics = json.load(f)\n    fx = intrinsics['camera_settings'][0]['intrinsic_settings']['fx']\n    disp = (fx * 6.0 * 100) / a.astype(np.float32)\n    valid = disp > 0\n    return disp, valid\n# Method taken from https://github.com/castacks/tartanair_tools/blob/master/data_type.md\ndef readDispTartanAir(file_name):",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispTartanAir",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispTartanAir(file_name):\n    depth = np.load(file_name)\n    disp = 80.0 / depth\n    valid = disp > 0\n    return disp, valid\ndef readDispMiddlebury(file_name):\n    if basename(file_name) == 'disp0GT.pfm':\n        disp = readPFM(file_name).astype(np.float32)\n        assert len(disp.shape) == 2\n        nocc_pix = file_name.replace('disp0GT.pfm', 'mask0nocc.png')",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readDispMiddlebury",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readDispMiddlebury(file_name):\n    if basename(file_name) == 'disp0GT.pfm':\n        disp = readPFM(file_name).astype(np.float32)\n        assert len(disp.shape) == 2\n        nocc_pix = file_name.replace('disp0GT.pfm', 'mask0nocc.png')\n        assert exists(nocc_pix)\n        nocc_pix = imageio.imread(nocc_pix) == 255\n        assert np.any(nocc_pix)\n        return disp, nocc_pix\n    elif basename(file_name) == 'disp0.pfm':",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "writeFlowKITTI",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def writeFlowKITTI(filename, uv):\n    uv = 64.0 * uv + 2 ** 15\n    valid = np.ones([uv.shape[0], uv.shape[1], 1])\n    uv = np.concatenate([uv, valid], axis=-1).astype(np.uint16)\n    cv2.imwrite(filename, uv[..., ::-1])\ndef writeDispKITTI(filename, disp):\n    disp = np.round(disp * 256).astype(np.uint16)\n    cv2.imwrite(filename, disp)\ndef readOcclusionMap(filename):\n    return cv2.imread(filename)",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "writeDispKITTI",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def writeDispKITTI(filename, disp):\n    disp = np.round(disp * 256).astype(np.uint16)\n    cv2.imwrite(filename, disp)\ndef readOcclusionMap(filename):\n    return cv2.imread(filename)\ndef read_super_pixel_label(filename):\n    label = cv2.imread(filename, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH).astype(np.int32)\n    return label\ndef read_gen(file_name, pil=False):\n    ext = splitext(file_name)[-1]",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "readOcclusionMap",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def readOcclusionMap(filename):\n    return cv2.imread(filename)\ndef read_super_pixel_label(filename):\n    label = cv2.imread(filename, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH).astype(np.int32)\n    return label\ndef read_gen(file_name, pil=False):\n    ext = splitext(file_name)[-1]\n    if ext == '.png' or ext == '.jpeg' or ext == '.ppm' or ext == '.jpg':\n        return Image.open(file_name)\n    elif ext == '.bin' or ext == '.raw':",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "read_super_pixel_label",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def read_super_pixel_label(filename):\n    label = cv2.imread(filename, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH).astype(np.int32)\n    return label\ndef read_gen(file_name, pil=False):\n    ext = splitext(file_name)[-1]\n    if ext == '.png' or ext == '.jpeg' or ext == '.ppm' or ext == '.jpg':\n        return Image.open(file_name)\n    elif ext == '.bin' or ext == '.raw':\n        return np.load(file_name)\n    elif ext == '.flo':",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "read_gen",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def read_gen(file_name, pil=False):\n    ext = splitext(file_name)[-1]\n    if ext == '.png' or ext == '.jpeg' or ext == '.ppm' or ext == '.jpg':\n        return Image.open(file_name)\n    elif ext == '.bin' or ext == '.raw':\n        return np.load(file_name)\n    elif ext == '.flo':\n        return readFlow(file_name).astype(np.float32)\n    elif ext == '.pfm':\n        flow = readPFM(file_name).astype(np.float32)",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "downsample_disp",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "def downsample_disp(disp, label, stride=8):\n    ht, wd = disp.shape[-2:]\n    right_pad = (stride - wd % stride) % stride\n    down_pad = (stride - ht % stride) % stride\n    disp = F.pad(disp, pad=(0, right_pad, 0, down_pad), mode='constant', value=0)\n    label = F.pad(label, pad=(0, right_pad, 0, down_pad), mode='constant', value=-1)\n    bs, ht, wd = disp.shape\n    disp = rearrange(disp, 'b (h hs) (w ws) -> (b h w) (hs ws)', hs=stride, ws=stride)\n    disp = disp.contiguous()\n    label = rearrange(label, 'b (h hs) (w ws) -> (b h w) (hs ws)', hs=stride, ws=stride)",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "TAG_CHAR",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.utils.frame_utils",
        "description": "stereo.modeling.models.nmrf.utils.frame_utils",
        "peekOfCode": "TAG_CHAR = np.array([202021.25], np.float32)\ndef readFlow(fn):\n    \"\"\" Read .flo file in Middlebury format\"\"\"\n    # Code adapted from:\n    # http://stackoverflow.com/questions/28013200/reading-middlebury-flow-files-with-python-bytes-array-numpy\n    # WARNING: this will work on little-endian architectures (eg Intel x86) only!\n    # print 'fn = %s'%(fn)\n    with open(fn, 'rb') as f:\n        magic = np.fromfile(f, np.float32, count=1)\n        if 202021.25 != magic:",
        "detail": "stereo.modeling.models.nmrf.utils.frame_utils",
        "documentation": {}
    },
    {
        "label": "_ColorfulFormatter",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.utils.logger",
        "description": "stereo.modeling.models.nmrf.utils.logger",
        "peekOfCode": "class _ColorfulFormatter(logging.Formatter):\n    def __init__(self, *args, **kwargs):\n        self._root_name = kwargs.pop(\"root_name\") + \".\"\n        self._abbrev_name = kwargs.pop(\"abbrev_name\", \"\")\n        if len(self._abbrev_name):\n            self._abbrev_name = self._abbrev_name + \".\"\n        super(_ColorfulFormatter, self).__init__(*args, **kwargs)\n    def formatMessage(self, record):\n        record.name = record.name.replace(self._root_name, self._abbrev_name)\n        log = super(_ColorfulFormatter, self).formatMessage(record)",
        "detail": "stereo.modeling.models.nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "setup_logger",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.logger",
        "description": "stereo.modeling.models.nmrf.utils.logger",
        "peekOfCode": "def setup_logger(\n    output=None, distributed_rank=0, *, color=True, name=\"imagenet\", abbrev_name=None\n):\n    \"\"\"\n    Initialize the detectron2 logger and set its verbosity level to \"INFO\".\n    Args:\n        output (str): a file name or a directory to save log. If None, will not save log file.\n            If ends with \".txt\" or \".log\", assumed to be a file name.\n            Otherwise, logs will be saved to `output/log.txt`.\n        name (str): the root module name of this logger",
        "detail": "stereo.modeling.models.nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "log_every_n_seconds",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.logger",
        "description": "stereo.modeling.models.nmrf.utils.logger",
        "peekOfCode": "def log_every_n_seconds(lvl, msg, n=1, *, name=None):\n    \"\"\"\n    Log no more than once per n seconds.\n    Args:\n        lvl (int): the logging level\n        msg (str):\n        n (int):\n        name (str): name of the logger to use. Will use the caller's module by default.\n    \"\"\"\n    caller_module, key = _find_caller()",
        "detail": "stereo.modeling.models.nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "_LOG_COUNTER",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.utils.logger",
        "description": "stereo.modeling.models.nmrf.utils.logger",
        "peekOfCode": "_LOG_COUNTER = Counter()\n_LOG_TIMER = {}\ndef log_every_n_seconds(lvl, msg, n=1, *, name=None):\n    \"\"\"\n    Log no more than once per n seconds.\n    Args:\n        lvl (int): the logging level\n        msg (str):\n        n (int):\n        name (str): name of the logger to use. Will use the caller's module by default.",
        "detail": "stereo.modeling.models.nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "_LOG_TIMER",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.utils.logger",
        "description": "stereo.modeling.models.nmrf.utils.logger",
        "peekOfCode": "_LOG_TIMER = {}\ndef log_every_n_seconds(lvl, msg, n=1, *, name=None):\n    \"\"\"\n    Log no more than once per n seconds.\n    Args:\n        lvl (int): the logging level\n        msg (str):\n        n (int):\n        name (str): name of the logger to use. Will use the caller's module by default.\n    \"\"\"",
        "detail": "stereo.modeling.models.nmrf.utils.logger",
        "documentation": {}
    },
    {
        "label": "SmoothedValue",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "class SmoothedValue(object):\n    \"\"\"Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    \"\"\"\n    def __init__(self, window_size=20, fmt=None):\n        if fmt is None:\n            fmt = \"{median:.4f} ({avg:.4f})\"\n        self.deque = deque(maxlen=window_size)\n        self.total = 0.0\n        self.count = 0",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "MetricLogger",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "class MetricLogger(object):\n    def __init__(self, delimiter=\"\\t\"):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "check_path",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def check_path(path):\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)  # explicitly set exist_ok when multi-processing\ndef save_command(save_path, filename='command_train.txt'):\n    check_path(save_path)\n    command = sys.argv\n    save_file = os.path.join(save_path, filename)\n    # Save all training commands when resuming training\n    with open(save_file, 'a') as f:\n        f.write(' '.join(command))",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "save_command",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def save_command(save_path, filename='command_train.txt'):\n    check_path(save_path)\n    command = sys.argv\n    save_file = os.path.join(save_path, filename)\n    # Save all training commands when resuming training\n    with open(save_file, 'a') as f:\n        f.write(' '.join(command))\n        f.write('\\n\\n')\ndef save_config(config, filename='config.txt'):\n    check_path(config.checkpoint_dir)",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "save_config",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def save_config(config, filename='config.txt'):\n    check_path(config.checkpoint_dir)\n    save_path = os.path.join(config.checkpoint_dir, filename)\n    # save all training config when resuming training\n    with open(save_path, 'a') as f:\n        f.write(config.dump())\n        f.write('\\n\\n')\n@torch.no_grad()\ndef accuracy(output, target):\n    \"\"\"Computes the precision\"\"\"",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "accuracy",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def accuracy(output, target):\n    \"\"\"Computes the precision\"\"\"\n    if target.numel() == 0:\n        return [torch.zeros([], device=output.device)]\n    pred = output > 0.5\n    correct = pred.eq(target > 0.5)\n    batch_size = target.size(0)\n    res = correct.float().sum().mul_(100.0 / batch_size)\n    return res\ndef get_sha():",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "get_sha",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def get_sha():\n    cwd = os.path.dirname(os.path.abspath(__file__))\n    def _run(command):\n        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n    sha = 'N/A'\n    diff = 'clean'\n    branch = 'N/A'\n    try:\n        sha = _run(['git', 'rev-parse', 'HEAD'])\n        subprocess.check_output(['git', 'diff'], cwd=cwd)",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "detect_compute_compatibility",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def detect_compute_compatibility(CUDA_HOME, so_file):\n    try:\n        cuobjdump = os.path.join(CUDA_HOME, \"bin\", \"cuobjdump\")\n        if os.path.isfile(cuobjdump):\n            output = subprocess.check_output(\n                \"'{}' --list-elf '{}'\".format(cuobjdump, so_file), shell=True\n            )\n            output = output.decode(\"utf-8\").strip().split(\"\\n\")\n            arch = []\n            for line in output:",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "collect_torch_env",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def collect_torch_env():\n    try:\n        import torch.__config__\n        return torch.__config__.show()\n    except ImportError:\n        # compatible with older versions of pytorch\n        from torch.utils.collect_env import get_pretty_env_info\n        return get_pretty_env_info()\ndef collect_env_info():\n    has_gpu = torch.cuda.is_available()  # true for both CUDA & ROCM",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "collect_env_info",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def collect_env_info():\n    has_gpu = torch.cuda.is_available()  # true for both CUDA & ROCM\n    torch_version = torch.__version__\n    # NOTE that CUDA_HOME/ROCM_HOME could be None even when CUDA runtime libs are functional\n    from torch.utils.cpp_extension import CUDA_HOME, ROCM_HOME\n    has_rocm = False\n    if (getattr(torch.version, \"hip\", None) is not None) and (ROCM_HOME is not None):\n        has_rocm = True\n    has_cuda = has_gpu and (not has_rocm)\n    data = []",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "seed_all_rng",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "def seed_all_rng(seed=None):\n    \"\"\"\n    Set the random seed for the RNG in torch, numpy and python.\n    Args:\n        seed (int): if None, will use a strong random seed.\n    \"\"\"\n    if seed is None:\n        seed = (\n            os.getpid()\n            + int(datetime.now().strftime(\"%S%f\"))",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "__torchvision_need_compat_flag",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.utils.misc",
        "description": "stereo.modeling.models.nmrf.utils.misc",
        "peekOfCode": "__torchvision_need_compat_flag = float(torchvision.__version__.split('.')[1]) < 7\nif __torchvision_need_compat_flag:\n    from torchvision.ops import _new_empty_tensor\n    from torchvision.ops.misc import _output_size\ndef check_path(path):\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)  # explicitly set exist_ok when multi-processing\ndef save_command(save_path, filename='command_train.txt'):\n    check_path(save_path)\n    command = sys.argv",
        "detail": "stereo.modeling.models.nmrf.utils.misc",
        "documentation": {}
    },
    {
        "label": "VisImage",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.utils.visualization",
        "description": "stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "class VisImage:\n    def __init__(self, img, scale=1.0):\n        \"\"\"\n        Args:\n            img (ndarray): an RGB image of shape (H, W, 3) in range[0, 255].\n            scale (float): scale the input image\n        \"\"\"\n        self.img = img\n        self.scale = scale\n        self.width, self.height = img.shape[1], img.shape[0]",
        "detail": "stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "Visualizer",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.utils.visualization",
        "description": "stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "class Visualizer:\n    \"\"\"\n    Visualizer that draws data about disparity on images.\n    It contains methods like `draw_{uncertainty,disp,normal,error}`\n    that draws primitive objects to images in some pre-defined style.\n    This visualizer focuses on high rendering quality rather than performance. It is not\n    designed to be used for real-time applications.\n    \"\"\"\n    def __init__(self, img_rgb, scale=1.0):\n        \"\"\"",
        "detail": "stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "plot_disparity",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.visualization",
        "description": "stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "def plot_disparity(savename, data, max_disp):\n    plt.imsave(savename, data, vmin=0, vmax=max_disp, cmap='turbo')\ndef plot_gradient_map(savename, data):\n    data = (data + 1.0) / 2\n    data = 255 * data\n    data = data.astype(np.uint8)\n    plt.imsave(savename, data)\ndef gen_error_colormap():\n    cols = np.array(\n        [[0 / 3.0, 0.1875 / 3.0, 49, 54, 149],",
        "detail": "stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "plot_gradient_map",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.visualization",
        "description": "stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "def plot_gradient_map(savename, data):\n    data = (data + 1.0) / 2\n    data = 255 * data\n    data = data.astype(np.uint8)\n    plt.imsave(savename, data)\ndef gen_error_colormap():\n    cols = np.array(\n        [[0 / 3.0, 0.1875 / 3.0, 49, 54, 149],\n         [0.1875 / 3.0, 0.375 / 3.0, 69, 117, 180],\n         [0.375 / 3.0, 0.75 / 3.0, 116, 173, 209],",
        "detail": "stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "gen_error_colormap",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.visualization",
        "description": "stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "def gen_error_colormap():\n    cols = np.array(\n        [[0 / 3.0, 0.1875 / 3.0, 49, 54, 149],\n         [0.1875 / 3.0, 0.375 / 3.0, 69, 117, 180],\n         [0.375 / 3.0, 0.75 / 3.0, 116, 173, 209],\n         [0.75 / 3.0, 1.5 / 3.0, 171, 217, 233],\n         [1.5 / 3.0, 3 / 3.0, 224, 243, 248],\n         [3 / 3.0, 6 / 3.0, 254, 224, 144],\n         [6 / 3.0, 12 / 3.0, 253, 174, 97],\n         [12 / 3.0, 24 / 3.0, 244, 109, 67],",
        "detail": "stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "disp_error_img",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.visualization",
        "description": "stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "def disp_error_img(save_name, pred, gt, abs_thres=3., rel_thres=0.05):\n    pred_np = pred.detach().cpu().numpy()\n    gt_np = gt.detach().cpu().numpy()\n    H, W = pred_np.shape\n    # valid mask\n    mask = gt_np > 0\n    # error in percentage. When error <= 1, the pixel is valid since <= 3px & 5%\n    error = np.abs(gt_np - pred_np)\n    error[np.logical_not(mask)] = 0\n    error[mask] = np.minimum(error[mask] / abs_thres, (error[mask] / gt_np[mask]) / rel_thres)",
        "detail": "stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "gen_kitti_cmap",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.utils.visualization",
        "description": "stereo.modeling.models.nmrf.utils.visualization",
        "peekOfCode": "def gen_kitti_cmap():\n    map = np.array([[0, 0, 0, 114],\n                    [0, 0, 1, 185],\n                    [1, 0, 0, 114],\n                    [1, 0, 1, 174],\n                    [0, 1, 0, 114],\n                    [0, 1, 1, 185],\n                    [1, 1, 0, 114],\n                    [1, 1, 1, 0]])\n    bins = map[:-1, 3]",
        "detail": "stereo.modeling.models.nmrf.utils.visualization",
        "documentation": {}
    },
    {
        "label": "DPN",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.DPN",
        "description": "stereo.modeling.models.nmrf.DPN",
        "peekOfCode": "class DPN(nn.Module):\n    \"\"\"Disparity proposal seed extraction network.\n    Args:\n        cost_group (int): group number of groupwise cost volume\n        num_proposals (int): number of proposals for each pixel\n        feat_dim (int): dimension of backbone feature map\n        context_dim (int): dimension of visual context\n        prop_embed_dim (int): dimension of label seed embedding\n        split_size (int): width of stripe\n        prop_n_heads: head of attention",
        "detail": "stereo.modeling.models.nmrf.DPN",
        "documentation": {}
    },
    {
        "label": "MLP",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class MLP(nn.Module):\n    \"\"\" Very simple multi-layer perception (also called FFN)\"\"\"\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n        super().__init__()\n        self.num_layers = num_layers\n        h = [hidden_dim] * (num_layers - 1)\n        self.layers = nn.ModuleList(nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim]))\n    def forward(self, x):\n        for i, layer in enumerate(self.layers):\n            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "BasicAttention",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class BasicAttention(nn.Module):\n    \"\"\"\n    label representation:  [B, N, C]\n    \"\"\"\n    def __init__(self, dim, qk_dim, num_heads=8, attn_drop=0., proj_drop=0., drop_path=0., dropout=0., normalize_before=False):\n        super().__init__()\n        assert dim % num_heads == 0, f'dim {dim} should be multiple times of heads {num_heads}'\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class WindowAttention(nn.Module):\n    \"\"\" Window based multi-head positional sensitive self attention (W-MSA).\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        shift_size (int): Shift size for SW-MSA.\n        num_heads (int): Number of attention heads.\n        qk_scale (float | None, optional): Override a default qk scale of head_dim ** -0.5 if set\n        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "SwinNMP",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class SwinNMP(nn.Module):\n    r\"\"\"Swin Message Passing Block.\n    Args:\n        dim (int): Number of input channels.\n        qkv_dim (int): Number of input token channels\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "CSWinAttention",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class CSWinAttention(nn.Module):\n    def __init__(self, dim, resolution, idx, split_size=7, num_heads=8, qk_scale=None, attn_drop=0.):\n        \"\"\"Attention within cross-shaped windows.\n        \"\"\"\n        super().__init__()\n        self.dim = dim\n        self.resolution = resolution\n        self.split_size = split_size\n        self.num_heads = num_heads\n        head_dim = dim // num_heads",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "CSWinNMP",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class CSWinNMP(nn.Module):\n    def __init__(self, dim, qk_dim, v_dim, patches_resolution, num_heads,\n                 split_size=7, mlp_ratio=4., qk_scale=None,\n                 attn_drop=0., proj_drop=0., drop_path=0., dropout=0.,\n                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, normalize_before=False):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.patches_resolution = patches_resolution\n        self.split_size = split_size",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "Propagation",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class Propagation(nn.Module):\n    \"\"\"Label seed propagation\"\"\"\n    def __init__(self, embed_dim, cost_group, layers, norm=None, return_intermediate=False):\n        super().__init__()\n        self.cost_encoder = nn.Sequential(\n            nn.Linear(cost_group*9, embed_dim),\n            nn.GELU(),\n            nn.Linear(embed_dim, embed_dim),\n        )\n        self.proj = nn.Linear(embed_dim+31, embed_dim, bias=False)",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "Inference",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class Inference(nn.Module):\n    \"\"\"Neural MRF Inference\"\"\"\n    def __init__(self, cost_group, dim, layers, norm, return_intermediate=False):\n        super().__init__()\n        self.ffn = Mlp(dim+cost_group, dim, dim)\n        self.dim = dim\n        self.layers = layers\n        self.norm = norm\n        self.cost_group = cost_group\n        self.return_intermediate = return_intermediate",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "Refinement",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class Refinement(Inference):\n    @staticmethod\n    def gen_shift_window_attn_mask(input_resolution, window_size, shift_size, device=torch.device('cuda')):\n        \"\"\"\n        input_resolution (tuple[int]): The height and width of input\n        window_size (tuple[int]): The height, width and depth of window\n        shift_size (int): Shift size for SW-MSA.\n        \"\"\"\n        H, W = input_resolution\n        img_mask = torch.zeros((1, H, W, 1), device=device)  # 1 H W 1",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "PropagationLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class PropagationLayer(nn.Module):\n    def __init__(self, embed_dim, mlp_ratio, context_dim, split_size, n_heads,\n                 activation=\"gelu\", attn_drop=0., proj_drop=0., drop_path=0., dropout=0., normalize_before=False):\n        super().__init__()\n        # self attention\n        act_layer = _get_activation_fn(activation)\n        # concat seed embedding with visual context when linearly projecting to\n        # query and key since visually similar pixel tends to have coherent disparities\n        qk_dim = embed_dim + context_dim\n        v_dim = embed_dim",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "InferenceLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class InferenceLayer(nn.Module):\n    def __init__(self, embed_dim, mlp_ratio, window_size, shift_size, n_heads,\n                 activation=\"gelu\", attn_drop=0., proj_drop=0., drop_path=0., dropout=0., normalize_before=False):\n        super().__init__()\n        # self attention\n        act_layer = _get_activation_fn(activation)\n        qk_dim = embed_dim + 31\n        self.window_size = window_size\n        self.shift_size = shift_size\n        # attend to proposals of the same pixel to suppress non-accurate proposals",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "RefinementLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "class RefinementLayer(nn.Module):\n    def __init__(self, dim, mlp_ratio, window_size, shift_size, n_heads,\n                 activation=\"gelu\", attn_drop=0., proj_drop=0., drop_path=0., dropout=0., normalize_before=False):\n        super().__init__()\n        act_layer = _get_activation_fn(activation)\n        qk_dim = dim + 31\n        self.window_size = window_size\n        self.shift_size = shift_size\n        self.nmp = SwinNMP(dim, qk_dim, num_heads=n_heads, window_size=window_size,\n                           shift_size=shift_size, mlp_ratio=mlp_ratio, attn_drop=attn_drop,",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "fourier_grid_embed",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "def fourier_grid_embed(data, embed_dim):\n    \"\"\"data format: B[spatial dims]C\n    Returns pos_embedding: same format with data\n    \"\"\"\n    b, *axis, _, device, dtype = *data.shape, data.device, data.dtype\n    assert embed_dim % (2 * len(axis)) == 0\n    # calculate fourier encoded positions in the range of [-1, 1], for all axis\n    axis_pos = list(map(lambda size: torch.linspace(-1., 1., steps=size, device=device, dtype=dtype), axis))\n    pos = torch.stack(torch.meshgrid(*axis_pos, indexing='ij'), dim=-1)\n    num_bands = embed_dim // (2 * len(axis))",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "fourier_coord_embed",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.NMP",
        "description": "stereo.modeling.models.nmrf.NMP",
        "peekOfCode": "def fourier_coord_embed(coord, N_freqs, normalizer=3.14/512, logscale=True):\n    \"\"\"\n    coord: [...]D\n    returns:\n        [...]dim, where dim=(2*N_freqs+1)*D\n    \"\"\"\n    if logscale:\n        freq_bands = 2**torch.linspace(0, N_freqs-1, N_freqs, device=coord.device)\n    else:\n        freq_bands = torch.linspace(1, 2**(N_freqs-1), N_freqs)",
        "detail": "stereo.modeling.models.nmrf.NMP",
        "documentation": {}
    },
    {
        "label": "NMRF",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMRF",
        "description": "stereo.modeling.models.nmrf.NMRF",
        "peekOfCode": "class NMRF(nn.Module):\n    @configurable\n    def __init__(self,\n                 cfgs,\n                 num_proposals=4,\n                 max_disp=320,\n                 num_infer_layers=5,\n                 num_refine_layers=5,\n                 infer_embed_dim=128,\n                 infer_n_heads=4,",
        "detail": "stereo.modeling.models.nmrf.NMRF",
        "documentation": {}
    },
    {
        "label": "Criterion",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.NMRF",
        "description": "stereo.modeling.models.nmrf.NMRF",
        "peekOfCode": "class Criterion(nn.Module):\n    \"\"\" This class computes the loss for disparity proposal extraction.\n    The process happens in two steps:\n        1) we compute a one-to-one matching between ground truth disparities and the outputs of the model\n        2) we supervise each output to be closer to the ground truth disparity it was matched to\n    Note: to avoid trivial solution, we add a prior term in the loss computation that we favor positive output.\n    \"\"\"\n    def __init__(self, weight_dict, max_disp, loss_type):\n        \"\"\" Create the criterion.\n        Parameters:",
        "detail": "stereo.modeling.models.nmrf.NMRF",
        "documentation": {}
    },
    {
        "label": "build",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.NMRF",
        "description": "stereo.modeling.models.nmrf.NMRF",
        "peekOfCode": "def build(cfg):\n    model = NMRF(cfg)\n    weight_dict = {'proposal_disp': 1, 'init': 1}\n    assert len(cfg.SOLVER.LOSS_WEIGHTS) == cfg.NMP.NUM_INFER_LAYERS + cfg.NMP.NUM_REFINE_LAYERS\n    if cfg.SOLVER.AUX_LOSS:\n        aux_weight_dict = {}\n        for i in range(cfg.NMP.NUM_INFER_LAYERS + cfg.NMP.NUM_REFINE_LAYERS-1):\n            if i < cfg.NMP.NUM_INFER_LAYERS:\n                aux_weight_dict.update({f'loss_coarse_disp_{i}': cfg.SOLVER.LOSS_WEIGHTS[i]})\n            else:",
        "detail": "stereo.modeling.models.nmrf.NMRF",
        "documentation": {}
    },
    {
        "label": "ConvFFN",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.adaptor_modules",
        "description": "stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "class ConvFFN(nn.Module):\n    def __init__(self, in_features, hidden_features=None, out_features=None,\n                 act_layer=nn.GELU, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.dwconv = DWConv(hidden_features)\n        self.act = act_layer()\n        self.fc2 = nn.Linear(hidden_features, out_features)",
        "detail": "stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "DWConv",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.adaptor_modules",
        "description": "stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "class DWConv(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n    def forward(self, x, H, W):\n        B, N, C = x.shape\n        x = x.transpose(1, 2).view(B, C, H, W).contiguous()\n        x = self.dwconv(x).flatten(2).transpose(1, 2)\n        return x\nclass Extractor(nn.Module):",
        "detail": "stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "Extractor",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.adaptor_modules",
        "description": "stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "class Extractor(nn.Module):\n    def __init__(self, dim, num_heads=8, n_points=4, n_levels=1, deform_ratio=1.0,\n                 with_cffn=True, cffn_ratio=0.25, drop=0., drop_path=0.,\n                 norm_layer=partial(nn.LayerNorm, eps=1e-6), with_cp=False):\n        super().__init__()\n        self.query_norm = norm_layer(dim)\n        self.feat_norm = norm_layer(dim)\n        self.attn = MSDeformAttn(d_model=dim, n_levels=n_levels, n_heads=num_heads,\n                                 n_points=n_points, ratio=deform_ratio)\n        self.with_cffn = with_cffn",
        "detail": "stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "ConvStem",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.adaptor_modules",
        "description": "stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "class ConvStem(nn.Module):\n    def __init__(self, inplanes=64, out_channels=256, norm_layer=nn.InstanceNorm2d, with_cp=False):\n        super().__init__()\n        self.with_cp = with_cp\n        self.stem = nn.Sequential(*[\n            nn.Conv2d(3, inplanes, kernel_size=3, stride=2, padding=1, bias=False),\n            norm_layer(inplanes),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(inplanes, inplanes, kernel_size=3, stride=1, padding=1, bias=False),\n            norm_layer(inplanes),",
        "detail": "stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "DeformNeck",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.adaptor_modules",
        "description": "stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "class DeformNeck(nn.Module):\n    def __init__(self, dim, in_channel_list, num_heads=8, n_points=4,\n                 norm_layer=partial(nn.LayerNorm, eps=1e-6), drop=0., drop_path=0.,\n                 with_cffn=True, cffn_ratio=0.25, deform_ratio=1.0, with_cp=False):\n        super().__init__()\n        self.stem = ConvStem(inplanes=64, out_channels=dim)\n        self.dim = dim\n        self.extractors = nn.ModuleList([\n            Extractor(dim=dim, n_levels=1, num_heads=num_heads, n_points=n_points,\n                      norm_layer=norm_layer, deform_ratio=deform_ratio, with_cffn=with_cffn,",
        "detail": "stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "get_reference_points",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.adaptor_modules",
        "description": "stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "def get_reference_points(spatial_shapes, device):\n    reference_points_list = []\n    for lvl, (H_, W_) in enumerate(spatial_shapes):\n        ref_y, ref_x = torch.meshgrid(\n            torch.linspace(0.5, H_ - 0.5, H_, dtype=torch.float32, device=device),\n            torch.linspace(0.5, W_ - 0.5, W_, dtype=torch.float32, device=device))\n        ref_y = ref_y.reshape(-1)[None] / H_\n        ref_x = ref_x.reshape(-1)[None] / W_\n        ref = torch.stack((ref_x, ref_y), -1)\n        reference_points_list.append(ref)",
        "detail": "stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "deform_inputs_dn",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.adaptor_modules",
        "description": "stereo.modeling.models.nmrf.adaptor_modules",
        "peekOfCode": "def deform_inputs_dn(x):\n    bs, c, h, w = x.shape\n    spatial_shapes = torch.as_tensor([(h // 4, w // 4),\n                                      (h // 8, w // 8),\n                                      (h // 16, w // 16),\n                                      (h // 32, w // 32)],\n                                     dtype=torch.long, device=x.device)\n    level_start_index = spatial_shapes.new_zeros((1,))\n    reference_points = get_reference_points([(h // 4, w // 4)], x.device)\n    return reference_points, spatial_shapes, level_start_index",
        "detail": "stereo.modeling.models.nmrf.adaptor_modules",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.backbone",
        "description": "stereo.modeling.models.nmrf.backbone",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_layer=nn.InstanceNorm2d, stride=1, dilation=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n                               dilation=dilation, padding=dilation, stride=stride, bias=False)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               dilation=dilation, padding=dilation, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.norm1 = norm_layer(planes)\n        self.norm2 = norm_layer(planes)",
        "detail": "stereo.modeling.models.nmrf.backbone",
        "documentation": {}
    },
    {
        "label": "Backbone",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.backbone",
        "description": "stereo.modeling.models.nmrf.backbone",
        "peekOfCode": "class Backbone(nn.Module):\n    def __init__(self, output_dim=128, norm_layer=nn.InstanceNorm2d):\n        super(Backbone, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) # 1/2\n        self.norm1 = norm_layer(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.in_planes = 64\n        self.layer1 = self._make_layer(64, stride=1, norm_layer=norm_layer) # 1/2\n        self.layer2 = self._make_layer(96, stride=2, norm_layer=norm_layer) # 1/4\n        self.layer3 = self._make_layer(128, stride=1, norm_layer=norm_layer) # 1/4",
        "detail": "stereo.modeling.models.nmrf.backbone",
        "documentation": {}
    },
    {
        "label": "SwinAdaptor",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.backbone",
        "description": "stereo.modeling.models.nmrf.backbone",
        "peekOfCode": "class SwinAdaptor(nn.Module):\n    def __init__(self, out_channels, drop_path_rate=0.):\n        super().__init__()\n        self.backbone = SwinTransformer(\n            depths=(2, 2, 6, 2),\n            drop_path_rate=drop_path_rate,\n            embed_dim=96,\n            num_heads=(3, 6, 12, 24),\n        )\n        self.neck = DeformNeck(",
        "detail": "stereo.modeling.models.nmrf.backbone",
        "documentation": {}
    },
    {
        "label": "checkpoint_filter_fn",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.backbone",
        "description": "stereo.modeling.models.nmrf.backbone",
        "peekOfCode": "def checkpoint_filter_fn(state_dict):\n    out_dict = {}\n    state_dict = state_dict.get('model', state_dict)\n    state_dict = state_dict.get('state_dict', state_dict)\n    for k, v in state_dict.items():\n        if \"attn_mask\" in k:\n            continue  # skip buffers that should not be persistent\n        if any([k.startswith(n) for n in ('norm', 'head')]):\n            continue\n        out_dict[k] = v",
        "detail": "stereo.modeling.models.nmrf.backbone",
        "documentation": {}
    },
    {
        "label": "create_backbone",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.backbone",
        "description": "stereo.modeling.models.nmrf.backbone",
        "peekOfCode": "def create_backbone(model_type, norm_fn, out_channels, drop_path):\n    model_type = model_type\n    if model_type == \"resnet\":\n        if norm_fn == \"instance\":\n            norm_layer = nn.InstanceNorm2d\n        elif norm_fn == 'batch':\n            norm_layer = nn.BatchNorm2d\n        else:\n            raise ValueError(f'Invalid backbone normalization type: {norm_fn}')\n        backbone = Backbone(out_channels, norm_layer)",
        "detail": "stereo.modeling.models.nmrf.backbone",
        "documentation": {}
    },
    {
        "label": "for_compatibility",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.build_optimizer",
        "description": "stereo.modeling.models.nmrf.build_optimizer",
        "peekOfCode": "def for_compatibility(model):\n    return model\ndef build_optimizer(params, base_lr):\n    model = params.module\n    base_lr = base_lr\n    backbone_lr_decay = 0.1\n    backbone_weight_decay = 1e-05\n    weight_decay_norm = 1e-05\n    norm_module_types = (\n        torch.nn.BatchNorm2d,",
        "detail": "stereo.modeling.models.nmrf.build_optimizer",
        "documentation": {}
    },
    {
        "label": "build_optimizer",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.build_optimizer",
        "description": "stereo.modeling.models.nmrf.build_optimizer",
        "peekOfCode": "def build_optimizer(params, base_lr):\n    model = params.module\n    base_lr = base_lr\n    backbone_lr_decay = 0.1\n    backbone_weight_decay = 1e-05\n    weight_decay_norm = 1e-05\n    norm_module_types = (\n        torch.nn.BatchNorm2d,\n        torch.nn.InstanceNorm2d,\n        torch.nn.LayerNorm,",
        "detail": "stereo.modeling.models.nmrf.build_optimizer",
        "documentation": {}
    },
    {
        "label": "MemoryEfficientCrossAttention",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.hybrid_backbone",
        "description": "stereo.modeling.models.nmrf.hybrid_backbone",
        "peekOfCode": "class MemoryEfficientCrossAttention(nn.Module):\n    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0, qkv_bias=False):\n        super().__init__()\n        inner_dim = dim_head * heads\n        self.heads = heads\n        self.dim_head = dim_head\n        self.to_q = nn.Linear(query_dim, inner_dim, bias=qkv_bias)\n        self.to_k = nn.Linear(context_dim, inner_dim, bias=qkv_bias)\n        self.to_v = nn.Linear(context_dim, inner_dim, bias=qkv_bias)\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))",
        "detail": "stereo.modeling.models.nmrf.hybrid_backbone",
        "documentation": {}
    },
    {
        "label": "FPNLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.hybrid_backbone",
        "description": "stereo.modeling.models.nmrf.hybrid_backbone",
        "peekOfCode": "class FPNLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, high_dim):\n        super(FPNLayer, self).__init__()\n        self.deconv = nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n                                    nn.BatchNorm2d(out_channels),\n                                    nn.ReLU())\n        self.conv = nn.Sequential(nn.Conv2d(out_channels + high_dim, out_channels, kernel_size=3, padding=1, bias=False),\n                                  nn.BatchNorm2d(out_channels),\n                                  nn.ReLU())\n    def forward(self, low, high):",
        "detail": "stereo.modeling.models.nmrf.hybrid_backbone",
        "documentation": {}
    },
    {
        "label": "ResidualConvUnit",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.hybrid_backbone",
        "description": "stereo.modeling.models.nmrf.hybrid_backbone",
        "peekOfCode": "class ResidualConvUnit(nn.Module):\n    def __init__(self, features, activation, bn):\n        super().__init__()\n        self.bn = bn\n        self.groups = 1\n        self.conv1 = nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1, bias=True, groups=self.groups)\n        self.conv2 = nn.Conv2d(features, features, kernel_size=3, stride=1, padding=1, bias=True, groups=self.groups)\n        if self.bn:\n            self.bn1 = nn.BatchNorm2d(features)\n            self.bn2 = nn.BatchNorm2d(features)",
        "detail": "stereo.modeling.models.nmrf.hybrid_backbone",
        "documentation": {}
    },
    {
        "label": "FeatureFusionBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.hybrid_backbone",
        "description": "stereo.modeling.models.nmrf.hybrid_backbone",
        "peekOfCode": "class FeatureFusionBlock(nn.Module):\n    def __init__(\n            self,\n            features,\n            activation,\n            deconv=False,\n            bn=False,\n            expand=False,\n            align_corners=True,\n            size=None",
        "detail": "stereo.modeling.models.nmrf.hybrid_backbone",
        "documentation": {}
    },
    {
        "label": "Hybrid",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.hybrid_backbone",
        "description": "stereo.modeling.models.nmrf.hybrid_backbone",
        "peekOfCode": "class Hybrid(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.vit_backbone = DINOv2(model_name='vits')\n        state_dict = torch.hub.load_state_dict_from_url('dinov2_vits14_pretrain.pth', map_location=\"cpu\")\n        self.vit_backbone.load_state_dict(state_dict, strict=True)\n        self.projects = nn.ModuleList([\n            nn.Conv2d(in_channels=384, out_channels=128, kernel_size=1) for _ in range(4)\n        ])\n        self.resize_layers = nn.ModuleList([",
        "detail": "stereo.modeling.models.nmrf.hybrid_backbone",
        "documentation": {}
    },
    {
        "label": "NearestMatcher",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.matcher",
        "description": "stereo.modeling.models.nmrf.matcher",
        "peekOfCode": "class NearestMatcher(nn.Module):\n    \"\"\"This class computes an assignment between the targets and the predictions of the network\n    The targets include null (=0) disparity. We do a many-to-one matching and identify the best predictions\n    among the ones assigned to a same target.\n    \"\"\"\n    def __init__(self, cost_class: float = 1, cost_disp: float = 1):\n        \"\"\"Create the matcher\n        Params:\n            cost_class: This is the relative weight of the classification error in the matching cost\n            cost_disp: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost",
        "detail": "stereo.modeling.models.nmrf.matcher",
        "documentation": {}
    },
    {
        "label": "bf_match",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.matcher",
        "description": "stereo.modeling.models.nmrf.matcher",
        "peekOfCode": "def bf_match(outputs, targets, num=4):\n    \"\"\" This function computes an assignment between the targets and the predictions of the network.\n    The targets include null (=0) disparity. We do a 1-to-1 matching between non-null targets and best predictions,\n    while the others are un-matched (and thus not propagate gradients).\n    outputs: This is a tensor of dim [batch_size*H*W, num_queries]\n    targets: This is a tensor of dim [batch_size*H*W, 4] containing the target disparities\n    Returns:\n        index tensor of dim [batch_size*H*W, 4], where ith element of each row denotes the index of matched proposal\n            with ith ground truth.\n    \"\"\"",
        "detail": "stereo.modeling.models.nmrf.matcher",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.submodule",
        "description": "stereo.modeling.models.nmrf.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride, downsample, pad, dilation):\n        super().__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(inplanes, planes, 3, stride, padding=dilation if dilation > 1 else pad, dilation=dilation), nn.ReLU(inplace=True))\n        self.conv2 = nn.Conv2d(planes, planes, 3, 1, padding=dilation if dilation > 1 else pad, dilation=dilation)\n        self.downsample = downsample\n        self.stride = stride\n    def forward(self, x):\n        out = self.conv1(x)",
        "detail": "stereo.modeling.models.nmrf.submodule",
        "documentation": {}
    },
    {
        "label": "groupwise_correlation",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.submodule",
        "description": "stereo.modeling.models.nmrf.submodule",
        "peekOfCode": "def groupwise_correlation(fea1, fea2, num_groups):\n    B, C, H, W = fea1.shape\n    assert C % num_groups == 0\n    channels_per_group = C // num_groups\n    cost = (fea1 * fea2).view([B, num_groups, channels_per_group, H, W]).mean(dim=2)\n    assert cost.shape == (B, num_groups, H, W)\n    return cost\ndef build_correlation_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])",
        "detail": "stereo.modeling.models.nmrf.submodule",
        "documentation": {}
    },
    {
        "label": "build_correlation_volume",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.submodule",
        "description": "stereo.modeling.models.nmrf.submodule",
        "peekOfCode": "def build_correlation_volume(refimg_fea, targetimg_fea, maxdisp, num_groups):\n    B, C, H, W = refimg_fea.shape\n    volume = refimg_fea.new_zeros([B, num_groups, maxdisp, H, W])\n    for i in range(maxdisp):\n        if i > 0:\n            volume[:, :, i, :, i:] = groupwise_correlation(refimg_fea[:, :, :, i:], targetimg_fea[:, :, :, :-i],\n                                                           num_groups)\n        else:\n            volume[:, :, i, :, :] = groupwise_correlation(refimg_fea, targetimg_fea, num_groups)\n    volume = volume.contiguous()",
        "detail": "stereo.modeling.models.nmrf.submodule",
        "documentation": {}
    },
    {
        "label": "Mlp",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.swin",
        "description": "stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n    def __init__(\n        self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = act_layer()",
        "detail": "stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "WindowAttention",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.swin",
        "description": "stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class WindowAttention(nn.Module):\n    \"\"\"Window based multi-head self attention (W-MSA) module with relative position bias.\n    It supports both of shifted and non-shifted window.\n    Args:\n        dim (int): Number of input channels.\n        window_size (tuple[int]): The height and width of the window.\n        num_heads (int): Number of attention heads.\n        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value.\n            Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set",
        "detail": "stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "SwinTransformerBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.swin",
        "description": "stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class SwinTransformerBlock(nn.Module):\n    \"\"\"Swin Transformer Block.\n    Args:\n        dim (int): Number of input channels.\n        num_heads (int): Number of attention heads.\n        window_size (int): Window size.\n        shift_size (int): Shift size for SW-MSA.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "PatchMerging",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.swin",
        "description": "stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class PatchMerging(nn.Module):\n    \"\"\"Patch Merging Layer\n    Args:\n        dim (int): Number of input channels.\n        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n    \"\"\"\n    def __init__(self, dim, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)",
        "detail": "stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "BasicLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.swin",
        "description": "stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class BasicLayer(nn.Module):\n    \"\"\"A basic Swin Transformer layer for one stage.\n    Args:\n        dim (int): Number of feature channels\n        depth (int): Depths of this stage.\n        num_heads (int): Number of attention head.\n        window_size (int): Local window size. Default: 7.\n        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.",
        "detail": "stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "PatchEmbed",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.swin",
        "description": "stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class PatchEmbed(nn.Module):\n    \"\"\"Image to Patch Embedding\n    Args:\n        patch_size (int): Patch token size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.\n        norm_layer (nn.Module, optional): Normalization layer. Default: None\n    \"\"\"\n    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n        super().__init__()",
        "detail": "stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "SwinTransformer",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.swin",
        "description": "stereo.modeling.models.nmrf.swin",
        "peekOfCode": "class SwinTransformer(nn.Module):\n    \"\"\"Swin Transformer backbone.\n        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted\n            Windows`  - https://arxiv.org/pdf/2103.14030\n    Args:\n        pretrain_img_size (int): Input image size for training the pretrained model,\n            used in absolute postion embedding. Default 224.\n        patch_size (int | tuple(int)): Patch size. Default: 4.\n        in_chans (int): Number of input image channels. Default: 3.\n        embed_dim (int): Number of linear projection output channels. Default: 96.",
        "detail": "stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "window_partition",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.swin",
        "description": "stereo.modeling.models.nmrf.swin",
        "peekOfCode": "def window_partition(x, window_size):\n    \"\"\"\n    Args:\n        x: (B, H, W, C)\n        window_size (int): window size\n    Returns:\n        windows: (num_windows*B, window_size, window_size, C)\n    \"\"\"\n    B, H, W, C = x.shape\n    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)",
        "detail": "stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "window_reverse",
        "kind": 2,
        "importPath": "stereo.modeling.models.nmrf.swin",
        "description": "stereo.modeling.models.nmrf.swin",
        "peekOfCode": "def window_reverse(windows, window_size, H, W):\n    \"\"\"\n    Args:\n        windows: (num_windows*B, window_size, window_size, C)\n        window_size (int): Window size\n        H (int): Height of image\n        W (int): Width of image\n    Returns:\n        x: (B, H, W, C)\n    \"\"\"",
        "detail": "stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "_to_2tuple",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.swin",
        "description": "stereo.modeling.models.nmrf.swin",
        "peekOfCode": "_to_2tuple = nn.modules.utils._ntuple(2)\nclass Mlp(nn.Module):\n    \"\"\"Multilayer perceptron.\"\"\"\n    def __init__(\n        self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.0\n    ):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)",
        "detail": "stereo.modeling.models.nmrf.swin",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.nmrf.trainer",
        "description": "stereo.modeling.models.nmrf.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)\n        if self.args.run_mode == 'train':\n            self.optimizer = self.build_optimizer(self.model, cfgs.OPTIMIZATION.OPTIMIZER.LR)\n            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=self.optimizer, max_lr=cfgs.OPTIMIZATION.OPTIMIZER.LR,\n                                                                 total_steps=self.max_iter, pct_start=0.05,\n                                                                 cycle_momentum=False, anneal_strategy='cos')\n    def build_optimizer(self, params, base_lr):",
        "detail": "stereo.modeling.models.nmrf.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.nmrf.trainer",
        "description": "stereo.modeling.models.nmrf.trainer",
        "peekOfCode": "__all__ = {\n    'NMRF': NMRF,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)\n        if self.args.run_mode == 'train':\n            self.optimizer = self.build_optimizer(self.model, cfgs.OPTIMIZATION.OPTIMIZER.LR)\n            self.scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=self.optimizer, max_lr=cfgs.OPTIMIZATION.OPTIMIZER.LR,",
        "detail": "stereo.modeling.models.nmrf.trainer",
        "documentation": {}
    },
    {
        "label": "PSMNet",
        "kind": 6,
        "importPath": "stereo.modeling.models.psmnet.psmnet",
        "description": "stereo.modeling.models.psmnet.psmnet",
        "peekOfCode": "class PSMNet(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.maxdisp = cfgs.MAX_DISP\n        self.Backbone = PSMNetBackbone()\n        self.CostProcessor = PSMCostProcessor(max_disp=self.maxdisp)\n        self.DispProcessor = PSMDispProcessor(max_disp=self.maxdisp)\n    def forward(self, inputs):\n        \"\"\"Forward the network.\"\"\"\n        backbone_out = self.Backbone(inputs)",
        "detail": "stereo.modeling.models.psmnet.psmnet",
        "documentation": {}
    },
    {
        "label": "PSMNet",
        "kind": 6,
        "importPath": "stereo.modeling.models.psmnet.psmnet_backbone",
        "description": "stereo.modeling.models.psmnet.psmnet_backbone",
        "peekOfCode": "class PSMNet(nn.Module):\n    \"\"\"\n    Backbone proposed in PSMNet.\n    Args:\n        in_planes (int): the channels of input\n        batch_norm (bool): whether use batch normalization layer, default True\n    Inputs:\n        l_img (Tensor): left image, in [BatchSize, 3, Height, Width] layout\n        r_img (Tensor): right image, in [BatchSize, 3, Height, Width] layout\n    Outputs:",
        "detail": "stereo.modeling.models.psmnet.psmnet_backbone",
        "documentation": {}
    },
    {
        "label": "Hourglass",
        "kind": 6,
        "importPath": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "description": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "peekOfCode": "class Hourglass(nn.Module):\n    \"\"\"\n    An implementation of hourglass module proposed in PSMNet.\n    Args:\n        in_planes (int): the channels of raw cost volume\n        batch_norm (bool): whether use batch normalization layer,\n            default True\n    Inputs:\n        x, (Tensor): cost volume\n            in [BatchSize, in_planes, MaxDisparity, Height, Width] layout",
        "detail": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "PSMAggregator",
        "kind": 6,
        "importPath": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "description": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "peekOfCode": "class PSMAggregator(nn.Module):\n    \"\"\"\n    Args:\n        max_disp (int): max disparity\n        in_planes (int): the channels of raw cost volume\n        batch_norm (bool): whether use batch normalization layer, default True\n    Inputs:\n        raw_cost (Tensor): concatenation-based cost volume without further processing,\n            in [BatchSize, in_planes, MaxDisparity//4, Height//4, Width//4] layout\n    Outputs:",
        "detail": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "PSMCostProcessor",
        "kind": 6,
        "importPath": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "description": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "peekOfCode": "class PSMCostProcessor(nn.Module):\n    def __init__(self, max_disp=192, in_planes=64):\n        super().__init__()\n        self.cat_func = partial(\n            cat_fms,\n            max_disp=int(max_disp // 4),\n            start_disp=0,\n            dilation=1,\n        )\n        self.aggregator = PSMAggregator(",
        "detail": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "cat_fms",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "description": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "peekOfCode": "def cat_fms(reference_fm, target_fm, max_disp=192, start_disp=0, dilation=1):\n    \"\"\"\n    Concat left and right in Channel dimension to form the raw cost volume.\n    Args:\n        max_disp, (int): under the scale of feature used,\n            often equals to (end disp - start disp + 1), the maximum searching range of disparity\n        start_disp (int): the start searching disparity index, usually be 0\n            dilation (int): the step between near disparity index\n        dilation (int): the step between near disparity index\n    Inputs:",
        "detail": "stereo.modeling.models.psmnet.psmnet_cost_processor",
        "documentation": {}
    },
    {
        "label": "FasterSoftArgmin",
        "kind": 6,
        "importPath": "stereo.modeling.models.psmnet.psmnet_disp_processor",
        "description": "stereo.modeling.models.psmnet.psmnet_disp_processor",
        "peekOfCode": "class FasterSoftArgmin(nn.Module):\n    \"\"\"\n    A faster implementation of soft argmin.\n    details can refer to dmb.modeling.stereo.disp_predictors.soft_argmin\n    Args:\n        max_disp, (int): under the scale of feature used,\n            often equals to (end disp - start disp + 1), the maximum searching range of disparity\n        start_disp (int): the start searching disparity index, usually be 0\n        dilation (optional, int): the step between near disparity index\n        alpha (float or int): a factor will times with cost_volume",
        "detail": "stereo.modeling.models.psmnet.psmnet_disp_processor",
        "documentation": {}
    },
    {
        "label": "PSMDispProcessor",
        "kind": 6,
        "importPath": "stereo.modeling.models.psmnet.psmnet_disp_processor",
        "description": "stereo.modeling.models.psmnet.psmnet_disp_processor",
        "peekOfCode": "class PSMDispProcessor(nn.Module):\n    def __init__(self, max_disp=192):\n        super().__init__()\n        self.disp_processor = FasterSoftArgmin(\n            # the maximum disparity of disparity search range\n            max_disp=max_disp,\n            # the start disparity of disparity search range\n            start_disp=0,\n            # the step between near disparity sample\n            dilation=1,",
        "detail": "stereo.modeling.models.psmnet.psmnet_disp_processor",
        "documentation": {}
    },
    {
        "label": "BasicBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "class BasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, batchNorm, in_planes, out_planes, stride, downsample, padding, dilation):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv_bn_relu(\n            batchNorm=batchNorm, in_planes=in_planes, out_planes=out_planes,\n            kernel_size=3, stride=stride, padding=padding, dilation=dilation, bias=False\n        )\n        self.conv2 = conv_bn(\n            batchNorm=batchNorm, in_planes=out_planes, out_planes=out_planes,",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "consistent_padding_with_dilation",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def consistent_padding_with_dilation(padding, dilation, dim=2):\n    assert dim == 2 or dim == 3, 'Convolution layer only support 2D and 3D'\n    if dim == 2:\n        padding = _pair(padding)\n        dilation = _pair(dilation)\n    else:  # dim == 3\n        padding = _triple(padding)\n        dilation = _triple(dilation)\n    padding = list(padding)\n    for d in range(dim):",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv_bn",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def conv_bn(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=2)\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv2d(\n                in_planes, out_planes, kernel_size=kernel_size,\n                stride=stride, padding=padding, dilation=dilation, bias=bias\n            ),\n            nn.BatchNorm2d(out_planes),\n        )",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "deconv_bn",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def deconv_bn(batchNorm, in_planes, out_planes, kernel_size=4, stride=2, padding=1, output_padding=0, bias=True):\n    if batchNorm:\n        return nn.Sequential(\n            nn.ConvTranspose2d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, output_padding=output_padding, bias=bias\n            ),\n            nn.BatchNorm2d(out_planes),\n        )\n    else:",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv3d_bn",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def conv3d_bn(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=3)\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv3d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, dilation=dilation, bias=bias),\n            nn.BatchNorm3d(out_planes),\n        )\n    else:",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "deconv3d_bn",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def deconv3d_bn(batchNorm, in_planes, out_planes, kernel_size=4, stride=2, padding=1, output_padding=0, bias=True):\n    if batchNorm:\n        return nn.Sequential(\n            nn.ConvTranspose3d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, output_padding=output_padding, bias=bias),\n            nn.BatchNorm3d(out_planes),\n        )\n    else:\n        return nn.Sequential(",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv_bn_relu",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def conv_bn_relu(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=2)\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv2d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, dilation=dilation, bias=bias),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU(inplace=True),\n        )",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "bn_relu_conv",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def bn_relu_conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=2)\n    if batchNorm:\n        return nn.Sequential(\n            nn.BatchNorm2d(in_planes),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, dilation=dilation, bias=bias),\n        )",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "deconv_bn_relu",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def deconv_bn_relu(batchNorm, in_planes, out_planes, kernel_size=4, stride=2, padding=1, output_padding=0, bias=True):\n    if batchNorm:\n        return nn.Sequential(\n            nn.ConvTranspose2d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, output_padding=output_padding, bias=bias),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU(inplace=True),\n        )\n    else:",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "conv3d_bn_relu",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def conv3d_bn_relu(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=3)\n    if batchNorm:\n        return nn.Sequential(\n            nn.Conv3d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, dilation=dilation, bias=bias),\n            nn.BatchNorm3d(out_planes),\n            nn.ReLU(inplace=True),\n        )",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "bn_relu_conv3d",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def bn_relu_conv3d(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1, bias=True):\n    padding, dilation = consistent_padding_with_dilation(padding, dilation, dim=3)\n    if batchNorm:\n        return nn.Sequential(\n            nn.BatchNorm3d(in_planes),\n            nn.ReLU(inplace=True),\n            nn.Conv3d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, dilation=dilation, bias=bias),\n        )",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "deconv3d_bn_relu",
        "kind": 2,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "def deconv3d_bn_relu(batchNorm, in_planes, out_planes, kernel_size=4, stride=2, padding=1, output_padding=0, bias=True):\n    if batchNorm:\n        return nn.Sequential(\n            nn.ConvTranspose3d(\n                in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n                padding=padding, output_padding=output_padding, bias=bias),\n            nn.BatchNorm3d(out_planes),\n            nn.ReLU(inplace=True),\n        )\n    else:",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.psmnet.submodule",
        "description": "stereo.modeling.models.psmnet.submodule",
        "peekOfCode": "__all__ = ['conv_bn', 'deconv_bn',\n           'conv_bn_relu', 'bn_relu_conv', 'deconv_bn_relu',\n           'conv3d_bn', 'deconv3d_bn',\n           'conv3d_bn_relu', 'bn_relu_conv3d', 'deconv3d_bn_relu',\n           'BasicBlock',\n           ]\ndef consistent_padding_with_dilation(padding, dilation, dim=2):\n    assert dim == 2 or dim == 3, 'Convolution layer only support 2D and 3D'\n    if dim == 2:\n        padding = _pair(padding)",
        "detail": "stereo.modeling.models.psmnet.submodule",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.psmnet.trainer",
        "description": "stereo.modeling.models.psmnet.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.psmnet.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.psmnet.trainer",
        "description": "stereo.modeling.models.psmnet.trainer",
        "peekOfCode": "__all__ = {\n    'PSMNet': PSMNet,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.psmnet.trainer",
        "documentation": {}
    },
    {
        "label": "SubModule",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.backbone",
        "description": "stereo.modeling.models.stereobase.backbone",
        "peekOfCode": "class SubModule(nn.Module):\n    def __init__(self):\n        super(SubModule, self).__init__()\n    def weight_init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.Conv3d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels",
        "detail": "stereo.modeling.models.stereobase.backbone",
        "documentation": {}
    },
    {
        "label": "Feature",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.backbone",
        "description": "stereo.modeling.models.stereobase.backbone",
        "peekOfCode": "class Feature(SubModule):\n    def __init__(self):\n        super(Feature, self).__init__()\n        model = timm.create_model('mobilenetv2_100', pretrained=True, features_only=True)\n        layers = [1, 2, 3, 5, 6]\n        chans = [16, 24, 32, 96, 160]\n        self.conv_stem = model.conv_stem\n        self.bn1 = model.bn1\n        self.act1 = model.act1\n        self.block0 = torch.nn.Sequential(*model.blocks[0:layers[0]])",
        "detail": "stereo.modeling.models.stereobase.backbone",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.gru_blocks",
        "description": "stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class ResidualBlock(nn.Module):\n    def __init__(self, in_planes, planes, norm_fn='group', stride=1):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, stride=stride)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n        num_groups = planes // 8\n        if norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)\n            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=planes)",
        "detail": "stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "MultiBasicEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.gru_blocks",
        "description": "stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class MultiBasicEncoder(nn.Module):\n    def __init__(self, output_dim=None, norm_fn='batch', downsample=3):\n        super(MultiBasicEncoder, self).__init__()\n        if output_dim is None:\n            output_dim = [128]\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1 + (downsample > 2), padding=3)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.norm_fn = norm_fn\n        if self.norm_fn == 'group':\n            self.norm1 = nn.GroupNorm(num_groups=8, num_channels=64)",
        "detail": "stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "CombinedGeoEncodingVolume",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.gru_blocks",
        "description": "stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class CombinedGeoEncodingVolume:\n    def __init__(self, init_fmap1, init_fmap2, geo_volume, num_levels=2, radius=4):\n        self.num_levels = num_levels\n        self.radius = radius\n        self.geo_volume_pyramid = []\n        self.init_corr_pyramid = []\n        # all pairs correlation\n        init_corr = self.corr(init_fmap1, init_fmap2)  # [bz, H/4, W/4, 1, W/4]\n        b, h, w, _, w2 = init_corr.shape\n        b, c, d, h, w = geo_volume.shape  # [bz, channel, maxdisp/4, H/4, W/4]",
        "detail": "stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "BasicMotionEncoder",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.gru_blocks",
        "description": "stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class BasicMotionEncoder(nn.Module):\n    def __init__(self, corr_levels, corr_radius, volume_channel):\n        super(BasicMotionEncoder, self).__init__()\n        cor_planes = corr_levels * (2 * corr_radius + 1) * (volume_channel + 1)\n        self.convc1 = nn.Conv2d(cor_planes, 64, 1, padding=0)\n        self.convc2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.convd1 = nn.Conv2d(1, 64, 7, padding=3)\n        self.convd2 = nn.Conv2d(64, 64, 3, padding=1)\n        self.conv = nn.Conv2d(64 + 64, 128 - 1, 3, padding=1)\n    def forward(self, disp, corr):",
        "detail": "stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "ConvGRU",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.gru_blocks",
        "description": "stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class ConvGRU(nn.Module):\n    def __init__(self, hidden_dim, input_dim, kernel_size=3):\n        super(ConvGRU, self).__init__()\n        self.convz = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convr = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n        self.convq = nn.Conv2d(hidden_dim + input_dim, hidden_dim, kernel_size, padding=kernel_size // 2)\n    def forward(self, h, cz, cr, cq, *x_list):\n        x = torch.cat(x_list, dim=1)\n        hx = torch.cat([h, x], dim=1)\n        z = torch.sigmoid(self.convz(hx) + cz)",
        "detail": "stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "DispHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.gru_blocks",
        "description": "stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class DispHead(nn.Module):\n    def __init__(self, input_dim=128, hidden_dim=256, output_dim=1):\n        super(DispHead, self).__init__()\n        self.conv1 = nn.Conv2d(input_dim, hidden_dim, 3, padding=1)\n        self.conv2 = nn.Conv2d(hidden_dim, output_dim, 3, padding=1)\n        self.relu = nn.ReLU(inplace=True)\n    def forward(self, x):\n        return self.conv2(self.relu(self.conv1(x)))\ndef pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)",
        "detail": "stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "BasicMultiUpdateBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.gru_blocks",
        "description": "stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "class BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []\n        self.n_gru_layers = n_gru_layers\n        self.encoder = BasicMotionEncoder(corr_levels, corr_radius, volume_channel)\n        encoder_output_dim = 128\n        self.gru04 = ConvGRU(hidden_dims[2], encoder_output_dim + hidden_dims[1] * (self.n_gru_layers > 1))\n        self.gru08 = ConvGRU(hidden_dims[1], hidden_dims[0] * (self.n_gru_layers == 3) + hidden_dims[2])",
        "detail": "stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "bilinear_sampler",
        "kind": 2,
        "importPath": "stereo.modeling.models.stereobase.gru_blocks",
        "description": "stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "def bilinear_sampler(img, coords, mask=False):\n    \"\"\" Wrapper for grid_sample, uses pixel coordinates \"\"\"\n    H, W = img.shape[-2:]\n    xgrid, ygrid = coords.split([1, 1], dim=-1)\n    xgrid = 2 * xgrid / (W - 1) - 1\n    assert torch.unique(ygrid).numel() == 1 and H == 1  # This is a stereo problem\n    grid = torch.cat([xgrid, ygrid], dim=-1)\n    img = F.grid_sample(img, grid, align_corners=True)\n    if mask:\n        mask = (xgrid > -1) & (ygrid > -1) & (xgrid < 1) & (ygrid < 1)",
        "detail": "stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "pool2x",
        "kind": 2,
        "importPath": "stereo.modeling.models.stereobase.gru_blocks",
        "description": "stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "def pool2x(x):\n    return F.avg_pool2d(x, 3, stride=2, padding=1)\ndef interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []",
        "detail": "stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "interp",
        "kind": 2,
        "importPath": "stereo.modeling.models.stereobase.gru_blocks",
        "description": "stereo.modeling.models.stereobase.gru_blocks",
        "peekOfCode": "def interp(x, dest):\n    interp_args = {'mode': 'bilinear', 'align_corners': True}\n    return F.interpolate(x, dest.shape[2:], **interp_args)\nclass BasicMultiUpdateBlock(nn.Module):\n    def __init__(self, n_gru_layers, corr_levels, corr_radius, volume_channel, hidden_dims=None):\n        super().__init__()\n        if hidden_dims is None:\n            hidden_dims = []\n        self.n_gru_layers = n_gru_layers\n        self.encoder = BasicMotionEncoder(corr_levels, corr_radius, volume_channel)",
        "detail": "stereo.modeling.models.stereobase.gru_blocks",
        "documentation": {}
    },
    {
        "label": "Hourglass",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.hourglass",
        "description": "stereo.modeling.models.stereobase.hourglass",
        "peekOfCode": "class Hourglass(nn.Module):\n    def __init__(self, in_channels, backbone_channels=None):\n        super(Hourglass, self).__init__()\n        if backbone_channels is None:\n            backbone_channels = [48, 64, 192, 120]\n        self.conv1 = nn.Sequential(\n            BasicConv3d(in_channels, in_channels * 2,\n                        norm_layer=nn.BatchNorm3d, act_layer=nn.LeakyReLU,\n                        kernel_size=3, padding=1, stride=2, dilation=1),\n            BasicConv3d(in_channels * 2, in_channels * 2,",
        "detail": "stereo.modeling.models.stereobase.hourglass",
        "documentation": {}
    },
    {
        "label": "Conv2xUp",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.igev_blocks",
        "description": "stereo.modeling.models.stereobase.igev_blocks",
        "peekOfCode": "class Conv2xUp(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer, concat=True):\n        super(Conv2xUp, self).__init__()\n        self.concat = concat\n        self.conv1 = BasicDeconv2d(in_channels, out_channels,\n                                   norm_layer=norm_layer, act_layer=nn.LeakyReLU,\n                                   kernel_size=4, stride=2, padding=1)\n        self.conv2 = BasicConv2d(out_channels * 2, out_channels * 2,\n                                 norm_layer=norm_layer, act_layer=nn.LeakyReLU,\n                                 kernel_size=3, stride=1, padding=1)",
        "detail": "stereo.modeling.models.stereobase.igev_blocks",
        "documentation": {}
    },
    {
        "label": "FeatureAtt",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.igev_blocks",
        "description": "stereo.modeling.models.stereobase.igev_blocks",
        "peekOfCode": "class FeatureAtt(nn.Module):\n    def __init__(self, cv_chan, feat_chan):\n        super(FeatureAtt, self).__init__()\n        self.feat_att = nn.Sequential(\n            BasicConv2d(feat_chan, feat_chan // 2,\n                        norm_layer=nn.BatchNorm2d, act_layer=nn.LeakyReLU,\n                        kernel_size=1, stride=1, padding=0),\n            nn.Conv2d(feat_chan // 2, cv_chan, 1))\n    def forward(self, cv, feat):\n        feat_att = self.feat_att(feat).unsqueeze(2)",
        "detail": "stereo.modeling.models.stereobase.igev_blocks",
        "documentation": {}
    },
    {
        "label": "context_upsample",
        "kind": 2,
        "importPath": "stereo.modeling.models.stereobase.igev_blocks",
        "description": "stereo.modeling.models.stereobase.igev_blocks",
        "peekOfCode": "def context_upsample(disp_low, up_weights, scale_factor=4):\n    ###\n    # disp_low (b,1,h,w)\n    # up_weights (b,9,4*h,4*w)\n    ###\n    b, c, h, w = disp_low.shape\n    disp_unfold = F.unfold(disp_low.reshape(b, c, h, w), 3, 1, 1).reshape(b, -1, h, w)\n    disp_unfold = F.interpolate(disp_unfold, (h * scale_factor, w * scale_factor), mode='nearest').reshape(b, 9, h * scale_factor, w * scale_factor)\n    disp = (disp_unfold * up_weights).sum(1)\n    return disp",
        "detail": "stereo.modeling.models.stereobase.igev_blocks",
        "documentation": {}
    },
    {
        "label": "StereoBase",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.stereobase_gru",
        "description": "stereo.modeling.models.stereobase.stereobase_gru",
        "peekOfCode": "class StereoBase(nn.Module):\n    def __init__(self, cfgs):\n        super().__init__()\n        self.cfgs = cfgs\n        self.max_disp = cfgs.MAX_DISP\n        self.num_groups = cfgs.get('NUM_GROUPS', 8)\n        self.use_concat_volume = cfgs.get('USE_CONCAT_VOLUME', False)\n        self.use_gwc_volume = cfgs.get('USE_GWC_VOLUME', True)\n        self.use_sub_volume = cfgs.get('USE_SUB_VOLUME', False)\n        self.use_interlaced_volume = cfgs.get('USE_INTERLACED_VOLUME', False)",
        "detail": "stereo.modeling.models.stereobase.stereobase_gru",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.stereobase.trainer",
        "description": "stereo.modeling.models.stereobase.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.stereobase.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.stereobase.trainer",
        "description": "stereo.modeling.models.stereobase.trainer",
        "peekOfCode": "__all__ = {\n    'StereoBaseGRU': StereoBaseGRU,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.stereobase.trainer",
        "documentation": {}
    },
    {
        "label": "MultiheadAttentionRelative",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.attention",
        "description": "stereo.modeling.models.sttr.utilities.attention",
        "peekOfCode": "class MultiheadAttentionRelative(nn.MultiheadAttention):\n    \"\"\"\n    Multihead attention with relative positional encoding\n    \"\"\"\n    def __init__(self, embed_dim, num_heads):\n        super(MultiheadAttentionRelative, self).__init__(embed_dim, num_heads, dropout=0.0, bias=True,\n                                                         add_bias_kv=False, add_zero_attn=False,\n                                                         kdim=None, vdim=None)\n    def forward(self, query, key, value, attn_mask=None, pos_enc=None, pos_indexes=None):\n        \"\"\"",
        "detail": "stereo.modeling.models.sttr.utilities.attention",
        "documentation": {}
    },
    {
        "label": "ContextAdjustmentLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "description": "stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "peekOfCode": "class ContextAdjustmentLayer(nn.Module):\n    \"\"\"\n    Adjust the disp and occ based on image context, design loosely follows https://github.com/JiahuiYu/wdsr_ntire2018\n    \"\"\"\n    def __init__(self, num_blocks=8, feature_dim=16, expansion=3):\n        super().__init__()\n        self.num_blocks = num_blocks\n        # disp head\n        self.in_conv = nn.Conv2d(4, feature_dim, kernel_size=3, padding=1)\n        self.layers = nn.ModuleList([ResBlock(feature_dim, expansion) for _ in range(num_blocks)])",
        "detail": "stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "description": "stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "peekOfCode": "class ResBlock(nn.Module):\n    def __init__(self, n_feats: int, expansion_ratio: int, res_scale: int = 1.0):\n        super(ResBlock, self).__init__()\n        self.res_scale = res_scale\n        self.module = nn.Sequential(\n            weight_norm(nn.Conv2d(n_feats + 1, n_feats * expansion_ratio, kernel_size=3, padding=1)),\n            nn.ReLU(inplace=True),\n            weight_norm(nn.Conv2d(n_feats * expansion_ratio, n_feats, kernel_size=3, padding=1))\n        )\n    def forward(self, x: torch.Tensor, disp: torch.Tensor):",
        "detail": "stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "documentation": {}
    },
    {
        "label": "build_context_adjustment_layer",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "description": "stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "peekOfCode": "def build_context_adjustment_layer(args):\n    if args.CONTEXT_ADJUSTMENT_LAYER == 'cal':\n        return ContextAdjustmentLayer(args.CAL_NUM_BLOCKS, args.CAL_FEAT_DIM,\n                                      args.CAL_EXPANSION_RATIO)\n    elif args.CONTEXT_ADJUSTMENT_LAYER == 'none':\n        return None\n    else:\n        raise ValueError(f'Context adjustment layer option not recognized: {args.context_adjustment_layer}')",
        "detail": "stereo.modeling.models.sttr.utilities.context_adjustment_layer",
        "documentation": {}
    },
    {
        "label": "_DenseLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.densenet_in",
        "description": "stereo.modeling.models.sttr.utilities.densenet_in",
        "peekOfCode": "class _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm1', nn.InstanceNorm2d(num_input_features)),\n        self.add_module('relu1', nn.ReLU(inplace=True)),\n        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n                                           growth_rate, kernel_size=1, stride=1,\n                                           bias=False)),\n        self.add_module('norm2', nn.InstanceNorm2d(bn_size * growth_rate)),\n        self.add_module('relu2', nn.ReLU(inplace=True)),",
        "detail": "stereo.modeling.models.sttr.utilities.densenet_in",
        "documentation": {}
    },
    {
        "label": "_DenseBlock",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.densenet_in",
        "description": "stereo.modeling.models.sttr.utilities.densenet_in",
        "peekOfCode": "class _DenseBlock(nn.ModuleDict):\n    _version = 2\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(\n                num_input_features + i * growth_rate,\n                growth_rate=growth_rate,\n                bn_size=bn_size,\n                drop_rate=drop_rate,",
        "detail": "stereo.modeling.models.sttr.utilities.densenet_in",
        "documentation": {}
    },
    {
        "label": "_Transition",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.densenet_in",
        "description": "stereo.modeling.models.sttr.utilities.densenet_in",
        "peekOfCode": "class _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module('norm', nn.InstanceNorm2d(num_input_features))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))",
        "detail": "stereo.modeling.models.sttr.utilities.densenet_in",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.sttr.utilities.densenet_in",
        "description": "stereo.modeling.models.sttr.utilities.densenet_in",
        "peekOfCode": "__all__ = ['DenseNet', 'densenet121', 'densenet169', 'densenet201', 'densenet161']\nmodel_urls = {\n    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n}\nclass _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n        super(_DenseLayer, self).__init__()",
        "detail": "stereo.modeling.models.sttr.utilities.densenet_in",
        "documentation": {}
    },
    {
        "label": "model_urls",
        "kind": 5,
        "importPath": "stereo.modeling.models.sttr.utilities.densenet_in",
        "description": "stereo.modeling.models.sttr.utilities.densenet_in",
        "peekOfCode": "model_urls = {\n    'densenet121': 'https://download.pytorch.org/models/densenet121-a639ec97.pth',\n    'densenet169': 'https://download.pytorch.org/models/densenet169-b2777c0a.pth',\n    'densenet201': 'https://download.pytorch.org/models/densenet201-c1103571.pth',\n    'densenet161': 'https://download.pytorch.org/models/densenet161-8d451a50.pth',\n}\nclass _DenseLayer(nn.Module):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm1', nn.InstanceNorm2d(num_input_features)),",
        "detail": "stereo.modeling.models.sttr.utilities.densenet_in",
        "documentation": {}
    },
    {
        "label": "SppBackbone",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "description": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "peekOfCode": "class SppBackbone(nn.Module):\n    \"\"\"\n    Contracting path of feature descriptor using Spatial Pyramid Pooling,\n    SPP followed by PSMNet (https://github.com/JiaRenChang/PSMNet)\n    \"\"\"\n    def __init__(self):\n        super(SppBackbone, self).__init__()\n        self.inplanes = 32\n        self.in_conv = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, padding=1, stride=2, bias=False),\n                                     nn.BatchNorm2d(16),",
        "detail": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "description": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "peekOfCode": "def build_backbone(args):\n    return SppBackbone()",
        "detail": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone",
        "documentation": {}
    },
    {
        "label": "SppBackbone",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "description": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "peekOfCode": "class SppBackbone(nn.Module):\n    \"\"\"\n    Contracting path of feature descriptor using Spatial Pyramid Pooling,\n    SPP followed by PSMNet (https://github.com/JiaRenChang/PSMNet)\n    \"\"\"\n    def __init__(self):\n        super(SppBackbone, self).__init__()\n        self.inplanes = 32\n        self.in_conv = nn.Sequential(nn.Conv2d(3, 16, kernel_size=3, padding=1, stride=2, bias=False),\n                                     nn.InstanceNorm2d(16),",
        "detail": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "documentation": {}
    },
    {
        "label": "build_backbone",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "description": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "peekOfCode": "def build_backbone(args):\n    return SppBackbone()",
        "detail": "stereo.modeling.models.sttr.utilities.feat_extractor_backbone_in",
        "documentation": {}
    },
    {
        "label": "TransitionUp",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "description": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "peekOfCode": "class TransitionUp(nn.Module):\n    \"\"\"\n    Scale the resolution up by transposed convolution\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int, scale: int = 2):\n        super().__init__()\n        if scale == 2:\n            self.convTrans = nn.ConvTranspose2d(\n                in_channels=in_channels, out_channels=out_channels,\n                kernel_size=3, stride=2, padding=0, bias=True)",
        "detail": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "documentation": {}
    },
    {
        "label": "DoubleConv",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "description": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "peekOfCode": "class DoubleConv(nn.Module):\n    \"\"\"\n    Two conv2d-bn-relu modules\n    \"\"\"\n    def __init__(self, in_channels: int, out_channels: int):\n        super(DoubleConv, self).__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True),",
        "detail": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "description": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "peekOfCode": "class Tokenizer(nn.Module):\n    \"\"\"\n    Expanding path of feature descriptor using DenseBlocks\n    \"\"\"\n    def __init__(self, block_config: list, backbone_feat_channel: list, hidden_dim: int, growth_rate: int):\n        super(Tokenizer, self).__init__()\n        backbone_feat_channel.reverse()  # reverse so we have high-level first (lowest-spatial res)\n        block_config.reverse()\n        self.num_resolution = len(backbone_feat_channel)\n        self.block_config = block_config",
        "detail": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "documentation": {}
    },
    {
        "label": "build_tokenizer",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "description": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "peekOfCode": "def build_tokenizer(args, layer_channel):\n    growth_rate = 4\n    block_config = [4, 4, 4, 4]\n    return Tokenizer(block_config, layer_channel, args.CHANNEL_DIM, growth_rate)",
        "detail": "stereo.modeling.models.sttr.utilities.feat_extractor_tokenizer",
        "documentation": {}
    },
    {
        "label": "Criterion",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.loss",
        "description": "stereo.modeling.models.sttr.utilities.loss",
        "peekOfCode": "class Criterion(nn.Module):\n    \"\"\"\n    Compute loss and evaluation metrics\n    \"\"\"\n    def __init__(self, threshold: int = 3, validation_max_disp: int = -1, loss_weight: list = None):\n        super(Criterion, self).__init__()\n        if loss_weight is None:\n            loss_weight = {}\n        self.px_threshold = threshold\n        self.validation_max_disp = validation_max_disp",
        "detail": "stereo.modeling.models.sttr.utilities.loss",
        "documentation": {}
    },
    {
        "label": "build_criterion",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.loss",
        "description": "stereo.modeling.models.sttr.utilities.loss",
        "peekOfCode": "def build_criterion(args):\n    loss_weight = {}\n    for weight in args.loss_weight.split(','):\n        k, v = weight.split(':')\n        k = k.strip()\n        v = float(v)\n        loss_weight[k] = v\n    return Criterion(args.px_error_threshold, args.validation_max_disp, loss_weight)",
        "detail": "stereo.modeling.models.sttr.utilities.loss",
        "documentation": {}
    },
    {
        "label": "NestedTensor",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.misc",
        "description": "stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "class NestedTensor(object):\n    def __init__(self, left, right, disp=None, sampled_cols=None, sampled_rows=None, occ_mask=None,\n                 occ_mask_right=None):\n        self.left = left\n        self.right = right\n        self.disp = disp\n        self.occ_mask = occ_mask\n        self.occ_mask_right = occ_mask_right\n        self.sampled_cols = sampled_cols\n        self.sampled_rows = sampled_rows",
        "detail": "stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "center_crop",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.misc",
        "description": "stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def center_crop(layer, max_height, max_width):\n    _, _, h, w = layer.size()\n    xy1 = (w - max_width) // 2\n    xy2 = (h - max_height) // 2\n    return layer[:, :, xy2:(xy2 + max_height), xy1:(xy1 + max_width)]\ndef batched_index_select(source, dim, index):\n    views = [source.shape[0]] + [1 if i != dim else -1 for i in range(1, len(source.shape))]\n    expanse = list(source.shape)\n    expanse[0] = -1\n    expanse[dim] = -1",
        "detail": "stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "batched_index_select",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.misc",
        "description": "stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def batched_index_select(source, dim, index):\n    views = [source.shape[0]] + [1 if i != dim else -1 for i in range(1, len(source.shape))]\n    expanse = list(source.shape)\n    expanse[0] = -1\n    expanse[dim] = -1\n    index = index.view(views).expand(expanse)\n    return torch.gather(source, dim, index)\ndef torch_1d_sample(source, sample_points, mode='linear'):\n    \"\"\"\n    linearly sample source tensor along the last dimension",
        "detail": "stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "torch_1d_sample",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.misc",
        "description": "stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def torch_1d_sample(source, sample_points, mode='linear'):\n    \"\"\"\n    linearly sample source tensor along the last dimension\n    input:\n        source [N,D1,D2,D3...,Dn]\n        sample_points [N,D1,D2,....,Dn-1,1]\n    output:\n        [N,D1,D2...,Dn-1]\n    \"\"\"\n    idx_l = torch.floor(sample_points).long().clamp(0, source.size(-1) - 1)",
        "detail": "stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "get_clones",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.misc",
        "description": "stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def get_clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\ndef find_occ_mask(disp_left, disp_right):\n    \"\"\"\n    find occlusion map\n    1 indicates occlusion\n    disp range [0,w]\n    \"\"\"\n    w = disp_left.shape[-1]\n    # # left occlusion",
        "detail": "stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "find_occ_mask",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.misc",
        "description": "stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def find_occ_mask(disp_left, disp_right):\n    \"\"\"\n    find occlusion map\n    1 indicates occlusion\n    disp range [0,w]\n    \"\"\"\n    w = disp_left.shape[-1]\n    # # left occlusion\n    # find corresponding pixels in target image\n    coord = np.linspace(0, w - 1, w)[None,]  # 1xW",
        "detail": "stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "save_and_clear",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.misc",
        "description": "stereo.modeling.models.sttr.utilities.misc",
        "peekOfCode": "def save_and_clear(idx, output_file):\n    with open('output-' + str(idx) + '.dat', 'wb') as f:\n        torch.save(output_file, f)\n    idx += 1\n    # clear\n    for key in output_file:\n        output_file[key].clear()\n    return idx",
        "detail": "stereo.modeling.models.sttr.utilities.misc",
        "documentation": {}
    },
    {
        "label": "PositionEncodingSine1DRelative",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.pos_encoder",
        "description": "stereo.modeling.models.sttr.utilities.pos_encoder",
        "peekOfCode": "class PositionEncodingSine1DRelative(nn.Module):\n    \"\"\"\n    relative sine encoding 1D, partially inspired by DETR (https://github.com/facebookresearch/detr)\n    \"\"\"\n    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n        super().__init__()\n        self.num_pos_feats = num_pos_feats\n        self.temperature = temperature\n        self.normalize = normalize\n        if scale is not None and normalize is False:",
        "detail": "stereo.modeling.models.sttr.utilities.pos_encoder",
        "documentation": {}
    },
    {
        "label": "no_pos_encoding",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.pos_encoder",
        "description": "stereo.modeling.models.sttr.utilities.pos_encoder",
        "peekOfCode": "def no_pos_encoding(x):\n    return None\ndef build_position_encoding(args):\n    mode = args.POSITION_ENCODING\n    channel_dim = args.CHANNEL_DIM\n    if mode == 'sine1d_rel':\n        n_steps = channel_dim\n        position_encoding = PositionEncodingSine1DRelative(n_steps, normalize=False)\n    elif mode == 'none':\n        position_encoding = no_pos_encoding",
        "detail": "stereo.modeling.models.sttr.utilities.pos_encoder",
        "documentation": {}
    },
    {
        "label": "build_position_encoding",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.pos_encoder",
        "description": "stereo.modeling.models.sttr.utilities.pos_encoder",
        "peekOfCode": "def build_position_encoding(args):\n    mode = args.POSITION_ENCODING\n    channel_dim = args.CHANNEL_DIM\n    if mode == 'sine1d_rel':\n        n_steps = channel_dim\n        position_encoding = PositionEncodingSine1DRelative(n_steps, normalize=False)\n    elif mode == 'none':\n        position_encoding = no_pos_encoding\n    else:\n        raise ValueError(f\"not supported {mode}\")",
        "detail": "stereo.modeling.models.sttr.utilities.pos_encoder",
        "documentation": {}
    },
    {
        "label": "RegressionHead",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.regression_head",
        "description": "stereo.modeling.models.sttr.utilities.regression_head",
        "peekOfCode": "class RegressionHead(nn.Module):\n    \"\"\"\n    Regress disparity and occlusion mask\n    \"\"\"\n    def __init__(self, cal: nn.Module, ot: bool = True):\n        super(RegressionHead, self).__init__()\n        self.cal = cal\n        self.ot = ot\n        self.phi = nn.Parameter(torch.tensor(0.0, requires_grad=True))  # dustbin cost\n    def _compute_unscaled_pos_shift(self, w: int, device: torch.device):",
        "detail": "stereo.modeling.models.sttr.utilities.regression_head",
        "documentation": {}
    },
    {
        "label": "build_regression_head",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.regression_head",
        "description": "stereo.modeling.models.sttr.utilities.regression_head",
        "peekOfCode": "def build_regression_head(args):\n    cal = build_context_adjustment_layer(args)\n    if args.REGRESSION_HEAD == 'ot':\n        ot = True\n    elif args.REGRESSION_HEAD == 'softmax':\n        ot = False\n    else:\n        raise Exception('Regression head type not recognized: ', args.regression_head)\n    return RegressionHead(cal, ot)",
        "detail": "stereo.modeling.models.sttr.utilities.regression_head",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.transformer",
        "description": "stereo.modeling.models.sttr.utilities.transformer",
        "peekOfCode": "class Transformer(nn.Module):\n    \"\"\"\n    Transformer computes self (intra image) and cross (inter image) attention\n    \"\"\"\n    def __init__(self, hidden_dim: int = 128, nhead: int = 8, num_attn_layers: int = 6):\n        super().__init__()\n        self_attn_layer = TransformerSelfAttnLayer(hidden_dim, nhead)\n        self.self_attn_layers = get_clones(self_attn_layer, num_attn_layers)\n        cross_attn_layer = TransformerCrossAttnLayer(hidden_dim, nhead)\n        self.cross_attn_layers = get_clones(cross_attn_layer, num_attn_layers)",
        "detail": "stereo.modeling.models.sttr.utilities.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerSelfAttnLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.transformer",
        "description": "stereo.modeling.models.sttr.utilities.transformer",
        "peekOfCode": "class TransformerSelfAttnLayer(nn.Module):\n    \"\"\"\n    Self attention layer\n    \"\"\"\n    def __init__(self, hidden_dim: int, nhead: int):\n        super().__init__()\n        self.self_attn = MultiheadAttentionRelative(hidden_dim, nhead)\n        self.norm1 = nn.LayerNorm(hidden_dim)\n    def forward(self, feat: Tensor,\n                pos: Optional[Tensor] = None,",
        "detail": "stereo.modeling.models.sttr.utilities.transformer",
        "documentation": {}
    },
    {
        "label": "TransformerCrossAttnLayer",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.utilities.transformer",
        "description": "stereo.modeling.models.sttr.utilities.transformer",
        "peekOfCode": "class TransformerCrossAttnLayer(nn.Module):\n    \"\"\"\n    Cross attention layer\n    \"\"\"\n    def __init__(self, hidden_dim: int, nhead: int):\n        super().__init__()\n        self.cross_attn = MultiheadAttentionRelative(hidden_dim, nhead)\n        self.norm1 = nn.LayerNorm(hidden_dim)\n        self.norm2 = nn.LayerNorm(hidden_dim)\n    def forward(self, feat_left: Tensor, feat_right: Tensor,",
        "detail": "stereo.modeling.models.sttr.utilities.transformer",
        "documentation": {}
    },
    {
        "label": "build_transformer",
        "kind": 2,
        "importPath": "stereo.modeling.models.sttr.utilities.transformer",
        "description": "stereo.modeling.models.sttr.utilities.transformer",
        "peekOfCode": "def build_transformer(args):\n    return Transformer(\n        hidden_dim=args.CHANNEL_DIM,\n        nhead=args.NHEADS,\n        num_attn_layers=args.NUM_ATTN_LAYERS\n    )",
        "detail": "stereo.modeling.models.sttr.utilities.transformer",
        "documentation": {}
    },
    {
        "label": "layer_idx",
        "kind": 5,
        "importPath": "stereo.modeling.models.sttr.utilities.transformer",
        "description": "stereo.modeling.models.sttr.utilities.transformer",
        "peekOfCode": "layer_idx = 0\nclass Transformer(nn.Module):\n    \"\"\"\n    Transformer computes self (intra image) and cross (inter image) attention\n    \"\"\"\n    def __init__(self, hidden_dim: int = 128, nhead: int = 8, num_attn_layers: int = 6):\n        super().__init__()\n        self_attn_layer = TransformerSelfAttnLayer(hidden_dim, nhead)\n        self.self_attn_layers = get_clones(self_attn_layer, num_attn_layers)\n        cross_attn_layer = TransformerCrossAttnLayer(hidden_dim, nhead)",
        "detail": "stereo.modeling.models.sttr.utilities.transformer",
        "documentation": {}
    },
    {
        "label": "STTR",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.sttr",
        "description": "stereo.modeling.models.sttr.sttr",
        "peekOfCode": "class STTR(nn.Module):\n    \"\"\"\n    STTR: it consists of\n        - backbone: contracting path of feature descriptor\n        - tokenizer: expanding path of feature descriptor\n        - pos_encoder: generates relative sine pos encoding\n        - transformer: computes self and cross attention\n        - regression_head: regresses disparity and occlusion, including optimal transport\n    \"\"\"\n    def __init__(self, args):",
        "detail": "stereo.modeling.models.sttr.sttr",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "kind": 6,
        "importPath": "stereo.modeling.models.sttr.trainer",
        "description": "stereo.modeling.models.sttr.trainer",
        "peekOfCode": "class Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.sttr.trainer",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "stereo.modeling.models.sttr.trainer",
        "description": "stereo.modeling.models.sttr.trainer",
        "peekOfCode": "__all__ = {\n    'STTR': STTR,\n}\nclass Trainer(TrainerTemplate):\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer):\n        model = __all__[cfgs.MODEL.NAME](cfgs.MODEL)\n        super().__init__(args, cfgs, local_rank, global_rank, logger, tb_writer, model)",
        "detail": "stereo.modeling.models.sttr.trainer",
        "documentation": {}
    },
    {
        "label": "TrainerTemplate",
        "kind": 6,
        "importPath": "stereo.modeling.trainer_template",
        "description": "stereo.modeling.trainer_template",
        "peekOfCode": "class TrainerTemplate:\n    def __init__(self, args, cfgs, local_rank, global_rank, logger, tb_writer, model):\n        self.args = args\n        self.cfgs = cfgs\n        self.local_rank = local_rank\n        self.global_rank = global_rank\n        self.logger = logger\n        self.tb_writer = tb_writer\n        self.model = self.build_model(model)\n        if self.args.run_mode in ['train', 'eval']:",
        "detail": "stereo.modeling.trainer_template",
        "documentation": {}
    },
    {
        "label": "ClipGrad",
        "kind": 6,
        "importPath": "stereo.utils.clip_grad",
        "description": "stereo.utils.clip_grad",
        "peekOfCode": "class ClipGrad:\n    def __init__(self, clip_type=\"None\", clip_value=0.1, max_norm=35, norm_type=2):\n        self.clip_type = clip_type\n        self.clip_value = clip_value\n        self.max_norm = max_norm\n        self.norm_type = norm_type\n    def __call__(self, model):\n        if self.clip_type == 'value':\n            torch.nn.utils.clip_grad_value_(model.parameters(), self.clip_value)\n        elif self.clip_type == 'norm':",
        "detail": "stereo.utils.clip_grad",
        "documentation": {}
    },
    {
        "label": "config_loader",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def config_loader(path):\n    with open(path, 'r') as stream:\n        src_cfgs = yaml.safe_load(stream)\n    return src_cfgs\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "set_random_seed",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\ndef create_logger(log_file=None, rank=0, log_level=logging.INFO):\n    logger = logging.getLogger(__name__)\n    logger.setLevel(log_level if rank == 0 else 'ERROR')",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "create_logger",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def create_logger(log_file=None, rank=0, log_level=logging.INFO):\n    logger = logging.getLogger(__name__)\n    logger.setLevel(log_level if rank == 0 else 'ERROR')\n    formatter = logging.Formatter('%(asctime)s  %(levelname)5s  %(message)s')\n    console = logging.StreamHandler()\n    console.setLevel(log_level if rank == 0 else 'ERROR')\n    console.setFormatter(formatter)\n    logger.addHandler(console)\n    if log_file is not None:\n        file_handler = logging.FileHandler(filename=log_file)",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_valid_args",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def get_valid_args(obj, input_args, free_keys=None):\n    if free_keys is None:\n        free_keys = []\n    if inspect.isfunction(obj):\n        expected_keys = inspect.getfullargspec(obj)[0]\n    elif inspect.isclass(obj):\n        expected_keys = inspect.getfullargspec(obj.__init__)[0]\n    else:\n        raise ValueError('Just support function and class object!')\n    unexpect_keys = list()",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "backup_source_code",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def backup_source_code(backup_dir):\n    # \n    ignore_hidden = shutil.ignore_patterns(\n        \".idea\", \".git*\", \"*pycache*\",\n        \"cfgs\", \"data\", \"output\")\n    if os.path.exists(backup_dir):\n        shutil.rmtree(backup_dir)\n    shutil.copytree('.', backup_dir, ignore=ignore_hidden)\n    # os.system(\"chmod -R g+w {}\".format(backup_dir))\ndef log_configs(cfgs, pre='cfgs', logger=None):",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "log_configs",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def log_configs(cfgs, pre='cfgs', logger=None):\n    for key, val in cfgs.items():\n        if isinstance(cfgs[key], EasyDict):\n            logger.info('----------- %s -----------' % key)\n            log_configs(cfgs[key], pre=pre + '.' + key, logger=logger)\n            continue\n        logger.info('%s.%s: %s' % (pre, key, val))\ndef save_checkpoint(model, optimizer, scheduler, scaler, is_dist, epoch, filename='checkpoint'):\n    if is_dist:\n        model_state = model.module.state_dict()",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def save_checkpoint(model, optimizer, scheduler, scaler, is_dist, epoch, filename='checkpoint'):\n    if is_dist:\n        model_state = model.module.state_dict()\n    else:\n        model_state = model.state_dict()\n    optim_state = optimizer.state_dict()\n    scheduler_state = scheduler.state_dict()\n    scaler_state = scaler.state_dict()\n    state = {'epoch': epoch,\n             'model_state': model_state,",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "freeze_bn",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def freeze_bn(module):\n    \"\"\"Freeze the batch normalization layers.\"\"\"\n    for m in module.modules():\n        classname = m.__class__.__name__\n        if classname.find('BatchNorm') != -1:\n            m.eval()\n    return module\n# def convert_state_dict(ori_state_dict, is_dist=True):\n#     new_state_dict = OrderedDict()\n#     if is_dist:",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "load_params_from_file",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def load_params_from_file(model, filename, device, dist_mode, logger, strict=True):\n    checkpoint = torch.load(filename, map_location=device)\n    pretrained_state_dict = checkpoint['model_state']\n    tmp_model = model.module if dist_mode else model\n    state_dict = tmp_model.state_dict()\n    unused_state_dict = {}\n    update_state_dict = {}\n    unupdate_state_dict = {}\n    for key, val in pretrained_state_dict.items():\n        if key in state_dict and state_dict[key].shape == val.shape:",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "color_map_tensorboard",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def color_map_tensorboard(disp_gt, pred, disp_max=192):\n    cm = plt.get_cmap('plasma')\n    disp_gt = disp_gt.detach().data.cpu().numpy()\n    pred = pred.detach().data.cpu().numpy()\n    error_map = np.abs(pred - disp_gt)\n    disp_gt = np.clip(disp_gt, a_min=0, a_max=disp_max)\n    pred = np.clip(pred, a_min=0, a_max=disp_max)\n    gt_tmp = 255.0 * disp_gt / disp_max\n    pred_tmp = 255.0 * pred / disp_max\n    error_map_tmp = 255.0 * error_map / np.max(error_map)",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "write_tensorboard",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def write_tensorboard(tb_writer, tb_info, step):\n    for k, v in tb_info.items():\n        module_name = k.split('/')[0]\n        writer_module = getattr(tb_writer, 'add_' + module_name)\n        board_name = k.replace(module_name + \"/\", '')\n        v = v.detach() if torch.is_tensor(v) else v\n        if module_name == 'image' and v.dim() == 2:\n            writer_module(board_name, v, step, dataformats='HW')\n        else:\n            writer_module(board_name, v, step)",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "draw_depth_2_image",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def draw_depth_2_image(disp, image, baseline=0.54, focallength=1.003556e+3):\n    # baseline \n    disp = disp.detach().data.cpu().numpy()  # [h, w]\n    image = image.detach().data.cpu().numpy()  # [3, h, w]\n    image = np.transpose(image, [1, 2, 0])  # [h, w, 3]\n    image = np.ascontiguousarray(image, dtype=np.uint8)\n    point_size = 2\n    point_colors = [(255, 255, 0), (0, 0, 255), (0, 255, 0), (255, 0, 0), (0, 255, 255)]\n    thickness = -1\n    depths = baseline * focallength / disp",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "get_pos_fullres",
        "kind": 2,
        "importPath": "stereo.utils.common_utils",
        "description": "stereo.utils.common_utils",
        "peekOfCode": "def get_pos_fullres(fx, w, h):\n    x_range = (np.linspace(0, w - 1, w) + 0.5 - w // 2) / fx\n    y_range = (np.linspace(0, h - 1, h) + 0.5 - h // 2) / fx\n    x, y = np.meshgrid(x_range, y_range)\n    z = np.ones_like(x)\n    pos_grid = np.stack([x, y, z], axis=0).astype(np.float32)\n    return pos_grid",
        "detail": "stereo.utils.common_utils",
        "documentation": {}
    },
    {
        "label": "disp_to_color",
        "kind": 2,
        "importPath": "stereo.utils.disp_color",
        "description": "stereo.utils.disp_color",
        "peekOfCode": "def disp_to_color(disp, max_disp=None):\n    \"\"\"\n    Transfer disparity map to color map\n    Args:\n        disp (numpy.array): disparity map in (Height, Width) layout, value range [0, 255]\n        max_disp (int): max disparity, optionally specifies the scaling factor\n    Returns:\n        disparity color map (numpy.array): disparity map in (Height, Width, 3) layout, range [0,255]\n    \"\"\"\n    h, w = disp.shape",
        "detail": "stereo.utils.disp_color",
        "documentation": {}
    },
    {
        "label": "disp_map",
        "kind": 2,
        "importPath": "stereo.utils.disp_color",
        "description": "stereo.utils.disp_color",
        "peekOfCode": "def disp_map(disp):\n    \"\"\"\n    Based on color histogram, convert the gray disp into color disp map.\n    The histogram consists of 7 bins, value of each is e.g. [114.0, 185.0, 114.0, 174.0, 114.0, 185.0, 114.0]\n    Accumulate each bin, named cbins, and scale it to [0,1], e.g. [0.114, 0.299, 0.413, 0.587, 0.701, 0.886, 1.0]\n    For each value in disp, we have to find which bin it belongs to\n    Therefore, we have to compare it with every value in cbins\n    Finally, we have to get the ratio of it accounts for the bin, and then we can interpolate it with the histogram map\n    For example, 0.780 belongs to the 5th bin, the ratio is (0.780-0.701)/0.114,\n    then we can interpolate it into 3 channel with the 5th [0, 1, 0] and 6th [0, 1, 1] channel-map",
        "detail": "stereo.utils.disp_color",
        "documentation": {}
    },
    {
        "label": "Lamb",
        "kind": 6,
        "importPath": "stereo.utils.lamb",
        "description": "stereo.utils.lamb",
        "peekOfCode": "class Lamb(Optimizer):\n    r\"\"\"Implements Lamb algorithm.\n    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve",
        "detail": "stereo.utils.lamb",
        "documentation": {}
    },
    {
        "label": "BaseWarmup",
        "kind": 6,
        "importPath": "stereo.utils.warmup",
        "description": "stereo.utils.warmup",
        "peekOfCode": "class BaseWarmup(object):\n    \"\"\"Base class for all warmup schedules\n    Arguments:\n        optimizer (Optimizer): an instance of a subclass of Optimizer\n        warmup_params (list): warmup paramters\n        last_step (int): The index of last step. (Default: -1)\n    \"\"\"\n    def __init__(self, optimizer, warmup_params, last_step=-1):\n        if not isinstance(optimizer, Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(",
        "detail": "stereo.utils.warmup",
        "documentation": {}
    },
    {
        "label": "LinearWarmup",
        "kind": 6,
        "importPath": "stereo.utils.warmup",
        "description": "stereo.utils.warmup",
        "peekOfCode": "class LinearWarmup(BaseWarmup):\n    \"\"\"Linear warmup schedule.\n    Arguments:\n        optimizer (Optimizer): an instance of a subclass of Optimizer\n        warmup_period (int or list): Warmup period\n        last_step (int): The index of last step. (Default: -1)\n    \"\"\"\n    def __init__(self, optimizer, warmup_period, last_step=-1):\n        group_count = len(optimizer.param_groups)\n        warmup_params = get_warmup_params(warmup_period, group_count)",
        "detail": "stereo.utils.warmup",
        "documentation": {}
    },
    {
        "label": "ExponentialWarmup",
        "kind": 6,
        "importPath": "stereo.utils.warmup",
        "description": "stereo.utils.warmup",
        "peekOfCode": "class ExponentialWarmup(BaseWarmup):\n    \"\"\"Exponential warmup schedule.\n    Arguments:\n        optimizer (Optimizer): an instance of a subclass of Optimizer\n        warmup_period (int or list): Effective warmup period\n        last_step (int): The index of last step. (Default: -1)\n    \"\"\"\n    def __init__(self, optimizer, warmup_period, last_step=-1):\n        group_count = len(optimizer.param_groups)\n        warmup_params = get_warmup_params(warmup_period, group_count)",
        "detail": "stereo.utils.warmup",
        "documentation": {}
    },
    {
        "label": "get_warmup_params",
        "kind": 2,
        "importPath": "stereo.utils.warmup",
        "description": "stereo.utils.warmup",
        "peekOfCode": "def get_warmup_params(warmup_period, group_count):\n    if type(warmup_period) == list:\n        if len(warmup_period) != group_count:\n            raise ValueError('size of warmup_period does not equal {}.'.format(group_count))\n        for x in warmup_period:\n            if type(x) != int:\n                raise ValueError('An element in warmup_period, {}, is not an int.'.format(type(x).__name__))\n        warmup_params = [dict(warmup_period=x) for x in warmup_period]\n    elif type(warmup_period) == int:\n        warmup_params = [dict(warmup_period=warmup_period) for _ in range(group_count)]",
        "detail": "stereo.utils.warmup",
        "documentation": {}
    },
    {
        "label": "read_one_img",
        "kind": 2,
        "importPath": "tools.check_data",
        "description": "tools.check_data",
        "peekOfCode": "def read_one_img(img_path):\n    try:\n        img = cv2.imread(img_path)\n        if img is None:\n            print(\"File open error:\", img_path)\n            return img_path\n    except:\n        print(\"File does not exist:\", img_path)\n        return img_path\ndef read_all_images(input_txt,error_txt):",
        "detail": "tools.check_data",
        "documentation": {}
    },
    {
        "label": "read_all_images",
        "kind": 2,
        "importPath": "tools.check_data",
        "description": "tools.check_data",
        "peekOfCode": "def read_all_images(input_txt,error_txt):\n    with open(input_txt, 'r') as f:\n        img_paths = [path.strip() for path in f.readlines()]\n    print(\"Show first image path: \",img_paths[0])\n    print(\"Start Check: it may need hours according to your datasets\")\n    with ThreadPoolExecutor() as executor:\n        target_dir_list = tqdm.tqdm(executor.map(read_one_img, img_paths), total=len(img_paths))\n    empty_paths = sorted([x for x in target_dir_list if x is not None])\n    with open(error_txt, 'w') as f:\n        for path in empty_paths:",
        "detail": "tools.check_data",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.check_data",
        "description": "tools.check_data",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Check image by cv2.imread\")\n    parser.add_argument('--input_path_file', type=str, help='Path to the txt file containing image paths')\n    parser.add_argument('--error_path_file', type=str, help='Path to save the error image files')\n    args = parser.parse_args()\n    input_txt = args.input_path_file\n    error_txt = args.error_path_file\n    read_all_images(input_txt,error_txt)\nif __name__ == \"__main__\":\n    main()",
        "detail": "tools.check_data",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "tools.eval",
        "description": "tools.eval",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--dist_mode', action='store_true', default=False, help='torchrun ddp multi gpu')\n    parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for eval')\n    parser.add_argument('--eval_data_cfg_file', type=str, default=None)\n    parser.add_argument('--pretrained_model', type=str, default=None)\n    # dataloader\n    parser.add_argument('--workers', type=int, default=0, help='number of workers for dataloader')\n    parser.add_argument('--pin_memory', action='store_true', default=False, help='data loader pin memory')\n    parser.add_argument('--save_root_dir', type=str, default='./output', help='save root dir for this experiment')",
        "detail": "tools.eval",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.eval",
        "description": "tools.eval",
        "peekOfCode": "def main():\n    args, cfgs = parse_config()\n    if args.dist_mode:\n        dist.init_process_group(backend='nccl')\n        local_rank = int(os.environ[\"LOCAL_RANK\"])\n        global_rank = int(os.environ[\"RANK\"])\n    else:\n        local_rank = 0\n        global_rank = 0\n    # env",
        "detail": "tools.eval",
        "documentation": {}
    },
    {
        "label": "ExportWrapper",
        "kind": 6,
        "importPath": "tools.export",
        "description": "tools.export",
        "peekOfCode": "class ExportWrapper(torch.nn.Module):\n    def __init__(self,ori_mode):\n        super().__init__()\n        self.mode = ori_mode\n    def forward(self,left,right):\n        # left = left.permute(2, 0, 1).float() / 255.0\n        # right = right.permute(2, 0, 1).float() / 255.0\n        # # \n        # left[0] = (left[0] - 0.485) / 0.229\n        # left[1] = (left[1] - 0.456) / 0.224",
        "detail": "tools.export",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "tools.export",
        "description": "tools.export",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--cfg_file', type=str, default=\"/home/wanglin/workspace/OpenStereo/cfgs/lightstereo/lightstereo_l_sceneflow_general.yaml\", help='specify the config for eval')\n    parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n    args = parser.parse_args()\n    yaml_config = common_utils.config_loader(args.cfg_file)\n    cfgs = EasyDict(yaml_config)\n    if args.pretrained_model is not None:\n        cfgs.MODEL.PRETRAINED_MODEL = args.pretrained_model\n    args.dist_mode = False",
        "detail": "tools.export",
        "documentation": {}
    },
    {
        "label": "load_stereo_model",
        "kind": 2,
        "importPath": "tools.export",
        "description": "tools.export",
        "peekOfCode": "def load_stereo_model():\n    \"\"\"\n     MSNet3D \n    :param weights_path: \n    :return: \n    \"\"\"\n    args, cfgs = parse_config()\n    if args.dist_mode:\n        dist.init_process_group(backend='nccl')\n        local_rank = int(os.environ[\"LOCAL_RANK\"])",
        "detail": "tools.export",
        "documentation": {}
    },
    {
        "label": "export_to_onnx",
        "kind": 2,
        "importPath": "tools.export",
        "description": "tools.export",
        "peekOfCode": "def export_to_onnx(model, data,input_shape=input_shape):\n    \"\"\"\n     RAFT-Stereo  ONNX \n    :param model: RAFT-Stereo \n    :param onnx_path:  ONNX \n    :param input_shape:  (batch_size, channels, height, width)\n    \"\"\"\n    # \n    model.eval()    \n    # model = torch.jit.script(model)",
        "detail": "tools.export",
        "documentation": {}
    },
    {
        "label": "validate_onnx_model",
        "kind": 2,
        "importPath": "tools.export",
        "description": "tools.export",
        "peekOfCode": "def validate_onnx_model(data,disp_pred):\n    \"\"\"\n     ONNX Runtime  ONNX \n    :param onnx_path: ONNX \n    \"\"\"\n    #  ONNX \n    onnx_model = onnx.load(onnx_model_path)\n    onnx.checker.check_model(onnx_model)\n    # \n    dummy_left = data['left'].cpu().numpy().astype(np.float32)",
        "detail": "tools.export",
        "documentation": {}
    },
    {
        "label": "original_input_size",
        "kind": 5,
        "importPath": "tools.export",
        "description": "tools.export",
        "peekOfCode": "original_input_size = (0,0)\ndevice = 'cuda'#'cuda' if torch.cuda.is_available() else 'cpu'\nonnx_model_path = './output/model_480x640.onnx'\n# onnx_model_path = '/home/wanglin/workspace/OpenStereo/premodels/model_256x512.onnx'\n# onnx_model_path = '/home/lc/gaoshan/Workspace/OpenStereo/output/model_640x1280.onnx'\n# input_shape=(1, 3, 640, 1280)\n# input_size = (1280,640)\ninput_shape=(1, 3, 256, 512)\ninput_size = (512,256)\n# input_shape=(1, 3, 256, 512)",
        "detail": "tools.export",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "tools.export",
        "description": "tools.export",
        "peekOfCode": "device = 'cuda'#'cuda' if torch.cuda.is_available() else 'cpu'\nonnx_model_path = './output/model_480x640.onnx'\n# onnx_model_path = '/home/wanglin/workspace/OpenStereo/premodels/model_256x512.onnx'\n# onnx_model_path = '/home/lc/gaoshan/Workspace/OpenStereo/output/model_640x1280.onnx'\n# input_shape=(1, 3, 640, 1280)\n# input_size = (1280,640)\ninput_shape=(1, 3, 256, 512)\ninput_size = (512,256)\n# input_shape=(1, 3, 256, 512)\n# input_size = (512,256)",
        "detail": "tools.export",
        "documentation": {}
    },
    {
        "label": "onnx_model_path",
        "kind": 5,
        "importPath": "tools.export",
        "description": "tools.export",
        "peekOfCode": "onnx_model_path = './output/model_480x640.onnx'\n# onnx_model_path = '/home/wanglin/workspace/OpenStereo/premodels/model_256x512.onnx'\n# onnx_model_path = '/home/lc/gaoshan/Workspace/OpenStereo/output/model_640x1280.onnx'\n# input_shape=(1, 3, 640, 1280)\n# input_size = (1280,640)\ninput_shape=(1, 3, 256, 512)\ninput_size = (512,256)\n# input_shape=(1, 3, 256, 512)\n# input_size = (512,256)\nclass ExportWrapper(torch.nn.Module):",
        "detail": "tools.export",
        "documentation": {}
    },
    {
        "label": "input_size",
        "kind": 5,
        "importPath": "tools.export",
        "description": "tools.export",
        "peekOfCode": "input_size = (512,256)\n# input_shape=(1, 3, 256, 512)\n# input_size = (512,256)\nclass ExportWrapper(torch.nn.Module):\n    def __init__(self,ori_mode):\n        super().__init__()\n        self.mode = ori_mode\n    def forward(self,left,right):\n        # left = left.permute(2, 0, 1).float() / 255.0\n        # right = right.permute(2, 0, 1).float() / 255.0",
        "detail": "tools.export",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "tools.infer",
        "description": "tools.infer",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--dist_mode', action='store_true', default=False, help='torchrun ddp multi gpu')\n    parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for eval')\n    # data\n    parser.add_argument('--left_img_path', type=str, default=None)\n    parser.add_argument('--right_img_path', type=str, default=None)\n    parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n    parser.add_argument('--savename', type=str, default=None)\n    args = parser.parse_args()",
        "detail": "tools.infer",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.infer",
        "description": "tools.infer",
        "peekOfCode": "def main():\n    args, cfgs = parse_config()\n    if args.dist_mode:\n        dist.init_process_group(backend='nccl')\n        local_rank = int(os.environ[\"LOCAL_RANK\"])\n        global_rank = int(os.environ[\"RANK\"])\n    else:\n        local_rank = 0\n        global_rank = 0\n    # env",
        "detail": "tools.infer",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "tools.measure",
        "description": "tools.measure",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--dist_mode', action='store_true', default=False, help='torchrun ddp multi gpu')\n    parser.add_argument('--cfg_file', type=str, default=None, help='specify the config for training')\n    args = parser.parse_args()\n    yaml_config = common_utils.config_loader(args.cfg_file)\n    cfgs = EasyDict(yaml_config)\n    args.run_mode = 'measure'\n    return args, cfgs\ndef main():",
        "detail": "tools.measure",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.measure",
        "description": "tools.measure",
        "peekOfCode": "def main():\n    args, cfgs = parse_config()\n    model = build_trainer(args, cfgs, local_rank=0, global_rank=0, logger=None, tb_writer=None).model\n    shape = [1, 3, 544, 960]\n    infer_time(model, shape)\n    measure(model, shape)\n@torch.no_grad()\ndef measure(model, shape):\n    model.eval()\n    inputs = {'left': torch.randn(shape).cuda(),",
        "detail": "tools.measure",
        "documentation": {}
    },
    {
        "label": "measure",
        "kind": 2,
        "importPath": "tools.measure",
        "description": "tools.measure",
        "peekOfCode": "def measure(model, shape):\n    model.eval()\n    inputs = {'left': torch.randn(shape).cuda(),\n              'right': torch.randn(shape).cuda()}\n    flops, params = thop.profile(model, inputs=(inputs,))\n    print(\"Number of calculates:%.2fGFlops\" % (flops / 1e9))\n    print(\"Number of parameters:%.2fM\" % (params / 1e6))\n@torch.no_grad()\ndef infer_time(model, shape):\n    model.eval()",
        "detail": "tools.measure",
        "documentation": {}
    },
    {
        "label": "infer_time",
        "kind": 2,
        "importPath": "tools.measure",
        "description": "tools.measure",
        "peekOfCode": "def infer_time(model, shape):\n    model.eval()\n    repetitions = 100\n    inputs = {'left': torch.randn(shape).cuda(),\n              'right': torch.randn(shape).cuda()}\n    # , GPU , \n    print('warm up ...\\n')\n    with torch.no_grad():\n        for _ in range(10):\n            _ = model(inputs)",
        "detail": "tools.measure",
        "documentation": {}
    },
    {
        "label": "arr",
        "kind": 5,
        "importPath": "tools.npy_show",
        "description": "tools.npy_show",
        "peekOfCode": "arr = np.load(\"/home/extra/share/mgz3/compare_light_out/layerdump/disp_pred_quanted.npy\")\n# arr = np.load(\"/home/wanglin/workspace/backup/OpenStereo/board.npy\")\narr2 = np.fromfile(\"/home/wanglin/disp.bin\", dtype=np.float32).astype(np.float32)\nprint(arr2)\nprint(arr2.shape)\narr = arr.squeeze()\nprint(arr.shape)\nprint(arr)\n# arr = arr.transpose(1,2,0)\ndisp_vis = cv2.normalize(arr, None, 0, 255, cv2.NORM_MINMAX)",
        "detail": "tools.npy_show",
        "documentation": {}
    },
    {
        "label": "arr2",
        "kind": 5,
        "importPath": "tools.npy_show",
        "description": "tools.npy_show",
        "peekOfCode": "arr2 = np.fromfile(\"/home/wanglin/disp.bin\", dtype=np.float32).astype(np.float32)\nprint(arr2)\nprint(arr2.shape)\narr = arr.squeeze()\nprint(arr.shape)\nprint(arr)\n# arr = arr.transpose(1,2,0)\ndisp_vis = cv2.normalize(arr, None, 0, 255, cv2.NORM_MINMAX)\ndisp_vis = disp_vis.astype(np.uint8)\n# ",
        "detail": "tools.npy_show",
        "documentation": {}
    },
    {
        "label": "arr",
        "kind": 5,
        "importPath": "tools.npy_show",
        "description": "tools.npy_show",
        "peekOfCode": "arr = arr.squeeze()\nprint(arr.shape)\nprint(arr)\n# arr = arr.transpose(1,2,0)\ndisp_vis = cv2.normalize(arr, None, 0, 255, cv2.NORM_MINMAX)\ndisp_vis = disp_vis.astype(np.uint8)\n# \ndisp_color = cv2.applyColorMap(disp_vis, cv2.COLORMAP_JET)\ncv2.imshow(\"Disparity Map\", disp_vis)\ncv2.waitKey(0)",
        "detail": "tools.npy_show",
        "documentation": {}
    },
    {
        "label": "disp_vis",
        "kind": 5,
        "importPath": "tools.npy_show",
        "description": "tools.npy_show",
        "peekOfCode": "disp_vis = cv2.normalize(arr, None, 0, 255, cv2.NORM_MINMAX)\ndisp_vis = disp_vis.astype(np.uint8)\n# \ndisp_color = cv2.applyColorMap(disp_vis, cv2.COLORMAP_JET)\ncv2.imshow(\"Disparity Map\", disp_vis)\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
        "detail": "tools.npy_show",
        "documentation": {}
    },
    {
        "label": "disp_vis",
        "kind": 5,
        "importPath": "tools.npy_show",
        "description": "tools.npy_show",
        "peekOfCode": "disp_vis = disp_vis.astype(np.uint8)\n# \ndisp_color = cv2.applyColorMap(disp_vis, cv2.COLORMAP_JET)\ncv2.imshow(\"Disparity Map\", disp_vis)\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
        "detail": "tools.npy_show",
        "documentation": {}
    },
    {
        "label": "disp_color",
        "kind": 5,
        "importPath": "tools.npy_show",
        "description": "tools.npy_show",
        "peekOfCode": "disp_color = cv2.applyColorMap(disp_vis, cv2.COLORMAP_JET)\ncv2.imshow(\"Disparity Map\", disp_vis)\ncv2.waitKey(0)\ncv2.destroyAllWindows()",
        "detail": "tools.npy_show",
        "documentation": {}
    },
    {
        "label": "resize_image",
        "kind": 2,
        "importPath": "tools.resize",
        "description": "tools.resize",
        "peekOfCode": "def resize_image(input_path, input_path_root,output_path_root,target_size=(768, 384)):\n    try:\n        img = cv2.imread(input_path)\n        if img is None:\n            print(\"File open error:\", input_path)\n            return\n    except:\n        print(\"File does not exist:\", input_path)\n        return\n    output_path = input_path.replace(input_path_root, output_path_root)",
        "detail": "tools.resize",
        "documentation": {}
    },
    {
        "label": "resize_all_images",
        "kind": 2,
        "importPath": "tools.resize",
        "description": "tools.resize",
        "peekOfCode": "def resize_all_images(all_img_txt,input_path_root,output_path_root):\n    # Use ThreadPoolExecutor to process files in parallel \n    with open(all_img_txt, 'r') as f:\n        input_files = sorted([file.strip() for file in f.readlines()])\n    with ThreadPoolExecutor() as executor:\n        with tqdm.tqdm(total=len(input_files), desc=\"Resizing images\", unit=\"img\") as pbar:\n            for _ in executor.map(lambda img_path: resize_image(img_path, input_path_root,output_path_root),input_files):\n                pbar.update(1)\ndef main():\n    parser = argparse.ArgumentParser(description=\"Resize all image paths from a txt file.\")",
        "detail": "tools.resize",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.resize",
        "description": "tools.resize",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser(description=\"Resize all image paths from a txt file.\")\n    parser.add_argument('--input_path_file', type=str, help='Path to the txt file containing image paths')\n    parser.add_argument('--input_path_root', type=str, help='Root directory containing input images')\n    parser.add_argument('--output_path_root', type=str, help='Root directory to save output paths')\n    args = parser.parse_args()\n    all_img_txt = args.input_path_file\n    input_path_root = args.input_path_root\n    output_path_root = args.output_path_root\n    resize_all_images(all_img_txt,input_path_root,output_path_root)",
        "detail": "tools.resize",
        "documentation": {}
    },
    {
        "label": "KittiTestDataset",
        "kind": 6,
        "importPath": "tools.test_kitti",
        "description": "tools.test_kitti",
        "peekOfCode": "class KittiTestDataset(DatasetTemplate):\n    def __init__(self, data_info, data_cfg, mode='testing'):\n        super().__init__(data_info, data_cfg, mode)\n    def __getitem__(self, idx):\n        item = self.data_list[idx]\n        full_paths = [os.path.join(self.root, x) for x in item]\n        left_img_path, right_img_path = full_paths[:2]\n        left_img = np.array(Image.open(left_img_path).convert('RGB'), dtype=np.float32)\n        right_img = np.array(Image.open(right_img_path).convert('RGB'), dtype=np.float32)\n        sample = {",
        "detail": "tools.test_kitti",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "tools.test_kitti",
        "description": "tools.test_kitti",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    parser.add_argument('--workers', type=int, default=0, help='number of workers for dataloader')\n    parser.add_argument('--pin_memory', action='store_true', default=False, help='data loader pin memory')\n    parser.add_argument('--pretrained_model', type=str, default=None, help='pretrained_model')\n    parser.add_argument('--data_cfg_file', type=str, default='cfgs/kitti_eval_test.yaml')\n    args = parser.parse_args()\n    args.output_dir = str(Path(args.pretrained_model).parent.parent)\n    args.kitti_result_dir = os.path.join(args.output_dir, 'disp_0')\n    if not os.path.exists(args.kitti_result_dir):",
        "detail": "tools.test_kitti",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.test_kitti",
        "description": "tools.test_kitti",
        "peekOfCode": "def main():\n    args, cfgs = parse_config()\n    local_rank = 0\n    global_rank = 0\n    torch.cuda.set_device(local_rank)\n    # logger\n    log_file = os.path.join(args.output_dir, 'testkitti_%s.log' % datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))\n    logger = common_utils.create_logger(log_file, rank=local_rank)\n    # log args and cfgs\n    for key, val in vars(args).items():",
        "detail": "tools.test_kitti",
        "documentation": {}
    },
    {
        "label": "parse_config",
        "kind": 2,
        "importPath": "tools.train",
        "description": "tools.train",
        "peekOfCode": "def parse_config():\n    parser = argparse.ArgumentParser(description='arg parser')\n    # mode\n    parser.add_argument('--dist_mode', action='store_true', default=False, help='torchrun ddp multi gpu')\n    parser.add_argument('--cfg_file', type=str, default=None, required=True, help='specify the config for training')\n    parser.add_argument('--fix_random_seed', action='store_true', default=False, help='')\n    # save path\n    parser.add_argument('--save_root_dir', type=str, default='./output', help='save root dir for this experiment')\n    parser.add_argument('--extra_tag', type=str, default='default', help='extra tag for this experiment')\n    # dataloader",
        "detail": "tools.train",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "tools.train",
        "description": "tools.train",
        "peekOfCode": "def main():\n    args, cfgs = parse_config()\n    if args.dist_mode:\n        dist.init_process_group(backend='nccl')\n        local_rank = int(os.environ[\"LOCAL_RANK\"])\n        global_rank = int(os.environ[\"RANK\"])\n        group_rank = int(os.environ[\"GROUP_RANK\"])\n    else:\n        local_rank = 0\n        global_rank = 0",
        "detail": "tools.train",
        "documentation": {}
    },
    {
        "label": "mb",
        "kind": 2,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "def mb(x): return x / 1024.0 / 1024.0\ndef safe_load_engine(path):\n    \"\"\" engine  engine/context,  (MiB).\n        (None, error_message, mem_before, mem_after)\n    \"\"\"\n    if not os.path.exists(path):\n        return None, f\"Engine file not found: {path}\", None, None\n    size = os.path.getsize(path)\n    if size == 0:\n        return None, f\"Engine file empty: {path}\", None, None",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "safe_load_engine",
        "kind": 2,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "def safe_load_engine(path):\n    \"\"\" engine  engine/context,  (MiB).\n        (None, error_message, mem_before, mem_after)\n    \"\"\"\n    if not os.path.exists(path):\n        return None, f\"Engine file not found: {path}\", None, None\n    size = os.path.getsize(path)\n    if size == 0:\n        return None, f\"Engine file empty: {path}\", None, None\n    # free, total",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "prepare_buffers_and_bind",
        "kind": 2,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "def prepare_buffers_and_bind(context, engine, input_shapes):\n    \"\"\" engine/context  host/device TRT10 API\"\"\"\n    # \n    for name, shape in input_shapes.items():\n        if engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n            context.set_input_shape(name, shape)\n    # \n    buffers = {}\n    for name in engine:\n        mode = engine.get_tensor_mode(name)",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "fill_inputs_random",
        "kind": 2,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "def fill_inputs_random(buffers):\n    for name, buf in buffers.items():\n        if buf[\"is_input\"]:\n            arr = np.random.randn(*buf[\"shape\"]).astype(np.float32)\n            #  engine  dtype  float32 cast (we compare outputs in float32 later)\n            arr = arr.astype(np.dtype(buf[\"dtype\"]))\n            np.copyto(buf[\"host\"], arr.ravel())\n    return\ndef run_engine(engine, input_shapes, n_runs=N_RUNS):\n    \"\"\" avg_latency_ms, outputs_dict (name -> numpy array), mem_used_mib\"\"\"",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "run_engine",
        "kind": 2,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "def run_engine(engine, input_shapes, n_runs=N_RUNS):\n    \"\"\" avg_latency_ms, outputs_dict (name -> numpy array), mem_used_mib\"\"\"\n    context = engine.create_execution_context()\n    # prepare buffers\n    buffers = prepare_buffers_and_bind(context, engine, input_shapes)\n    # fill inputs\n    fill_inputs_random(buffers)\n    stream = cuda.Stream()\n    latencies = []\n    # warmup few runs (not counted)",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "compute_error_metrics",
        "kind": 2,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "def compute_error_metrics(ref, other):\n    \"\"\"ref, other : numpy arrays (float32), same shape\n        dict: L1_mean, RMSE, PSNR (dB)\n    \"\"\"\n    # ensure shape equal\n    if ref.shape != other.shape:\n        return {\"error\": \"shape_mismatch\", \"ref_shape\": ref.shape, \"other_shape\": other.shape}\n    diff = ref - other\n    abs_diff = np.abs(diff)\n    l1 = float(np.mean(abs_diff))",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "def main():\n    results = {}\n    reference_outputs = None\n    reference_name = None\n    print(f\"[INFO] TensorRT version: {trt.__version__}\")\n    for idx, path in enumerate(engine_paths):\n        base = os.path.basename(path)\n        print(f\"\\n[INFO] Processing engine ({idx+1}/{len(engine_paths)}): {base}\")\n        engine, err, mem_used_mib, gpu_total_mib = safe_load_engine(path)\n        if engine is None:",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "TRT_LOGGER",
        "kind": 5,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n# ----------  ----------\nengine_paths = [\n    \"/home/wanglin/workspace/logicAI/onnx/light_fp32.trt\",\n    \"/home/wanglin/workspace/logicAI/onnx/light_fp16.trt\",\n    \"/home/wanglin/workspace/logicAI/onnx/light_int8.trt\",\n]\n#  engine \ninput_shapes = {\n    \"left_img\": (1, 3, 256, 512),",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "engine_paths",
        "kind": 5,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "engine_paths = [\n    \"/home/wanglin/workspace/logicAI/onnx/light_fp32.trt\",\n    \"/home/wanglin/workspace/logicAI/onnx/light_fp16.trt\",\n    \"/home/wanglin/workspace/logicAI/onnx/light_int8.trt\",\n]\n#  engine \ninput_shapes = {\n    \"left_img\": (1, 3, 256, 512),\n    \"right_img\": (1, 3, 256, 512)\n}",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "input_shapes",
        "kind": 5,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "input_shapes = {\n    \"left_img\": (1, 3, 256, 512),\n    \"right_img\": (1, 3, 256, 512)\n}\nN_RUNS = 100\n# \n# ---------- end config ----------\ndef mb(x): return x / 1024.0 / 1024.0\ndef safe_load_engine(path):\n    \"\"\" engine  engine/context,  (MiB).",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "N_RUNS",
        "kind": 5,
        "importPath": "trt.compare_trt10_engines",
        "description": "trt.compare_trt10_engines",
        "peekOfCode": "N_RUNS = 100\n# \n# ---------- end config ----------\ndef mb(x): return x / 1024.0 / 1024.0\ndef safe_load_engine(path):\n    \"\"\" engine  engine/context,  (MiB).\n        (None, error_message, mem_before, mem_after)\n    \"\"\"\n    if not os.path.exists(path):\n        return None, f\"Engine file not found: {path}\", None, None",
        "detail": "trt.compare_trt10_engines",
        "documentation": {}
    },
    {
        "label": "preprocess_image",
        "kind": 2,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "def preprocess_image(img_path, shape):\n    img = cv2.imread(img_path, cv2.IMREAD_COLOR)  # HWC, BGR\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (shape[3], shape[2]))  # WH -> shape (N,C,H,W)\n    img = img.astype(np.float32) / 255.0\n    img = np.transpose(img, (2, 0, 1))  # C,H,W\n    img = np.expand_dims(img, axis=0)  # N,C,H,W\n    return img\nbindings[\"left_img\"][\"host\"][:] = preprocess_image(left_img_path, input_shapes[\"left_img\"]).ravel()\nbindings[\"right_img\"][\"host\"][:] = preprocess_image(right_img_path, input_shapes[\"right_img\"]).ravel()",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "engine_path",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "engine_path = \"/home/wanglin/workspace/logicAI/onnx/light_int8.trt\"\nleft_img_path = \"/home/wanglin/workspace/backup/OpenStereo/mgz/datas/left.png\"\nright_img_path = \"/home/wanglin/workspace/backup/OpenStereo/mgz/datas/right.png\"\noutput_disp_path = \"trt/disp_pred.npy\"\ninput_shapes = {\"left_img\": (1, 3, 256, 512),\n                \"right_img\": (1, 3, 256, 512)}\nN_WARMUP = 5\nN_RUNS = 100\n# ------------------------\n# 1.  TensorRT Runtime",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "left_img_path",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "left_img_path = \"/home/wanglin/workspace/backup/OpenStereo/mgz/datas/left.png\"\nright_img_path = \"/home/wanglin/workspace/backup/OpenStereo/mgz/datas/right.png\"\noutput_disp_path = \"trt/disp_pred.npy\"\ninput_shapes = {\"left_img\": (1, 3, 256, 512),\n                \"right_img\": (1, 3, 256, 512)}\nN_WARMUP = 5\nN_RUNS = 100\n# ------------------------\n# 1.  TensorRT Runtime\n# ------------------------",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "right_img_path",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "right_img_path = \"/home/wanglin/workspace/backup/OpenStereo/mgz/datas/right.png\"\noutput_disp_path = \"trt/disp_pred.npy\"\ninput_shapes = {\"left_img\": (1, 3, 256, 512),\n                \"right_img\": (1, 3, 256, 512)}\nN_WARMUP = 5\nN_RUNS = 100\n# ------------------------\n# 1.  TensorRT Runtime\n# ------------------------\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "output_disp_path",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "output_disp_path = \"trt/disp_pred.npy\"\ninput_shapes = {\"left_img\": (1, 3, 256, 512),\n                \"right_img\": (1, 3, 256, 512)}\nN_WARMUP = 5\nN_RUNS = 100\n# ------------------------\n# 1.  TensorRT Runtime\n# ------------------------\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\nassert os.path.exists(engine_path), f\"Engine not found: {engine_path}\"",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "input_shapes",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "input_shapes = {\"left_img\": (1, 3, 256, 512),\n                \"right_img\": (1, 3, 256, 512)}\nN_WARMUP = 5\nN_RUNS = 100\n# ------------------------\n# 1.  TensorRT Runtime\n# ------------------------\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\nassert os.path.exists(engine_path), f\"Engine not found: {engine_path}\"\nprint(f\"[INFO] TensorRT version: {trt.__version__}\")",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "N_WARMUP",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "N_WARMUP = 5\nN_RUNS = 100\n# ------------------------\n# 1.  TensorRT Runtime\n# ------------------------\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\nassert os.path.exists(engine_path), f\"Engine not found: {engine_path}\"\nprint(f\"[INFO] TensorRT version: {trt.__version__}\")\nprint(f\"[INFO] Loading TensorRT engine: {engine_path}\")\nwith open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "N_RUNS",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "N_RUNS = 100\n# ------------------------\n# 1.  TensorRT Runtime\n# ------------------------\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\nassert os.path.exists(engine_path), f\"Engine not found: {engine_path}\"\nprint(f\"[INFO] TensorRT version: {trt.__version__}\")\nprint(f\"[INFO] Loading TensorRT engine: {engine_path}\")\nwith open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n    engine = runtime.deserialize_cuda_engine(f.read())",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "TRT_LOGGER",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\nassert os.path.exists(engine_path), f\"Engine not found: {engine_path}\"\nprint(f\"[INFO] TensorRT version: {trt.__version__}\")\nprint(f\"[INFO] Loading TensorRT engine: {engine_path}\")\nwith open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n    engine = runtime.deserialize_cuda_engine(f.read())\ncontext = engine.create_execution_context()\nstream = cuda.Stream()\n# ------------------------\n# 2. ",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "context",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "context = engine.create_execution_context()\nstream = cuda.Stream()\n# ------------------------\n# 2. \n# ------------------------\nfor name, shape in input_shapes.items():\n    context.set_input_shape(name, shape)\n# ------------------------\n# 3. \n# ------------------------",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "stream",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "stream = cuda.Stream()\n# ------------------------\n# 2. \n# ------------------------\nfor name, shape in input_shapes.items():\n    context.set_input_shape(name, shape)\n# ------------------------\n# 3. \n# ------------------------\nbindings = {}",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "bindings",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "bindings = {}\nfor name in engine:\n    dtype = trt.nptype(engine.get_tensor_dtype(name))\n    shape = context.get_tensor_shape(name)\n    size = int(trt.volume(shape))\n    host_mem = cuda.pagelocked_empty(size, dtype)\n    device_mem = cuda.mem_alloc(host_mem.nbytes)\n    bindings[name] = {\n        \"host\": host_mem,\n        \"device\": device_mem,",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "bindings[\"left_img\"][\"host\"][:]",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "bindings[\"left_img\"][\"host\"][:] = preprocess_image(left_img_path, input_shapes[\"left_img\"]).ravel()\nbindings[\"right_img\"][\"host\"][:] = preprocess_image(right_img_path, input_shapes[\"right_img\"]).ravel()\n# ------------------------\n# 5.  tensor \n# ------------------------\nfor name, buf in bindings.items():\n    context.set_tensor_address(name, int(buf[\"device\"]))\n# ------------------------\n# 6.  CUDA Event \n# ------------------------",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "bindings[\"right_img\"][\"host\"][:]",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "bindings[\"right_img\"][\"host\"][:] = preprocess_image(right_img_path, input_shapes[\"right_img\"]).ravel()\n# ------------------------\n# 5.  tensor \n# ------------------------\nfor name, buf in bindings.items():\n    context.set_tensor_address(name, int(buf[\"device\"]))\n# ------------------------\n# 6.  CUDA Event \n# ------------------------\n#  GPU",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "latencies",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "latencies = []\nstart_event = cuda.Event()\nend_event = cuda.Event()\nfor _ in range(N_RUNS):\n    start_event.record(stream)\n    context.execute_async_v3(stream_handle=stream.handle)\n    end_event.record(stream)\n    end_event.synchronize()\n    latencies.append(start_event.time_till(end_event))  # ms\navg_latency = np.mean(latencies)",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "start_event",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "start_event = cuda.Event()\nend_event = cuda.Event()\nfor _ in range(N_RUNS):\n    start_event.record(stream)\n    context.execute_async_v3(stream_handle=stream.handle)\n    end_event.record(stream)\n    end_event.synchronize()\n    latencies.append(start_event.time_till(end_event))  # ms\navg_latency = np.mean(latencies)\nfps = 1000.0 / avg_latency if avg_latency > 0 else float(\"inf\")",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "end_event",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "end_event = cuda.Event()\nfor _ in range(N_RUNS):\n    start_event.record(stream)\n    context.execute_async_v3(stream_handle=stream.handle)\n    end_event.record(stream)\n    end_event.synchronize()\n    latencies.append(start_event.time_till(end_event))  # ms\navg_latency = np.mean(latencies)\nfps = 1000.0 / avg_latency if avg_latency > 0 else float(\"inf\")\nprint(f\"[INFO] Inference latency: {avg_latency:.3f} ms | FPS: {fps:.2f}\")",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "avg_latency",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "avg_latency = np.mean(latencies)\nfps = 1000.0 / avg_latency if avg_latency > 0 else float(\"inf\")\nprint(f\"[INFO] Inference latency: {avg_latency:.3f} ms | FPS: {fps:.2f}\")\n# ------------------------\n# 7.  npy\n# ------------------------\nfor name, buf in bindings.items():\n    if not buf[\"is_input\"]:\n        cuda.memcpy_dtoh_async(buf[\"host\"], buf[\"device\"], stream)\n        stream.synchronize()",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "fps",
        "kind": 5,
        "importPath": "trt.example",
        "description": "trt.example",
        "peekOfCode": "fps = 1000.0 / avg_latency if avg_latency > 0 else float(\"inf\")\nprint(f\"[INFO] Inference latency: {avg_latency:.3f} ms | FPS: {fps:.2f}\")\n# ------------------------\n# 7.  npy\n# ------------------------\nfor name, buf in bindings.items():\n    if not buf[\"is_input\"]:\n        cuda.memcpy_dtoh_async(buf[\"host\"], buf[\"device\"], stream)\n        stream.synchronize()\n        out_arr = np.frombuffer(buf[\"host\"], dtype=buf[\"dtype\"]).reshape(buf[\"shape\"]).astype(np.float32)",
        "detail": "trt.example",
        "documentation": {}
    },
    {
        "label": "TRT_LOGGER",
        "kind": 5,
        "importPath": "trt.infer",
        "description": "trt.infer",
        "peekOfCode": "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)\nengine_path = \"/home/wanglin/workspace/logicAI/onnx/light_fp32.trt\"\nassert os.path.exists(engine_path), f\"Engine not found: {engine_path}\"\nprint(f\"[INFO] TensorRT version: {trt.__version__}\")\nprint(f\"[INFO] Loading TensorRT engine: {engine_path}\")\nwith open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n    engine = runtime.deserialize_cuda_engine(f.read())\ncontext = engine.create_execution_context()\nstream = cuda.Stream()\n# ------------------------",
        "detail": "trt.infer",
        "documentation": {}
    },
    {
        "label": "engine_path",
        "kind": 5,
        "importPath": "trt.infer",
        "description": "trt.infer",
        "peekOfCode": "engine_path = \"/home/wanglin/workspace/logicAI/onnx/light_fp32.trt\"\nassert os.path.exists(engine_path), f\"Engine not found: {engine_path}\"\nprint(f\"[INFO] TensorRT version: {trt.__version__}\")\nprint(f\"[INFO] Loading TensorRT engine: {engine_path}\")\nwith open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n    engine = runtime.deserialize_cuda_engine(f.read())\ncontext = engine.create_execution_context()\nstream = cuda.Stream()\n# ------------------------\n# 2.  trtexec --shapes",
        "detail": "trt.infer",
        "documentation": {}
    },
    {
        "label": "context",
        "kind": 5,
        "importPath": "trt.infer",
        "description": "trt.infer",
        "peekOfCode": "context = engine.create_execution_context()\nstream = cuda.Stream()\n# ------------------------\n# 2.  trtexec --shapes\n# ------------------------\n# TensorRT 10.x:  set_input_shape()\ncontext.set_input_shape(\"left_img\", (1, 3, 256, 512))\ncontext.set_input_shape(\"right_img\", (1, 3, 256, 512))\n#  shape \nprint(\"\\n[INFO] Input/Output bindings:\")",
        "detail": "trt.infer",
        "documentation": {}
    },
    {
        "label": "stream",
        "kind": 5,
        "importPath": "trt.infer",
        "description": "trt.infer",
        "peekOfCode": "stream = cuda.Stream()\n# ------------------------\n# 2.  trtexec --shapes\n# ------------------------\n# TensorRT 10.x:  set_input_shape()\ncontext.set_input_shape(\"left_img\", (1, 3, 256, 512))\ncontext.set_input_shape(\"right_img\", (1, 3, 256, 512))\n#  shape \nprint(\"\\n[INFO] Input/Output bindings:\")\nfor name in engine:",
        "detail": "trt.infer",
        "documentation": {}
    },
    {
        "label": "bindings",
        "kind": 5,
        "importPath": "trt.infer",
        "description": "trt.infer",
        "peekOfCode": "bindings = {}\nfor name in engine:\n    dtype = trt.nptype(engine.get_tensor_dtype(name))\n    shape = context.get_tensor_shape(name)\n    size = int(trt.volume(shape))\n    host_mem = cuda.pagelocked_empty(size, dtype)\n    device_mem = cuda.mem_alloc(host_mem.nbytes)\n    bindings[name] = {\n        \"host\": host_mem,\n        \"device\": device_mem,",
        "detail": "trt.infer",
        "documentation": {}
    },
    {
        "label": "n_iterations",
        "kind": 5,
        "importPath": "trt.infer",
        "description": "trt.infer",
        "peekOfCode": "n_iterations = 100\nlatencies = []\nprint(\"\\n[INFO] Running inference ...\")\nfor i in range(n_iterations):\n    start = time.time()\n    #  GPU\n    for name, buf in bindings.items():\n        if buf[\"is_input\"]:\n            cuda.memcpy_htod_async(buf[\"device\"], buf[\"host\"], stream)\n    #  (TensorRT 10  execute_async_v3)",
        "detail": "trt.infer",
        "documentation": {}
    },
    {
        "label": "latencies",
        "kind": 5,
        "importPath": "trt.infer",
        "description": "trt.infer",
        "peekOfCode": "latencies = []\nprint(\"\\n[INFO] Running inference ...\")\nfor i in range(n_iterations):\n    start = time.time()\n    #  GPU\n    for name, buf in bindings.items():\n        if buf[\"is_input\"]:\n            cuda.memcpy_htod_async(buf[\"device\"], buf[\"host\"], stream)\n    #  (TensorRT 10  execute_async_v3)\n    context.execute_async_v3(stream_handle=stream.handle)",
        "detail": "trt.infer",
        "documentation": {}
    },
    {
        "label": "avg_latency",
        "kind": 5,
        "importPath": "trt.infer",
        "description": "trt.infer",
        "peekOfCode": "avg_latency = np.mean(latencies)\nprint(f\"[INFO] Average latency over {n_iterations} runs: {avg_latency:.3f} ms\")\n# ------------------------\n# 7. \n# ------------------------\nfor name, buf in bindings.items():\n    if not buf[\"is_input\"]:\n        out_file = f\"{name}_output.bin\"\n        buf[\"host\"].tofile(out_file)\n        print(f\"[INFO] Output saved to {out_file}\")",
        "detail": "trt.infer",
        "documentation": {}
    },
    {
        "label": "profile_data",
        "kind": 5,
        "importPath": "trt.infer",
        "description": "trt.infer",
        "peekOfCode": "profile_data = {\n    \"engine\": engine_path,\n    \"avg_latency_ms\": float(avg_latency),\n    \"bindings\": {\n        name: {\n            \"shape\": tuple(map(int, buf[\"shape\"])),\n            \"dtype\": str(buf[\"dtype\"]),\n            \"is_input\": bool(buf[\"is_input\"]),\n        }\n        for name, buf in bindings.items()",
        "detail": "trt.infer",
        "documentation": {}
    },
    {
        "label": "get_onnx_layers",
        "kind": 2,
        "importPath": "trt.layer_diff",
        "description": "trt.layer_diff",
        "peekOfCode": "def get_onnx_layers(onnx_path):\n    model = onnx.load(onnx_path)\n    layers = []\n    for node in model.graph.node:\n        layers.append({\n            \"name\": node.name if node.name else node.output[0],\n            \"type\": node.op_type\n        })\n    return layers\ndef get_trt_layers(trt_json_path):",
        "detail": "trt.layer_diff",
        "documentation": {}
    },
    {
        "label": "get_trt_layers",
        "kind": 2,
        "importPath": "trt.layer_diff",
        "description": "trt.layer_diff",
        "peekOfCode": "def get_trt_layers(trt_json_path):\n    with open(trt_json_path, \"r\") as f:\n        data = json.load(f)\n    layers = []\n    for l in data[\"Layers\"]:\n        layers.append({\n            \"name\": l.get(\"Name\", \"\"),\n            \"type\": l.get(\"LayerType\", \"\"),\n            \"metadata\": l.get(\"Metadata\", \"\")\n        })",
        "detail": "trt.layer_diff",
        "documentation": {}
    },
    {
        "label": "compute_fine_grained_stats",
        "kind": 2,
        "importPath": "trt.layer_diff",
        "description": "trt.layer_diff",
        "peekOfCode": "def compute_fine_grained_stats(onnx_layers, trt_layers):\n    n1 = 0  # 1:1 \n    n2 = 0  # ONNX \n    n3 = 0  #  TRT \n    n4 = 0  # TRT \n    n5 = 0  # ONNX \n    onnx_name_set = set([l[\"name\"] for l in onnx_layers])\n    onnx_in_trt = set()\n    for trt in trt_layers:\n        metadata = trt.get(\"metadata\", \"\")",
        "detail": "trt.layer_diff",
        "documentation": {}
    },
    {
        "label": "print_stats",
        "kind": 2,
        "importPath": "trt.layer_diff",
        "description": "trt.layer_diff",
        "peekOfCode": "def print_stats(stats):\n    print(\"\\n===============  ================\")\n    print(f\"ONNX  (N_onnx) : {stats['num_onnx']}\")\n    print(f\"TRT   (N_trt)  : {stats['num_trt']}\")\n    print(\"=============================================\\n\")\n    print(f\"      n1  : {stats['n1_exact']}\")\n    print(f\"ONNX  n2  : {stats['n2_fused_onnx']}\")\n    print(f\"TRT   n3  : {stats['n3_fused_trt']}\")\n    print(f\"TRT     n4  : {stats['n4_trt_added']}\")\n    print(f\"ONNX      n5  : {stats['n5_onnx_disappeared']}\")",
        "detail": "trt.layer_diff",
        "documentation": {}
    },
    {
        "label": "get_onnx_layers",
        "kind": 2,
        "importPath": "trt.layer_diff2",
        "description": "trt.layer_diff2",
        "peekOfCode": "def get_onnx_layers(onnx_path):\n    \"\"\"\n    DFS ONNX \n     name, type, inputs, outputs\n    \"\"\"\n    model = onnx.load(onnx_path)\n    graph = model.graph\n    #  name -> node \n    node_map = {}\n    for node in graph.node:",
        "detail": "trt.layer_diff2",
        "documentation": {}
    },
    {
        "label": "get_trt_layers",
        "kind": 2,
        "importPath": "trt.layer_diff2",
        "description": "trt.layer_diff2",
        "peekOfCode": "def get_trt_layers(trt_json_path):\n    with open(trt_json_path, \"r\") as f:\n        data = json.load(f)\n    layers = []\n    for l in data[\"Layers\"]:\n        layers.append({\n            \"name\": l.get(\"Name\", \"\"),\n            \"type\": l.get(\"LayerType\", \"\"),\n            \"metadata\": l.get(\"Metadata\", \"\")\n        })",
        "detail": "trt.layer_diff2",
        "documentation": {}
    },
    {
        "label": "compute_fine_grained_stats",
        "kind": 2,
        "importPath": "trt.layer_diff2",
        "description": "trt.layer_diff2",
        "peekOfCode": "def compute_fine_grained_stats(onnx_layers, trt_layers):\n    n1 = n2 = n3 = n4 = n5 = 0\n    onnx_name_set = set([l[\"name\"] for l in onnx_layers])\n    onnx_in_trt = set()\n    for trt in trt_layers:\n        metadata = trt.get(\"metadata\", \"\")\n        if metadata:\n            onnx_list = []\n            for item in metadata.split(\"\\u001e\"):\n                item = item.strip()",
        "detail": "trt.layer_diff2",
        "documentation": {}
    },
    {
        "label": "print_stats",
        "kind": 2,
        "importPath": "trt.layer_diff2",
        "description": "trt.layer_diff2",
        "peekOfCode": "def print_stats(stats):\n    print(\"\\n===============  ================\")\n    print(f\"ONNX  (N_onnx) : {stats['num_onnx']}\")\n    print(f\"TRT   (N_trt)  : {stats['num_trt']}\")\n    print(\"=============================================\\n\")\n    print(f\"      n1  : {stats['n1_exact']}\")\n    print(f\"ONNX  n2  : {stats['n2_fused_onnx']}\")\n    print(f\"TRT   n3  : {stats['n3_fused_trt']}\")\n    print(f\"TRT     n4  : {stats['n4_trt_added']}\")\n    print(f\"ONNX      n5  : {stats['n5_onnx_disappeared']}\")",
        "detail": "trt.layer_diff2",
        "documentation": {}
    },
    {
        "label": "load_layer_names",
        "kind": 2,
        "importPath": "trt.layer_diff_simple",
        "description": "trt.layer_diff_simple",
        "peekOfCode": "def load_layer_names(json_path):\n    \"\"\"\n     TensorRT  JSON \n    \n    1. simple.json: \"Layers\" \n    2. detailed.json: \"Layers\"  dict  \"Name\"\n    \"\"\"\n    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    if isinstance(data, dict) and \"Layers\" in data:",
        "detail": "trt.layer_diff_simple",
        "documentation": {}
    },
    {
        "label": "default_layers",
        "kind": 5,
        "importPath": "trt.layer_diff_simple",
        "description": "trt.layer_diff_simple",
        "peekOfCode": "default_layers = load_layer_names(\"trt/export_layer_fp32_simple.json\")\ndetailed_layers = load_layer_names(\"trt/export_layer_fp32.json\")\n# \nset_default = set(default_layers)\nset_detailed = set(detailed_layers)\nmissing_in_detailed = set_default - set_detailed\nextra_in_detailed = set_detailed - set_default\nprint(f\"Default : {len(default_layers)}\")\nprint(f\"Detailed : {len(detailed_layers)}\\n\")\nprint(f\"Default  Detailed  ({len(missing_in_detailed)}):\")",
        "detail": "trt.layer_diff_simple",
        "documentation": {}
    },
    {
        "label": "detailed_layers",
        "kind": 5,
        "importPath": "trt.layer_diff_simple",
        "description": "trt.layer_diff_simple",
        "peekOfCode": "detailed_layers = load_layer_names(\"trt/export_layer_fp32.json\")\n# \nset_default = set(default_layers)\nset_detailed = set(detailed_layers)\nmissing_in_detailed = set_default - set_detailed\nextra_in_detailed = set_detailed - set_default\nprint(f\"Default : {len(default_layers)}\")\nprint(f\"Detailed : {len(detailed_layers)}\\n\")\nprint(f\"Default  Detailed  ({len(missing_in_detailed)}):\")\nfor l in missing_in_detailed:",
        "detail": "trt.layer_diff_simple",
        "documentation": {}
    },
    {
        "label": "set_default",
        "kind": 5,
        "importPath": "trt.layer_diff_simple",
        "description": "trt.layer_diff_simple",
        "peekOfCode": "set_default = set(default_layers)\nset_detailed = set(detailed_layers)\nmissing_in_detailed = set_default - set_detailed\nextra_in_detailed = set_detailed - set_default\nprint(f\"Default : {len(default_layers)}\")\nprint(f\"Detailed : {len(detailed_layers)}\\n\")\nprint(f\"Default  Detailed  ({len(missing_in_detailed)}):\")\nfor l in missing_in_detailed:\n    print(\"  \", l)\nprint(f\"\\nDetailed  ({len(extra_in_detailed)}):\")",
        "detail": "trt.layer_diff_simple",
        "documentation": {}
    },
    {
        "label": "set_detailed",
        "kind": 5,
        "importPath": "trt.layer_diff_simple",
        "description": "trt.layer_diff_simple",
        "peekOfCode": "set_detailed = set(detailed_layers)\nmissing_in_detailed = set_default - set_detailed\nextra_in_detailed = set_detailed - set_default\nprint(f\"Default : {len(default_layers)}\")\nprint(f\"Detailed : {len(detailed_layers)}\\n\")\nprint(f\"Default  Detailed  ({len(missing_in_detailed)}):\")\nfor l in missing_in_detailed:\n    print(\"  \", l)\nprint(f\"\\nDetailed  ({len(extra_in_detailed)}):\")\nfor l in extra_in_detailed:",
        "detail": "trt.layer_diff_simple",
        "documentation": {}
    },
    {
        "label": "missing_in_detailed",
        "kind": 5,
        "importPath": "trt.layer_diff_simple",
        "description": "trt.layer_diff_simple",
        "peekOfCode": "missing_in_detailed = set_default - set_detailed\nextra_in_detailed = set_detailed - set_default\nprint(f\"Default : {len(default_layers)}\")\nprint(f\"Detailed : {len(detailed_layers)}\\n\")\nprint(f\"Default  Detailed  ({len(missing_in_detailed)}):\")\nfor l in missing_in_detailed:\n    print(\"  \", l)\nprint(f\"\\nDetailed  ({len(extra_in_detailed)}):\")\nfor l in extra_in_detailed:\n    print(\"  \", l)",
        "detail": "trt.layer_diff_simple",
        "documentation": {}
    },
    {
        "label": "extra_in_detailed",
        "kind": 5,
        "importPath": "trt.layer_diff_simple",
        "description": "trt.layer_diff_simple",
        "peekOfCode": "extra_in_detailed = set_detailed - set_default\nprint(f\"Default : {len(default_layers)}\")\nprint(f\"Detailed : {len(detailed_layers)}\\n\")\nprint(f\"Default  Detailed  ({len(missing_in_detailed)}):\")\nfor l in missing_in_detailed:\n    print(\"  \", l)\nprint(f\"\\nDetailed  ({len(extra_in_detailed)}):\")\nfor l in extra_in_detailed:\n    print(\"  \", l)",
        "detail": "trt.layer_diff_simple",
        "documentation": {}
    },
    {
        "label": "mb",
        "kind": 2,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "def mb(x): return x / 1024.0 / 1024.0\ndef safe_load_engine(path):\n    if not os.path.exists(path) or os.path.getsize(path) == 0:\n        return None, \"Engine file missing or empty\", None\n    free_before, _ = cuda.mem_get_info()\n    try:\n        with open(path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n            engine = runtime.deserialize_cuda_engine(f.read())\n        if engine is None:\n            return None, \"Deserialize returned None\", None",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "safe_load_engine",
        "kind": 2,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "def safe_load_engine(path):\n    if not os.path.exists(path) or os.path.getsize(path) == 0:\n        return None, \"Engine file missing or empty\", None\n    free_before, _ = cuda.mem_get_info()\n    try:\n        with open(path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n            engine = runtime.deserialize_cuda_engine(f.read())\n        if engine is None:\n            return None, \"Deserialize returned None\", None\n    except Exception as e:",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "preprocess_image",
        "kind": 2,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "def preprocess_image(img_path, shape):\n    img = Image.open(img_path).convert(\"RGB\")\n    img = img.resize((shape[3], shape[2]))  # W,H\n    img = np.array(img).astype(np.float32) / 255.0\n    # HWC -> CHW\n    img = img.transpose(2, 0, 1)\n    img = np.expand_dims(img, 0)  # 1xCxHxW\n    return img\ndef prepare_buffers_and_bind(context, engine):\n    buffers = {}",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "prepare_buffers_and_bind",
        "kind": 2,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "def prepare_buffers_and_bind(context, engine):\n    buffers = {}\n    for name in engine:\n        dtype = np.dtype(trt.nptype(engine.get_tensor_dtype(name)))\n        shape = context.get_tensor_shape(name)\n        size = int(trt.volume(shape))\n        host_mem = cuda.pagelocked_empty(size, dtype)\n        device_mem = cuda.mem_alloc(host_mem.nbytes)\n        buffers[name] = {\n            \"host\": host_mem,",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "run_inference",
        "kind": 2,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "def run_inference(context, buffers, stream, n_runs=N_RUNS):\n    latencies = []\n    for _ in range(N_WARMUP):\n        for name, buf in buffers.items():\n            if buf[\"is_input\"]:\n                cuda.memcpy_htod_async(buf[\"device\"], buf[\"host\"], stream)\n        context.execute_async_v3(stream_handle=stream.handle)\n        for name, buf in buffers.items():\n            if not buf[\"is_input\"]:\n                cuda.memcpy_dtoh_async(buf[\"host\"], buf[\"device\"], stream)",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "compute_metrics",
        "kind": 2,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "def compute_metrics(ref, other, top_k=10):\n    ref_flat = ref.flatten()\n    other_flat = other.flatten()\n    # \n    cos_sim = float(np.dot(ref_flat, other_flat) / \n                    (np.linalg.norm(ref_flat) * np.linalg.norm(other_flat) + 1e-12))\n    # RMSE\n    diff = ref_flat - other_flat\n    rmse = float(np.sqrt(np.mean(diff ** 2)))\n    #  top_k ",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "engine_paths",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "engine_paths = {\n    \"FP32\": \"/home/wanglin/workspace/logicAI/onnx/light_fp32.trt\",\n    \"FP16\": \"/home/wanglin/workspace/logicAI/onnx/light_fp16.trt\",\n    \"INT8\": \"/home/wanglin/workspace/logicAI/onnx/light_int8.trt\"\n}\n# left_dir = \"/media/wanglin/Elements/datasets/SceneFlow/Driving/frames_cleanpass/15mm_focallength/scene_backwards/fast/left\"\n# right_dir = \"/media/wanglin/Elements/datasets/SceneFlow/Driving/frames_cleanpass/15mm_focallength/scene_backwards/fast/right\"\n# left_dir = \"/media/wanglin/Elements/datasets/KITTI2015/training/image_2\"\n# right_dir = \"/media/wanglin/Elements/datasets/KITTI2015/training/image_3\"\nleft_dir = \"/media/wanglin/Elements/datasets/DrivingStereo/cloudy/left-image-full-size/cloudy/left-image-full-size\"",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "left_dir",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "left_dir = \"/media/wanglin/Elements/datasets/DrivingStereo/cloudy/left-image-full-size/cloudy/left-image-full-size\"\nright_dir = \"/media/wanglin/Elements/datasets/DrivingStereo/cloudy/right-image-full-size/cloudy/right-image-full-size\"\ndisp_out_dir = \"./trt/npy\"\nN = 10  #  N \ninput_shapes = {\n    \"left_img\": (1, 3, 256, 512),\n    \"right_img\": (1, 3, 256, 512)\n}\nN_WARMUP = 5\nN_RUNS = 50",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "right_dir",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "right_dir = \"/media/wanglin/Elements/datasets/DrivingStereo/cloudy/right-image-full-size/cloudy/right-image-full-size\"\ndisp_out_dir = \"./trt/npy\"\nN = 10  #  N \ninput_shapes = {\n    \"left_img\": (1, 3, 256, 512),\n    \"right_img\": (1, 3, 256, 512)\n}\nN_WARMUP = 5\nN_RUNS = 50\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "disp_out_dir",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "disp_out_dir = \"./trt/npy\"\nN = 10  #  N \ninput_shapes = {\n    \"left_img\": (1, 3, 256, 512),\n    \"right_img\": (1, 3, 256, 512)\n}\nN_WARMUP = 5\nN_RUNS = 50\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n# ------------------------",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "N",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "N = 10  #  N \ninput_shapes = {\n    \"left_img\": (1, 3, 256, 512),\n    \"right_img\": (1, 3, 256, 512)\n}\nN_WARMUP = 5\nN_RUNS = 50\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n# ------------------------\n# ",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "input_shapes",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "input_shapes = {\n    \"left_img\": (1, 3, 256, 512),\n    \"right_img\": (1, 3, 256, 512)\n}\nN_WARMUP = 5\nN_RUNS = 50\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n# ------------------------\n# \n# ------------------------",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "N_WARMUP",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "N_WARMUP = 5\nN_RUNS = 50\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n# ------------------------\n# \n# ------------------------\ndef mb(x): return x / 1024.0 / 1024.0\ndef safe_load_engine(path):\n    if not os.path.exists(path) or os.path.getsize(path) == 0:\n        return None, \"Engine file missing or empty\", None",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "N_RUNS",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "N_RUNS = 50\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n# ------------------------\n# \n# ------------------------\ndef mb(x): return x / 1024.0 / 1024.0\ndef safe_load_engine(path):\n    if not os.path.exists(path) or os.path.getsize(path) == 0:\n        return None, \"Engine file missing or empty\", None\n    free_before, _ = cuda.mem_get_info()",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "TRT_LOGGER",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n# ------------------------\n# \n# ------------------------\ndef mb(x): return x / 1024.0 / 1024.0\ndef safe_load_engine(path):\n    if not os.path.exists(path) or os.path.getsize(path) == 0:\n        return None, \"Engine file missing or empty\", None\n    free_before, _ = cuda.mem_get_info()\n    try:",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "left_images",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "left_images = sorted(glob.glob(os.path.join(left_dir, \"*.png\")))[:N]\nright_images = sorted(glob.glob(os.path.join(right_dir, \"*.png\")))[:N]\ndisp_outputs = [os.path.join(disp_out_dir, os.path.basename(f).replace(\".png\", \"_disp.npy\"))\n                for f in left_images]\nassert len(left_images) == len(right_images) == len(disp_outputs), \"\"\n# ------------------------\n# FP32/FP16/INT8\n# ------------------------\nresults = {}\nreference_outputs = None",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "right_images",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "right_images = sorted(glob.glob(os.path.join(right_dir, \"*.png\")))[:N]\ndisp_outputs = [os.path.join(disp_out_dir, os.path.basename(f).replace(\".png\", \"_disp.npy\"))\n                for f in left_images]\nassert len(left_images) == len(right_images) == len(disp_outputs), \"\"\n# ------------------------\n# FP32/FP16/INT8\n# ------------------------\nresults = {}\nreference_outputs = None\nreference_name = None",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "disp_outputs",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "disp_outputs = [os.path.join(disp_out_dir, os.path.basename(f).replace(\".png\", \"_disp.npy\"))\n                for f in left_images]\nassert len(left_images) == len(right_images) == len(disp_outputs), \"\"\n# ------------------------\n# FP32/FP16/INT8\n# ------------------------\nresults = {}\nreference_outputs = None\nreference_name = None\nfor mode, engine_path in engine_paths.items():",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "results",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "results = {}\nreference_outputs = None\nreference_name = None\nfor mode, engine_path in engine_paths.items():\n    print(f\"\\n[INFO] Processing {mode} engine: {engine_path}\")\n    engine, err, mem_used_mib = safe_load_engine(engine_path)\n    if engine is None:\n        print(f\"  [ERROR] Load failed: {err}\")\n        results[mode] = {\"path\": engine_path, \"error\": err}\n        continue",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "reference_outputs",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "reference_outputs = None\nreference_name = None\nfor mode, engine_path in engine_paths.items():\n    print(f\"\\n[INFO] Processing {mode} engine: {engine_path}\")\n    engine, err, mem_used_mib = safe_load_engine(engine_path)\n    if engine is None:\n        print(f\"  [ERROR] Load failed: {err}\")\n        results[mode] = {\"path\": engine_path, \"error\": err}\n        continue\n    context = engine.create_execution_context()",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "reference_name",
        "kind": 5,
        "importPath": "trt.metrics",
        "description": "trt.metrics",
        "peekOfCode": "reference_name = None\nfor mode, engine_path in engine_paths.items():\n    print(f\"\\n[INFO] Processing {mode} engine: {engine_path}\")\n    engine, err, mem_used_mib = safe_load_engine(engine_path)\n    if engine is None:\n        print(f\"  [ERROR] Load failed: {err}\")\n        results[mode] = {\"path\": engine_path, \"error\": err}\n        continue\n    context = engine.create_execution_context()\n    stream = cuda.Stream()",
        "detail": "trt.metrics",
        "documentation": {}
    },
    {
        "label": "get_layer_type",
        "kind": 2,
        "importPath": "trt.simple_layer_analysis",
        "description": "trt.simple_layer_analysis",
        "peekOfCode": "def get_layer_type(layer_name):\n    \"\"\"\"\"\"\n    layer_name = layer_name.strip()\n    if not layer_name:\n        return None\n    sublayers = layer_name.split('+')  # \n    types = []\n    for sub in sublayers:\n        sub = sub.strip()\n        if not sub:",
        "detail": "trt.simple_layer_analysis",
        "documentation": {}
    },
    {
        "label": "analyze_layers",
        "kind": 2,
        "importPath": "trt.simple_layer_analysis",
        "description": "trt.simple_layer_analysis",
        "peekOfCode": "def analyze_layers(layers):\n    total_layers_after = len(layers)\n    fusion_layers = [l for l in layers if '+' in l]\n    num_fusion_layers = len(fusion_layers)\n    original_layers = total_layers_after + sum(len(l.split('+')) - 1 for l in fusion_layers)\n    # \n    all_types = []\n    for l in layers:\n        types = get_layer_type(l)\n        if types:",
        "detail": "trt.simple_layer_analysis",
        "documentation": {}
    },
    {
        "label": "Layers",
        "kind": 5,
        "importPath": "trt.simple_layer_analysis",
        "description": "trt.simple_layer_analysis",
        "peekOfCode": "Layers = data.get(\"Layers\", [])\ndef get_layer_type(layer_name):\n    \"\"\"\"\"\"\n    layer_name = layer_name.strip()\n    if not layer_name:\n        return None\n    sublayers = layer_name.split('+')  # \n    types = []\n    for sub in sublayers:\n        sub = sub.strip()",
        "detail": "trt.simple_layer_analysis",
        "documentation": {}
    },
    {
        "label": "res",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def res(input_shape, output_shape):\n    input_res = f\"{input_shape[-2]}x{input_shape[-1]}\"\n    output_res = f\"{output_shape[-2]}x{output_shape[-1]}\" if len(output_shape) >= 2 else str(output_shape)\n    return (input_res, output_res)\ndef compute_avgerr(pred, gt, mask=None):\n    \"\"\"\n    MAE\n    \"\"\"\n    diff = (pred - gt)\n    abs_err = torch.abs(diff)",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "compute_avgerr",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def compute_avgerr(pred, gt, mask=None):\n    \"\"\"\n    MAE\n    \"\"\"\n    diff = (pred - gt)\n    abs_err = torch.abs(diff)\n    if mask is not None:\n        abs_err = abs_err[mask]\n    return abs_err.mean().item()\ndef compute_rmse(pred, gt, mask=None):",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "compute_rmse",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def compute_rmse(pred, gt, mask=None):\n    diff = (pred - gt)\n    if mask is not None:\n        mse = torch.mean((diff[mask]) ** 2)\n    else:\n        mse = torch.mean(diff ** 2)\n    return torch.sqrt(mse).item()\ndef get_model_size(model):\n    param_size = 0\n    for param in model.parameters():",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "get_model_size",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def get_model_size(model):\n    param_size = 0\n    for param in model.parameters():\n        param_size += param.nelement() * param.element_size()\n    buffer_size = 0\n    for buffer in model.buffers():\n        buffer_size += buffer.nelement() * buffer.element_size()\n    size_all_mb = (param_size + buffer_size) / 1024 ** 2\n    return round(size_all_mb, 2)\ndef get_gpu_memory_global(device_id: int = 0) -> float:",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "get_gpu_memory_global",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def get_gpu_memory_global(device_id: int = 0) -> float:\n    \"\"\"\n     GPU MB\n     Python  os.getpid() \n    \"\"\"\n    pynvml.nvmlInit()\n    handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)\n    #  B\n    procs = pynvml.nvmlDeviceGetComputeRunningProcesses(handle)\n    my_pid = os.getpid()",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "get_memory_usage",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def get_memory_usage():\n    process = psutil.Process(os.getpid())\n    mem = process.memory_info().rss / 1024 ** 2\n    return round(mem, 2)\ndef get_power_usage():\n    try:\n        output = os.popen(\"nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits\").read()\n        power = float(output.strip().split('\\n')[0])\n        return power\n    except Exception:",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "get_power_usage",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def get_power_usage():\n    try:\n        output = os.popen(\"nvidia-smi --query-gpu=power.draw --format=csv,noheader,nounits\").read()\n        power = float(output.strip().split('\\n')[0])\n        return power\n    except Exception:\n        return -1\ndef compute_bad(epe_flattened, val, threshold):\n    \"\"\"\n    epe_flattened:  (1D tensor)",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "compute_bad",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def compute_bad(epe_flattened, val, threshold):\n    \"\"\"\n    epe_flattened:  (1D tensor)\n    val: mask (bool tensor)\n    threshold: 1.0, 2.0, 3.0\n    \n    \"\"\"\n    out = (epe_flattened > threshold)\n    bad = out[val].float().mean().item()\n    return bad",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "format_flops",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def format_flops(flops, unit='GFLOPs'):\n    units = {\n        'FLOPs': 1,\n        'KFLOPs': 1e3,\n        'MFLOPs': 1e6,\n        'GFLOPs': 1e9,\n        'TFLOPs': 1e12,\n        'PFLOPs': 1e15,\n    }\n    if unit not in units:",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "get_flops",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def get_flops(model, tup, device):\n    # FLOPs\n    # try:\n    #     sample = next(iter(dataloader))\n    #     dummy1 = sample['image1'].to(device)\n    #     dummy2 = sample['image2'].to(device)\n    #     macs, params = summary(model, input_data=(dummy1, dummy2), verbose=0)\n    #     flops = macs\n    # except Exception:\n    #     flops = -1",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "evaluate_industrial_metrics",
        "description": "evaluate_industrial_metrics",
        "peekOfCode": "def evaluate(model, dataloader, device='cuda'):\n    model.eval()\n    epe_list, d1_list, rmse_list, latency_list = [], [], [], []\n    with torch.no_grad():\n        for batch in dataloader:\n            image1, image2, gt = batch['image1'].to(device), batch['image2'].to(device), batch['gt'].to(device)\n            mask = batch.get('mask', None)\n            if mask is not None:\n                mask = mask.to(device)\n            start = time.time()",
        "detail": "evaluate_industrial_metrics",
        "documentation": {}
    }
]