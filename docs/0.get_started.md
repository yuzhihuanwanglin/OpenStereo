# Get Started

## Installation

1. clone this repo.
    ```
    https://github.com/XiandaGuo/OpenStereo
    ```
2. Install dependenices:
    - pytorch >= 1.13.1
    - torchvision
    - timm == 0.5.4  (For the NMRFStereo model, timm >= 0.9)
    - pyyaml
    - tensorboard
    - opencv-python
    - tqdm
    - scikit-image

   Create a conda environment by:
   ```
   conda create -n openstereo python=3.8 
   ```
   
   If you want to train/eval NMRF-Stereo, you need to build deformable attention and superpixel-guided disparity downsample operator:
   ```
   cd stereo/modeling/models/nmrf/ops && sh make.sh && cd ..
   ```
   
   Install pytorch by [Anaconda](https://pytorch.org/get-started/locally/):
   ```
   conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia
   ```
   Install other dependencies by pip:
   ```
   pip install -r requirements.txt
   ```

## Prepare dataset

See [prepare dataset](2.prepare_dataset.md).

## Get trained model

Go to the [model zoom](1.model_zoo.md), download the model file and uncompress it to output.

## Train

Train a model with a Single GPU
```
python tools/train.py --cfg_file cfgs/lightstereo/lightstereo_s_sceneflow.yaml
```
Multi-GPU Training on Single Node
```
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
torchrun --nnodes=1 --nproc_per_node=8 --rdzv_backend=c10d --rdzv_endpoint=localhost:23456 tools/train.py --dist_mode --cfg_file cfgs/lightstereo/lightstereo_s_sceneflow.yaml
```

- `--config` The path to the config file.
- `--dist_mode` If specified, the program will use DDP to train.
- your exp will saved in '/save_root_dir/DATASET_NAME/MODEL_NAME/config_file_perfix/extra_tag', save_root_dir and extra_tag can specified in train argparse

## Val

Evaluate the trained model by
```
python tools/eval.py --cfg_file cfgs/lightstereo/lightstereo_s_sceneflow.yaml --eval_data_cfg_file cfgs/sceneflow_eval.yaml --pretrained_model your_pretrained_ckpt_path
```
Generalization Evaluation:
```
python tools/eval.py --cfg_file cfgs/lightstereo/lightstereo_s_sceneflow.yaml --eval_data_cfg_file cfgs/eth3d_eval.yaml --pretrained_model your_pretrained_ckpt_path
python tools/eval.py --cfg_file cfgs/lightstereo/lightstereo_s_sceneflow.yaml --eval_data_cfg_file cfgs/middlebury_eval.yaml --pretrained_model your_pretrained_ckpt_path
python tools/eval.py --cfg_file cfgs/lightstereo/lightstereo_s_sceneflow.yaml --eval_data_cfg_file cfgs/kitti15_eval.yaml --pretrained_model your_pretrained_ckpt_path
python tools/eval.py --cfg_file cfgs/lightstereo/lightstereo_s_sceneflow.yaml --eval_data_cfg_file cfgs/kitti12_eval.yaml --pretrained_model your_pretrained_ckpt_path
python tools/eval.py --cfg_file cfgs/lightstereo/lightstereo_s_sceneflow.yaml --eval_data_cfg_file cfgs/driving_eval.yaml --pretrained_model your_pretrained_ckpt_path
```

- `--cfg_file` The path to the config file.
- `--eval_data_cfg_file` The dataset config you want to eval.
- `--pretrained_model` your pre-trained checkpoint

**Tip**: Other arguments are the same as the train phase.

## Infer

Infer the trained model by
```
python tools/infer.py --cfg_file cfgs/lightstereo/lightstereo_s_sceneflow.yaml --left_img_path your_left_img_path --right_img_path your_right_img_path
```
**Tip**: the pretrained_model should be written in cfg_file, and if you want to process multiple image pairs at once, please organize the file structure and write a simple loop.


## Customize

1. Read the [detailed config](3.detailed_config.md) to figure out the usage of needed setting items;
2. See [how to create your model](4.how_to_create_your_model.md);

## Other
1. You can set the default pre-trained model path, by `export TORCH_HOME="/path/to/pretrained_models"`
